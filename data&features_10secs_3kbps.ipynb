{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7befb6d2-774d-464a-80da-826d4be5d578",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.9.9 (tags/v3.9.9:ccb0e6a, Nov 15 2021, 18:08:50) [MSC v.1929 64 bit (AMD64)]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'1.22.3'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.version)\n",
    "import numpy\n",
    "numpy.version.version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "50016272-c722-462d-a768-6685d37fdebb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy==1.19.5 in c:\\users\\ripple\\anaconda3\\envs\\rapacapella\\lib\\site-packages (1.19.5)\n",
      "Requirement already satisfied: torch in c:\\users\\ripple\\anaconda3\\envs\\rapacapella\\lib\\site-packages (1.13.1)\n",
      "Requirement already satisfied: torchaudio in c:\\users\\ripple\\anaconda3\\envs\\rapacapella\\lib\\site-packages (0.13.1)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\ripple\\anaconda3\\envs\\rapacapella\\lib\\site-packages (from torch) (4.7.1)\n",
      "Collecting git+https://github.com/huggingface/transformers.git@main\n",
      "  Cloning https://github.com/huggingface/transformers.git (to revision main) to c:\\users\\ripple\\appdata\\local\\temp\\pip-req-build-m0_jg_0z\n",
      "  Resolved https://github.com/huggingface/transformers.git to commit 3ea3ab62d80d91f9bdd16bd3cacd8133fb0d4566\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Requirement already satisfied: requests in c:\\users\\ripple\\anaconda3\\envs\\rapacapella\\lib\\site-packages (from transformers==4.47.0.dev0) (2.31.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\ripple\\anaconda3\\envs\\rapacapella\\lib\\site-packages (from transformers==4.47.0.dev0) (4.66.4)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\ripple\\anaconda3\\envs\\rapacapella\\lib\\site-packages (from transformers==4.47.0.dev0) (1.19.5)\n",
      "Collecting tokenizers<0.21,>=0.20\n",
      "  Downloading tokenizers-0.20.3-cp37-none-win_amd64.whl (2.4 MB)\n",
      "     ---------------------------------------- 2.4/2.4 MB 6.3 MB/s eta 0:00:00\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\ripple\\anaconda3\\envs\\rapacapella\\lib\\site-packages (from transformers==4.47.0.dev0) (2024.4.16)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\ripple\\anaconda3\\envs\\rapacapella\\lib\\site-packages (from transformers==4.47.0.dev0) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\ripple\\anaconda3\\envs\\rapacapella\\lib\\site-packages (from transformers==4.47.0.dev0) (6.0.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\ripple\\anaconda3\\envs\\rapacapella\\lib\\site-packages (from transformers==4.47.0.dev0) (3.12.2)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\ripple\\anaconda3\\envs\\rapacapella\\lib\\site-packages (from transformers==4.47.0.dev0) (0.4.3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  Running command git clone --filter=blob:none --quiet https://github.com/huggingface/transformers.git 'C:\\Users\\Ripple\\AppData\\Local\\Temp\\pip-req-build-m0_jg_0z'\n",
      "ERROR: Ignored the following versions that require a different python version: 0.17.0 Requires-Python >=3.8.0; 0.17.0rc0 Requires-Python >=3.8.0; 0.17.1 Requires-Python >=3.8.0; 0.17.2 Requires-Python >=3.8.0; 0.17.3 Requires-Python >=3.8.0; 0.18.0 Requires-Python >=3.8.0; 0.18.0rc0 Requires-Python >=3.8.0; 0.19.0 Requires-Python >=3.8.0; 0.19.0rc0 Requires-Python >=3.8.0; 0.19.1 Requires-Python >=3.8.0; 0.19.2 Requires-Python >=3.8.0; 0.19.3 Requires-Python >=3.8.0; 0.19.4 Requires-Python >=3.8.0; 0.20.0 Requires-Python >=3.8.0; 0.20.0rc0 Requires-Python >=3.8.0; 0.20.0rc1 Requires-Python >=3.8.0; 0.20.1 Requires-Python >=3.8.0; 0.20.2 Requires-Python >=3.8.0; 0.20.3 Requires-Python >=3.8.0; 0.21.0 Requires-Python >=3.8.0; 0.21.0rc0 Requires-Python >=3.8.0; 0.21.1 Requires-Python >=3.8.0; 0.21.2 Requires-Python >=3.8.0; 0.21.3 Requires-Python >=3.8.0; 0.21.4 Requires-Python >=3.8.0; 0.22.0 Requires-Python >=3.8.0; 0.22.0rc0 Requires-Python >=3.8.0; 0.22.0rc1 Requires-Python >=3.8.0; 0.22.1 Requires-Python >=3.8.0; 0.22.2 Requires-Python >=3.8.0; 0.23.0 Requires-Python >=3.8.0; 0.23.0rc0 Requires-Python >=3.8.0; 0.23.0rc1 Requires-Python >=3.8.0; 0.23.1 Requires-Python >=3.8.0; 0.23.2 Requires-Python >=3.8.0; 0.23.3 Requires-Python >=3.8.0; 0.23.4 Requires-Python >=3.8.0; 0.23.5 Requires-Python >=3.8.0; 0.24.0 Requires-Python >=3.8.0; 0.24.0rc0 Requires-Python >=3.8.0; 0.24.1 Requires-Python >=3.8.0; 0.24.2 Requires-Python >=3.8.0; 0.24.3 Requires-Python >=3.8.0; 0.24.4 Requires-Python >=3.8.0; 0.24.5 Requires-Python >=3.8.0; 0.24.6 Requires-Python >=3.8.0; 0.24.7 Requires-Python >=3.8.0; 0.25.0 Requires-Python >=3.8.0; 0.25.0rc0 Requires-Python >=3.8.0; 0.25.0rc1 Requires-Python >=3.8.0; 0.25.1 Requires-Python >=3.8.0; 0.25.2 Requires-Python >=3.8.0; 0.26.0 Requires-Python >=3.8.0; 0.26.0rc0 Requires-Python >=3.8.0; 0.26.1 Requires-Python >=3.8.0; 0.26.2 Requires-Python >=3.8.0\n",
      "ERROR: Could not find a version that satisfies the requirement huggingface-hub<1.0,>=0.23.2 (from transformers) (from versions: 0.0.1, 0.0.2, 0.0.3rc1, 0.0.3rc2, 0.0.5, 0.0.6, 0.0.7, 0.0.8, 0.0.9, 0.0.10, 0.0.11, 0.0.12, 0.0.13, 0.0.14, 0.0.15, 0.0.16, 0.0.17, 0.0.18, 0.0.19, 0.1.0, 0.1.1, 0.1.2, 0.2.0, 0.2.1, 0.4.0, 0.5.0, 0.5.1, 0.6.0rc0, 0.6.0, 0.7.0rc0, 0.7.0, 0.8.0rc0, 0.8.0rc1, 0.8.0rc2, 0.8.0rc3, 0.8.0rc4, 0.8.0, 0.8.1, 0.9.0.dev0, 0.9.0rc0, 0.9.0rc2, 0.9.0rc3, 0.9.0, 0.9.1, 0.10.0rc0, 0.10.0rc1, 0.10.0rc3, 0.10.0, 0.10.1, 0.11.0rc0, 0.11.0rc1, 0.11.0, 0.11.1, 0.12.0rc0, 0.12.0, 0.12.1, 0.13.0rc0, 0.13.0rc1, 0.13.0, 0.13.1, 0.13.2, 0.13.3, 0.13.4, 0.14.0rc0, 0.14.0rc1, 0.14.0, 0.14.1, 0.15.0rc0, 0.15.0, 0.15.1, 0.16.0rc0, 0.16.1, 0.16.2, 0.16.3, 0.16.4)\n",
      "ERROR: No matching distribution found for huggingface-hub<1.0,>=0.23.2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: cython in c:\\users\\ripple\\anaconda3\\envs\\rapacapella\\lib\\site-packages (3.0.10)\n",
      "Requirement already satisfied: madmom in c:\\users\\ripple\\anaconda3\\envs\\rapacapella\\lib\\site-packages (0.16.1)\n",
      "Requirement already satisfied: numpy>=1.13.4 in c:\\users\\ripple\\anaconda3\\envs\\rapacapella\\lib\\site-packages (from madmom) (1.19.5)\n",
      "Requirement already satisfied: cython>=0.25 in c:\\users\\ripple\\anaconda3\\envs\\rapacapella\\lib\\site-packages (from madmom) (3.0.10)\n",
      "Requirement already satisfied: scipy>=0.16 in c:\\users\\ripple\\anaconda3\\envs\\rapacapella\\lib\\site-packages (from madmom) (1.7.3)\n",
      "Requirement already satisfied: mido>=1.2.8 in c:\\users\\ripple\\anaconda3\\envs\\rapacapella\\lib\\site-packages (from madmom) (1.3.2)\n",
      "Requirement already satisfied: importlib-metadata in c:\\users\\ripple\\anaconda3\\envs\\rapacapella\\lib\\site-packages (from mido>=1.2.8->madmom) (6.7.0)\n",
      "Requirement already satisfied: packaging~=23.1 in c:\\users\\ripple\\anaconda3\\envs\\rapacapella\\lib\\site-packages (from mido>=1.2.8->madmom) (23.2)\n",
      "Requirement already satisfied: typing-extensions>=3.6.4 in c:\\users\\ripple\\anaconda3\\envs\\rapacapella\\lib\\site-packages (from importlib-metadata->mido>=1.2.8->madmom) (4.7.1)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\users\\ripple\\anaconda3\\envs\\rapacapella\\lib\\site-packages (from importlib-metadata->mido>=1.2.8->madmom) (3.15.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install numpy==1.19.5\n",
    "!pip install torch torchaudio\n",
    "!pip install git+https://github.com/huggingface/transformers.git@main\n",
    "!pip install cython\n",
    "!pip install madmom\n",
    "import os\n",
    "import numpy as np\n",
    "import torchaudio\n",
    "import torch\n",
    "from transformers import AutoProcessor, EncodecModel\n",
    "import madmom"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "062f2877",
   "metadata": {},
   "source": [
    "# First 10-Second Audio Encoding and Preprocessing for Machine Learning\n",
    "This script preprocesses an audio dataset for training by encoding the first 10 seconds of each sample using Facebook's EnCodec model to standardize the audio length. It extracts metadata such as BPM, key, and bars from accompanying track_info.txt files and generates one-hot encodings for the music key and track classes. Additionally, the script utilizes Madmom's RNN models to detect downbeats and beats from the audio, leveraging this information to create positional embeddings that represent time-dependent features. The processed data, including audio encodings, positional embeddings, global attributes, and one-hot encodings, is saved for further use, with detailed logs and summaries exported to CSV files. This ensures a structured and consistent dataset for downstream machine learning tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf74114f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torchaudio\n",
    "import torch\n",
    "from transformers import AutoProcessor, EncodecModel, EncodecConfig\n",
    "import madmom\n",
    "import csv\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import tempfile\n",
    "\n",
    "def define_keys():\n",
    "    major_keys = [note + \"_MAJOR\" for note in [\"C\", \"C#\", \"D\", \"D#\", \"E\", \"F\", \"F#\", \"G\", \"G#\", \"A\", \"A#\", \"B\"]]\n",
    "    minor_keys = [note + \"_MINOR\" for note in [\"C\", \"C#\", \"D\", \"D#\", \"E\", \"F\", \"F#\", \"G\", \"G#\", \"A\", \"A#\", \"B\"]]\n",
    "    return major_keys + minor_keys\n",
    "\n",
    "def one_hot_encode_keys(key, all_keys):\n",
    "    one_hot_vector = np.zeros(len(all_keys))\n",
    "    if key in all_keys:\n",
    "        index = all_keys.index(key)\n",
    "        one_hot_vector[index] = 1\n",
    "    return one_hot_vector\n",
    "\n",
    "def get_instrumental_classes(root_dir):\n",
    "    instrumental_classes = set()\n",
    "    for root, _, files in os.walk(root_dir):\n",
    "        if \"track_info.txt\" in files:\n",
    "            with open(os.path.join(root, \"track_info.txt\"), 'r') as f:\n",
    "                lines = f.readlines()\n",
    "                for line in lines:\n",
    "                    if ',' in line:\n",
    "                        _, attribute = [part.strip() for part in line.split(',', 1)]\n",
    "                        if 'vocal' not in attribute.lower():\n",
    "                            instrumental_classes.add(attribute)\n",
    "    return sorted(instrumental_classes)\n",
    "\n",
    "def one_hot_encode_track_classes(track_classes, instrumental_classes):\n",
    "    one_hot_vectors = np.zeros(len(instrumental_classes))\n",
    "    for track_class in track_classes:\n",
    "        if track_class in instrumental_classes:\n",
    "            index = instrumental_classes.index(track_class)\n",
    "            one_hot_vectors[index] = 1\n",
    "    return one_hot_vectors\n",
    "\n",
    "def extract_downbeats(audio_path, fps=100, duration=10):\n",
    "    # Load the audio file\n",
    "    audio, rate = torchaudio.load(audio_path)\n",
    "    \n",
    "    # Calculate the number of samples for the specified duration\n",
    "    num_samples = int(duration * rate)\n",
    "    \n",
    "    # Slice the audio to the first 10 seconds\n",
    "    if audio.shape[1] > num_samples:\n",
    "        audio = audio[:, :num_samples]\n",
    "    \n",
    "    # Save the sliced audio to a temporary file\n",
    "    with tempfile.NamedTemporaryFile(delete=False, suffix='.wav') as temp_audio_file:\n",
    "        temp_audio_path = temp_audio_file.name\n",
    "        torchaudio.save(temp_audio_path, audio, rate)\n",
    "    \n",
    "    # Process the temporary audio file to extract downbeats\n",
    "    proc = madmom.features.downbeats.DBNDownBeatTrackingProcessor(beats_per_bar=[4], fps=fps)\n",
    "    act = madmom.features.downbeats.RNNDownBeatProcessor(fps=fps)(temp_audio_path)\n",
    "    downbeats = proc(act)\n",
    "    \n",
    "    # Filter downbeats to keep only the ones labeled as downbeats\n",
    "    filtered_downbeats = downbeats[downbeats[:, 1] == 1, 0]\n",
    "    \n",
    "    # Delete the temporary file\n",
    "    os.remove(temp_audio_path)\n",
    "    \n",
    "    return filtered_downbeats\n",
    "\n",
    "def extract_beats(audio_path, fps=100, duration=10):\n",
    "    # Load the audio file\n",
    "    audio, rate = torchaudio.load(audio_path)\n",
    "    \n",
    "    # Calculate the number of samples for the specified duration\n",
    "    num_samples = int(duration * rate)\n",
    "    \n",
    "    # Slice the audio to the first 10 seconds\n",
    "    if audio.shape[1] > num_samples:\n",
    "        audio = audio[:, :num_samples]\n",
    "    \n",
    "    # Save the sliced audio to a temporary file\n",
    "    with tempfile.NamedTemporaryFile(delete=False, suffix='.wav') as temp_audio_file:\n",
    "        temp_audio_path = temp_audio_file.name\n",
    "        torchaudio.save(temp_audio_path, audio, rate)\n",
    "    \n",
    "    # Process the temporary audio file to extract beats\n",
    "    beat_act = madmom.features.beats.RNNBeatProcessor(fps=fps)(temp_audio_path)\n",
    "    beat_proc = madmom.features.beats.BeatDetectionProcessor(fps=fps)\n",
    "    beats = beat_proc(beat_act)\n",
    "    \n",
    "    # Delete the temporary file\n",
    "    os.remove(temp_audio_path)\n",
    "    \n",
    "    return beats\n",
    "\n",
    "\n",
    "def create_positional_embeddings(beat_times, downbeat_times, audio_duration, fps=75, K=32):\n",
    "    total_frames = int(np.ceil(audio_duration * fps))\n",
    "    print ('total frames: ', total_frames)\n",
    "    \n",
    "    def ramps(positions, size):\n",
    "        result = np.zeros(size)\n",
    "        for a, b in zip(positions[:-1], positions[1:]):\n",
    "            result[a:b] = np.linspace(0, 1, b - a, endpoint=False)\n",
    "        missing = positions[0]\n",
    "        if missing:\n",
    "            piece = result[positions[0]:positions[1]]\n",
    "            pieces = np.tile(piece, missing // len(piece) + 1)\n",
    "            result[:missing] = pieces[-missing:]\n",
    "        missing = size - positions[-1]\n",
    "        if missing:\n",
    "            piece = result[positions[-2]:positions[-1]]\n",
    "            pieces = np.tile(piece, missing // len(piece) + 1)\n",
    "            result[-missing:] = pieces[:missing]\n",
    "        return result\n",
    "\n",
    "    time_vector = np.linspace(0, audio_duration, total_frames)\n",
    "    vector_downbeat = ramps((downbeat_times * fps).astype(int), total_frames)\n",
    "    vector_beat = ramps((beat_times * fps).astype(int), total_frames)\n",
    "\n",
    "    frequencies = np.arange(1, K + 1)\n",
    "    embeddings_downbeat = []\n",
    "    embeddings_beat = []\n",
    "\n",
    "    for k in frequencies:\n",
    "        embeddings_downbeat.append(np.sin(2 * np.pi * vector_downbeat * k))\n",
    "        embeddings_downbeat.append(np.cos(2 * np.pi * vector_downbeat * k))\n",
    "        embeddings_beat.append(np.sin(2 * np.pi * vector_beat * k))\n",
    "        embeddings_beat.append(np.cos(2 * np.pi * vector_beat * k))\n",
    "\n",
    "    embeddings_downbeat = np.stack(embeddings_downbeat, axis=1)\n",
    "    embeddings_beat = np.stack(embeddings_beat, axis=1)\n",
    "    embeddings = np.hstack((embeddings_downbeat, embeddings_beat))\n",
    "\n",
    "    return torch.from_numpy(embeddings).float()\n",
    "\n",
    "def get_global_attributes(track_info_path):\n",
    "    global_attributes = {}\n",
    "    with open(track_info_path, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "        for line in lines:\n",
    "            if ':' in line:\n",
    "                key, value = line.strip().split(':', 1)\n",
    "                global_attributes[key.strip().lower()] = value.strip()\n",
    "    return global_attributes\n",
    "\n",
    "def encode_audio(audio_path):\n",
    "    audio, rate = torchaudio.load(audio_path)\n",
    "    \n",
    "    # Calculate the number of samples corresponding to 10 seconds\n",
    "    max_length_in_samples = int(rate * 10)\n",
    "    \n",
    "    # Trim or pad the audio to ensure it's exactly 10 seconds long\n",
    "    if audio.shape[1] > max_length_in_samples:\n",
    "        audio = audio[:, :max_length_in_samples]\n",
    "    else:\n",
    "        pad_length = max_length_in_samples - audio.shape[1]\n",
    "        audio = torch.nn.functional.pad(audio, (0, pad_length))\n",
    "    \n",
    "    # Ensure that the audio has a single channel\n",
    "    if audio.ndim > 1:\n",
    "        audio = audio[0]\n",
    "    \n",
    "    audio_np = audio.numpy()\n",
    "    inputs = processor(audio_np, sampling_rate=rate, return_tensors=\"pt\")\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.encode(inputs[\"input_values\"], inputs[\"padding_mask\"], 3)\n",
    "    length_in_seconds = min(audio.shape[0] / rate, 10)  # Ensure length is capped at 10 seconds\n",
    "    return outputs.audio_codes.squeeze(), length_in_seconds\n",
    "\n",
    "def process_folder(folder_path, instrumental_classes, fps=100):\n",
    "    folder_data = {\n",
    "        'generation_data': {},\n",
    "        'positional_embedding': None,\n",
    "        'global': {},\n",
    "        'track': None,\n",
    "        'key_one_hot': None\n",
    "    }\n",
    "    \n",
    "    track_info_path = os.path.join(folder_path, \"track_info.txt\")\n",
    "    all_keys = define_keys()\n",
    "    folder_data['global'] = get_global_attributes(track_info_path)\n",
    "    bars, tempo = int(folder_data['global'].get('bars', 0)), float(folder_data['global'].get('bpm', 120))\n",
    "    music_key = folder_data['global'].get('key', \"\").replace(\" \", \"_\").upper()\n",
    "    \n",
    "    if music_key:\n",
    "        folder_data['key_one_hot'] = one_hot_encode_keys(music_key, all_keys)\n",
    "        print('key_one_hot', folder_data['key_one_hot'].shape)\n",
    "    \n",
    "    filename_to_track_class = {}\n",
    "    with open(track_info_path, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "        for line in lines:\n",
    "            if ',' in line:\n",
    "                file_name, track_class = [part.strip() for part in line.split(',', 1)]\n",
    "                filename_to_track_class[file_name] = track_class\n",
    "                \n",
    "    for file_name, track_class in filename_to_track_class.items():\n",
    "        audio_path = os.path.join(folder_path, file_name)\n",
    "        if os.path.exists(audio_path) and file_name.endswith(\".wav\"):\n",
    "            audio_code, length_in_seconds = encode_audio(audio_path)\n",
    "            folder_data['generation_data'][track_class] = audio_code\n",
    "            if track_class == 'full_instrumental':\n",
    "                downbeats = extract_downbeats(audio_path, fps)\n",
    "                beats = extract_beats(audio_path, fps)\n",
    "                folder_data['positional_embedding'] = create_positional_embeddings(beats, downbeats, length_in_seconds)\n",
    "                print('pos, audio', folder_data['positional_embedding'].shape, audio_code.shape, length_in_seconds)\n",
    "                \n",
    "    track_classes = [track_class for file_name, track_class in filename_to_track_class.items() if 'vocal' not in track_class.lower()]\n",
    "    folder_data['track'] = one_hot_encode_track_classes(track_classes, instrumental_classes)\n",
    "    \n",
    "    return folder_data\n",
    "\n",
    "def process_tracks(root_dir, fps=100):\n",
    "    instrumental_classes = get_instrumental_classes(root_dir)\n",
    "    all_folders_data = {}\n",
    "    \n",
    "    for root, dirs, files in os.walk(root_dir):\n",
    "        if \"track_info.txt\" in files:\n",
    "            folder_data = process_folder(root, instrumental_classes, fps)\n",
    "            all_folders_data[os.path.basename(root)] = folder_data\n",
    "            \n",
    "    return all_folders_data\n",
    "\n",
    "def save_track_classes_to_csv(instrumental_classes, output_file='track_classes.csv'):\n",
    "    with open(output_file, 'w', newline='') as csvfile:\n",
    "        writer = csv.writer(csvfile)\n",
    "        writer.writerow(['Track Class', 'One Hot Encoding'])\n",
    "        for index, track_class in enumerate(instrumental_classes):\n",
    "            one_hot_vector = [0] * len(instrumental_classes)\n",
    "            one_hot_vector[index] = 1  # Set the current class index to 1\n",
    "            writer.writerow([track_class, one_hot_vector])\n",
    "\n",
    "def save_global_attributes_to_csv(all_folders_data, output_file='global_attributes.csv'):\n",
    "    with open(output_file, mode='w', newline='', encoding='utf-8') as csvfile:\n",
    "        writer = csv.writer(csvfile)\n",
    "        writer.writerow(['Folder Name', 'BPM', 'Bars', 'Key', 'Music Key One Hot', 'Track Class One Hot'])\n",
    "        for folder_name, data in all_folders_data.items():\n",
    "            global_attributes = data['global']\n",
    "            bpm = global_attributes.get('bpm', 'N/A')\n",
    "            bars = global_attributes.get('bars', 'N/A')\n",
    "            key = global_attributes.get('key', 'N/A').replace('_', ' ').title()\n",
    "            key_one_hot = data['key_one_hot']\n",
    "            track_class_one_hot = data['track']\n",
    "            key_one_hot_str = ', '.join(map(str, key_one_hot))\n",
    "            track_class_one_hot_str = ', '.join(map(str, track_class_one_hot))\n",
    "            writer.writerow([folder_name, bpm, bars, key, key_one_hot_str, track_class_one_hot_str])\n",
    "\n",
    "def log_sample_features(folder_data, log_file='sample_features_log.csv'):\n",
    "    with open(log_file, 'w', newline='') as csvfile:\n",
    "        writer = csv.writer(csvfile)\n",
    "        writer.writerow([\n",
    "            'Folder', 'File Name', 'Track Class', 'Audio Code Shape', 'Audio Scales Shape',\n",
    "            'Positional Embedding Shape', 'Global Attributes', 'Key One Hot Encoding Shape', 'Track Class One Hot Encoding Shape'\n",
    "        ])\n",
    "\n",
    "        for folder, data in folder_data.items():\n",
    "            global_attrs = ', '.join([f\"{key}: {value}\" for key, value in data['global'].items()])\n",
    "            key_one_hot_shape = data['key_one_hot'].shape if data['key_one_hot'] is not None else 'N/A'\n",
    "            track_class_one_hot_shape = data['track'].shape if data['track'] is not None else 'N/A'\n",
    "            for file_name, details in data['generation_data'].items():\n",
    "                audio_codes_shape = details[0].shape if details[0] is not None else 'N/A'\n",
    "                audio_scales_shape = len(details[1]) if details[1] is not None else 'N/A'\n",
    "                pos_emb_shape = data['positional_embedding'].shape if data['positional_embedding'] is not None else 'N/A'\n",
    "                writer.writerow([\n",
    "                    folder, file_name, file_name.split('.')[0], audio_codes_shape, audio_scales_shape,\n",
    "                    pos_emb_shape, global_attrs, key_one_hot_shape, track_class_one_hot_shape\n",
    "                ])\n",
    "# Initialize Encodec Model and Processor\n",
    "processor = AutoProcessor.from_pretrained(\"facebook/encodec_24khz\")\n",
    "        \n",
    "model = EncodecModel.from_pretrained(\"facebook/encodec_24khz\")\n",
    "\n",
    "\n",
    "# Example usage\n",
    "root_dir = '../processed_dataset/'  # Adjust this path to your dataset location\n",
    "all_data = process_tracks(root_dir)\n",
    "print(all_data, len(all_data))\n",
    "log_sample_features(all_data)\n",
    "\n",
    "# Save track classes and global attributes to CSV\n",
    "instrumental_classes = get_instrumental_classes(root_dir)\n",
    "save_track_classes_to_csv(instrumental_classes)\n",
    "save_global_attributes_to_csv(all_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "bcd43ecb-4388-4158-a82f-5e7afcd55a80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data saved to processed_tracks_data_final_10secs_embeddings_standardized.npy\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "\n",
    "def convert_data_for_npy(data):\n",
    "    \"\"\"Recursively convert data types in the nested dictionary for numpy compatibility.\"\"\"\n",
    "    if isinstance(data, dict):\n",
    "        return {k: convert_data_for_npy(v) for k, v in data.items()}\n",
    "    elif isinstance(data, (list, np.ndarray)):\n",
    "        return np.array([convert_data_for_npy(item) for item in data])\n",
    "    elif isinstance(data, torch.Tensor):\n",
    "        return data.detach().cpu().numpy()\n",
    "    else:\n",
    "        return data\n",
    "\n",
    "converted_data = convert_data_for_npy(all_data)\n",
    "\n",
    "output_file_name_npy = 'processed_tracks_data_final_10secs_embeddings_standardized.npy'\n",
    "np.save(output_file_name_npy, np.array(converted_data, dtype=object))\n",
    "\n",
    "print(f\"Data saved to {output_file_name_npy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bd48c0f",
   "metadata": {},
   "source": [
    "# Enhanced Audio Preprocessing with 10-Second Chunks for Richer Dataset\n",
    "This script preprocesses an audio dataset for training by dividing each audio track into 10-second chunks, significantly increasing the number of data samples compared to the earlier script, which processed only the first 10 seconds of each track. It encodes each chunk using Facebook's EnCodec model, ensuring that all audio segments are standardized in length. The script extracts metadata such as BPM, key, and bars from accompanying track_info.txt files and generates one-hot encodings for the music key and track classes.\n",
    "\n",
    "For tracks labeled as full_instrumental, the script calculates downbeats and beats using Madmom's RNN models, leveraging this information to create detailed positional embeddings for each chunk. By processing each track into multiple 10-second chunks, the script provides a richer dataset with finer granularity, enhancing its utility for machine learning models. Each chunk's data, including audio encodings, positional embeddings, and associated metadata, is aggregated and saved for efficient downstream usage, enabling better model training outcomes. This approach contrasts with the first script by maximizing data diversity and volume while maintaining temporal and structural consistency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "39070301",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration. Please open a PR/issue to update `preprocessor_config.json` to use `image_processor_type` instead of `feature_extractor_type`. This warning will be removed in v4.40.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DEBUG] Final positional embeddings shape: (750, 128)\n",
      "[DEBUG] Number of zero elements in final positional embeddings: 3232\n",
      "[DEBUG] Final positional embeddings shape: (750, 128)\n",
      "[DEBUG] Number of zero elements in final positional embeddings: 7744\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [27], line 244\u001b[0m\n\u001b[0;32m    242\u001b[0m \u001b[38;5;66;03m# Example Usage\u001b[39;00m\n\u001b[0;32m    243\u001b[0m root_dir \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m../processed_dataset\u001b[39m\u001b[38;5;124m'\u001b[39m  \u001b[38;5;66;03m# Update path as needed\u001b[39;00m\n\u001b[1;32m--> 244\u001b[0m all_data \u001b[38;5;241m=\u001b[39m \u001b[43mprocess_tracks_with_chunks\u001b[49m\u001b[43m(\u001b[49m\u001b[43mroot_dir\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    246\u001b[0m all_data\n",
      "Cell \u001b[1;32mIn [27], line 229\u001b[0m, in \u001b[0;36mprocess_tracks_with_chunks\u001b[1;34m(root_dir, fps_beats, fps_positional, chunk_duration)\u001b[0m\n\u001b[0;32m    227\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m root, _, files \u001b[38;5;129;01min\u001b[39;00m os\u001b[38;5;241m.\u001b[39mwalk(root_dir):\n\u001b[0;32m    228\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrack_info.txt\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m files:\n\u001b[1;32m--> 229\u001b[0m         folder_samples \u001b[38;5;241m=\u001b[39m \u001b[43mprocess_folder_with_chunks\u001b[49m\u001b[43m(\u001b[49m\u001b[43mroot\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minstrumental_classes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfps_beats\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfps_positional\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchunk_duration\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    231\u001b[0m         \u001b[38;5;66;03m# Assign unique IDs to each sample\u001b[39;00m\n\u001b[0;32m    232\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m sample \u001b[38;5;129;01min\u001b[39;00m folder_samples:\n",
      "Cell \u001b[1;32mIn [27], line 191\u001b[0m, in \u001b[0;36mprocess_folder_with_chunks\u001b[1;34m(folder_path, instrumental_classes, fps_beats, fps_positional, chunk_duration)\u001b[0m\n\u001b[0;32m    188\u001b[0m track_one_hot \u001b[38;5;241m=\u001b[39m one_hot_encode_track_classes([track_class], instrumental_classes)\n\u001b[0;32m    190\u001b[0m \u001b[38;5;66;03m# Process chunks for the current track_class\u001b[39;00m\n\u001b[1;32m--> 191\u001b[0m samples \u001b[38;5;241m=\u001b[39m \u001b[43mprocess_audio_chunks\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    192\u001b[0m \u001b[43m    \u001b[49m\u001b[43maudio_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    193\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrack_class\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    194\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_full_instrumental\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    195\u001b[0m \u001b[43m    \u001b[49m\u001b[43mglobal_attributes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    196\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkey_one_hot\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    197\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrack_one_hot\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    198\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfps_beats\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    199\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfps_positional\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    200\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchunk_duration\u001b[49m\n\u001b[0;32m    201\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    203\u001b[0m \u001b[38;5;66;03m# Aggregate data across track_classes for the same chunk index\u001b[39;00m\n\u001b[0;32m    204\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m sample \u001b[38;5;129;01min\u001b[39;00m samples:\n",
      "Cell \u001b[1;32mIn [27], line 145\u001b[0m, in \u001b[0;36mprocess_audio_chunks\u001b[1;34m(audio_path, track_class, is_full_instrumental, global_attributes, key_one_hot, track_one_hot, fps_beats, fps_positional, chunk_duration)\u001b[0m\n\u001b[0;32m    143\u001b[0m \u001b[38;5;66;03m# Compute beats and downbeats\u001b[39;00m\n\u001b[0;32m    144\u001b[0m downbeats \u001b[38;5;241m=\u001b[39m extract_downbeats(temp_audio_path, fps\u001b[38;5;241m=\u001b[39mfps_beats)\n\u001b[1;32m--> 145\u001b[0m beats \u001b[38;5;241m=\u001b[39m \u001b[43mextract_beats\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtemp_audio_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfps_beats\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    146\u001b[0m os\u001b[38;5;241m.\u001b[39mremove(temp_audio_path)\n\u001b[0;32m    148\u001b[0m \u001b[38;5;66;03m# Compute positional embedding for the chunk\u001b[39;00m\n",
      "Cell \u001b[1;32mIn [27], line 66\u001b[0m, in \u001b[0;36mextract_beats\u001b[1;34m(audio_path, fps)\u001b[0m\n\u001b[0;32m     65\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mextract_beats\u001b[39m(audio_path, fps\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m):\n\u001b[1;32m---> 66\u001b[0m     beat_act \u001b[38;5;241m=\u001b[39m \u001b[43mmadmom\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeatures\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbeats\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mRNNBeatProcessor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfps\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43maudio_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     67\u001b[0m     beat_proc \u001b[38;5;241m=\u001b[39m madmom\u001b[38;5;241m.\u001b[39mfeatures\u001b[38;5;241m.\u001b[39mbeats\u001b[38;5;241m.\u001b[39mBeatDetectionProcessor(fps\u001b[38;5;241m=\u001b[39mfps)\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m beat_proc(beat_act)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\madmom\\processors.py:120\u001b[0m, in \u001b[0;36mProcessor.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    118\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    119\u001b[0m     \u001b[38;5;66;03m# this magic method makes a Processor callable\u001b[39;00m\n\u001b[1;32m--> 120\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocess(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\madmom\\processors.py:424\u001b[0m, in \u001b[0;36mSequentialProcessor.process\u001b[1;34m(self, data, **kwargs)\u001b[0m\n\u001b[0;32m    422\u001b[0m \u001b[38;5;66;03m# sequentially process the data\u001b[39;00m\n\u001b[0;32m    423\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m processor \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocessors:\n\u001b[1;32m--> 424\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[43m_process\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocessor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    425\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\madmom\\processors.py:289\u001b[0m, in \u001b[0;36m_process\u001b[1;34m(process_tuple)\u001b[0m\n\u001b[0;32m    287\u001b[0m \u001b[38;5;66;03m# call the Processor with data and kwargs\u001b[39;00m\n\u001b[0;32m    288\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(process_tuple[\u001b[38;5;241m0\u001b[39m], Processor):\n\u001b[1;32m--> 289\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m process_tuple[\u001b[38;5;241m0\u001b[39m](\u001b[38;5;241m*\u001b[39mprocess_tuple[\u001b[38;5;241m1\u001b[39m:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m], \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mprocess_tuple[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n\u001b[0;32m    290\u001b[0m \u001b[38;5;66;03m# just call whatever we got here (e.g. a function) without kwargs\u001b[39;00m\n\u001b[0;32m    291\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m process_tuple[\u001b[38;5;241m0\u001b[39m](\u001b[38;5;241m*\u001b[39mprocess_tuple[\u001b[38;5;241m1\u001b[39m:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\madmom\\processors.py:120\u001b[0m, in \u001b[0;36mProcessor.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    118\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    119\u001b[0m     \u001b[38;5;66;03m# this magic method makes a Processor callable\u001b[39;00m\n\u001b[1;32m--> 120\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocess(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\madmom\\processors.py:424\u001b[0m, in \u001b[0;36mSequentialProcessor.process\u001b[1;34m(self, data, **kwargs)\u001b[0m\n\u001b[0;32m    422\u001b[0m \u001b[38;5;66;03m# sequentially process the data\u001b[39;00m\n\u001b[0;32m    423\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m processor \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocessors:\n\u001b[1;32m--> 424\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[43m_process\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocessor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    425\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\madmom\\processors.py:289\u001b[0m, in \u001b[0;36m_process\u001b[1;34m(process_tuple)\u001b[0m\n\u001b[0;32m    287\u001b[0m \u001b[38;5;66;03m# call the Processor with data and kwargs\u001b[39;00m\n\u001b[0;32m    288\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(process_tuple[\u001b[38;5;241m0\u001b[39m], Processor):\n\u001b[1;32m--> 289\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m process_tuple[\u001b[38;5;241m0\u001b[39m](\u001b[38;5;241m*\u001b[39mprocess_tuple[\u001b[38;5;241m1\u001b[39m:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m], \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mprocess_tuple[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n\u001b[0;32m    290\u001b[0m \u001b[38;5;66;03m# just call whatever we got here (e.g. a function) without kwargs\u001b[39;00m\n\u001b[0;32m    291\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m process_tuple[\u001b[38;5;241m0\u001b[39m](\u001b[38;5;241m*\u001b[39mprocess_tuple[\u001b[38;5;241m1\u001b[39m:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\madmom\\processors.py:120\u001b[0m, in \u001b[0;36mProcessor.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    118\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    119\u001b[0m     \u001b[38;5;66;03m# this magic method makes a Processor callable\u001b[39;00m\n\u001b[1;32m--> 120\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocess(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\madmom\\processors.py:484\u001b[0m, in \u001b[0;36mParallelProcessor.process\u001b[1;34m(self, data, **kwargs)\u001b[0m\n\u001b[0;32m    482\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [_process((\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocessors[\u001b[38;5;241m0\u001b[39m], data, kwargs))]\n\u001b[0;32m    483\u001b[0m \u001b[38;5;66;03m# process data in parallel and return a list with processed data\u001b[39;00m\n\u001b[1;32m--> 484\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_process\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mzip\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprocessors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mit\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrepeat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    485\u001b[0m \u001b[43m                                   \u001b[49m\u001b[43mit\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrepeat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\madmom\\processors.py:289\u001b[0m, in \u001b[0;36m_process\u001b[1;34m(process_tuple)\u001b[0m\n\u001b[0;32m    287\u001b[0m \u001b[38;5;66;03m# call the Processor with data and kwargs\u001b[39;00m\n\u001b[0;32m    288\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(process_tuple[\u001b[38;5;241m0\u001b[39m], Processor):\n\u001b[1;32m--> 289\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m process_tuple[\u001b[38;5;241m0\u001b[39m](\u001b[38;5;241m*\u001b[39mprocess_tuple[\u001b[38;5;241m1\u001b[39m:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m], \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mprocess_tuple[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n\u001b[0;32m    290\u001b[0m \u001b[38;5;66;03m# just call whatever we got here (e.g. a function) without kwargs\u001b[39;00m\n\u001b[0;32m    291\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m process_tuple[\u001b[38;5;241m0\u001b[39m](\u001b[38;5;241m*\u001b[39mprocess_tuple[\u001b[38;5;241m1\u001b[39m:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\madmom\\processors.py:120\u001b[0m, in \u001b[0;36mProcessor.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    118\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    119\u001b[0m     \u001b[38;5;66;03m# this magic method makes a Processor callable\u001b[39;00m\n\u001b[1;32m--> 120\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocess(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\madmom\\ml\\nn\\__init__.py:101\u001b[0m, in \u001b[0;36mNeuralNetwork.process\u001b[1;34m(self, data, reset, **kwargs)\u001b[0m\n\u001b[0;32m     98\u001b[0m \u001b[38;5;66;03m# loop over all layers\u001b[39;00m\n\u001b[0;32m     99\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers:\n\u001b[0;32m    100\u001b[0m     \u001b[38;5;66;03m# activate the layer and feed the output into the next one\u001b[39;00m\n\u001b[1;32m--> 101\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[43mlayer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mactivate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    102\u001b[0m \u001b[38;5;66;03m# ravel the predictions if needed\u001b[39;00m\n\u001b[0;32m    103\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m data\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m data\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\madmom\\ml\\nn\\layers.py:225\u001b[0m, in \u001b[0;36mmadmom.ml.nn.layers.BidirectionalLayer.activate\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\madmom\\ml\\nn\\layers.py:28\u001b[0m, in \u001b[0;36mmadmom.ml.nn.layers.Layer.__call__\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\madmom\\ml\\nn\\layers.py:436\u001b[0m, in \u001b[0;36mmadmom.ml.nn.layers.LSTMLayer.activate\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\madmom\\ml\\nn\\layers.py:285\u001b[0m, in \u001b[0;36mmadmom.ml.nn.layers.Gate.activate\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m<__array_function__ internals>:180\u001b[0m, in \u001b[0;36mdot\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torchaudio\n",
    "import torch\n",
    "from transformers import AutoProcessor, EncodecModel\n",
    "import madmom\n",
    "import tempfile\n",
    "\n",
    "# Define helper functions\n",
    "def get_global_attributes(track_info_path):\n",
    "    global_attributes = {}\n",
    "    with open(track_info_path, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "        for line in lines:\n",
    "            if ':' in line:\n",
    "                key, value = line.strip().split(':', 1)\n",
    "                global_attributes[key.strip().lower()] = value.strip()\n",
    "    return global_attributes\n",
    "\n",
    "def define_keys():\n",
    "    major_keys = [note + \"_MAJOR\" for note in [\"C\", \"C#\", \"D\", \"D#\", \"E\", \"F\", \"F#\", \"G\", \"G#\", \"A\", \"A#\", \"B\"]]\n",
    "    minor_keys = [note + \"_MINOR\" for note in [\"C\", \"C#\", \"D\", \"D#\", \"E\", \"F\", \"F#\", \"G\", \"G#\", \"A\", \"A#\", \"B\"]]\n",
    "    return major_keys + minor_keys\n",
    "\n",
    "def one_hot_encode_keys(key, all_keys):\n",
    "    one_hot_vector = np.zeros(len(all_keys))\n",
    "    if key in all_keys:\n",
    "        index = all_keys.index(key)\n",
    "        one_hot_vector[index] = 1\n",
    "    return one_hot_vector\n",
    "\n",
    "def get_instrumental_classes(root_dir):\n",
    "    instrumental_classes = set()\n",
    "    for root, _, files in os.walk(root_dir):\n",
    "        if \"track_info.txt\" in files:\n",
    "            with open(os.path.join(root, \"track_info.txt\"), 'r') as f:\n",
    "                lines = f.readlines()\n",
    "                for line in lines:\n",
    "                    if ',' in line:\n",
    "                        _, attribute = [part.strip() for part in line.split(',', 1)]\n",
    "                        if 'vocal' not in attribute.lower():\n",
    "                            instrumental_classes.add(attribute)\n",
    "    return sorted(instrumental_classes)\n",
    "\n",
    "def one_hot_encode_track_classes(track_classes, instrumental_classes):\n",
    "    one_hot_vectors = np.zeros(len(instrumental_classes))\n",
    "    for track_class in track_classes:\n",
    "        if track_class in instrumental_classes:\n",
    "            index = instrumental_classes.index(track_class)\n",
    "            one_hot_vectors[index] = 1\n",
    "    return one_hot_vectors\n",
    "\n",
    "def split_audio_into_chunks(audio, rate, chunk_duration=10):\n",
    "    chunk_length = int(chunk_duration * rate)\n",
    "    num_chunks = audio.shape[1] // chunk_length\n",
    "    chunks = [audio[:, i * chunk_length:(i + 1) * chunk_length] for i in range(num_chunks)]\n",
    "    return chunks\n",
    "\n",
    "def extract_downbeats(audio_path, fps=100):\n",
    "    proc = madmom.features.downbeats.DBNDownBeatTrackingProcessor(beats_per_bar=[4], fps=fps)\n",
    "    act = madmom.features.downbeats.RNNDownBeatProcessor(fps=fps)(audio_path)\n",
    "    downbeats = proc(act)\n",
    "    return downbeats[downbeats[:, 1] == 1, 0]\n",
    "\n",
    "def extract_beats(audio_path, fps=100):\n",
    "    beat_act = madmom.features.beats.RNNBeatProcessor(fps=fps)(audio_path)\n",
    "    beat_proc = madmom.features.beats.BeatDetectionProcessor(fps=fps)\n",
    "    return beat_proc(beat_act)\n",
    "\n",
    "def create_positional_embeddings(beat_times, downbeat_times, audio_duration, fps=75, K=32):\n",
    "    total_frames = int(np.ceil(audio_duration * fps))\n",
    "    \n",
    "    def ramps(positions, size):\n",
    "        result = np.zeros(size)\n",
    "        for a, b in zip(positions[:-1], positions[1:]):\n",
    "            result[a:b] = np.linspace(0, 1, b - a, endpoint=False)\n",
    "        return result\n",
    "\n",
    "    time_vector = np.linspace(0, audio_duration, total_frames)\n",
    "    vector_downbeat = ramps((downbeat_times * fps).astype(int), total_frames)\n",
    "    vector_beat = ramps((beat_times * fps).astype(int), total_frames)\n",
    "\n",
    "    frequencies = np.arange(1, K + 1)\n",
    "    embeddings_downbeat = []\n",
    "    embeddings_beat = []\n",
    "\n",
    "    for k in frequencies:\n",
    "        embeddings_downbeat.append(np.sin(2 * np.pi * vector_downbeat * k))\n",
    "        embeddings_downbeat.append(np.cos(2 * np.pi * vector_downbeat * k))\n",
    "        embeddings_beat.append(np.sin(2 * np.pi * vector_beat * k))\n",
    "        embeddings_beat.append(np.cos(2 * np.pi * vector_beat * k))\n",
    "\n",
    "    embeddings_downbeat = np.stack(embeddings_downbeat, axis=1)\n",
    "    embeddings_beat = np.stack(embeddings_beat, axis=1)\n",
    "    embeddings = np.hstack((embeddings_downbeat, embeddings_beat))\n",
    "    \n",
    "    num_zeros = np.sum(embeddings == 0)\n",
    "    print(f\"[DEBUG] Final positional embeddings shape: {embeddings.shape}\")\n",
    "    print(f\"[DEBUG] Number of zero elements in final positional embeddings: {num_zeros}\")\n",
    "\n",
    "    return torch.from_numpy(embeddings).float()\n",
    "\n",
    "def encode_audio_chunk(audio_chunk, rate):\n",
    "    # Ensure the audio has a single channel\n",
    "    if audio_chunk.ndim > 1:\n",
    "        audio_chunk = audio_chunk[0]\n",
    "    \n",
    "    audio_np = audio_chunk.numpy()\n",
    "    inputs = processor(audio_np, sampling_rate=rate, return_tensors=\"pt\")\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.encode(inputs[\"input_values\"], inputs[\"padding_mask\"], 3)\n",
    "    return outputs.audio_codes.squeeze()\n",
    "\n",
    "def process_audio_chunks(audio_path, track_class, is_full_instrumental, global_attributes, key_one_hot, track_one_hot, fps_beats=100, fps_positional=75, chunk_duration=10):\n",
    "    \"\"\"\n",
    "    Processes audio chunks for a specific track_class and ensures all track_classes' audio codes\n",
    "    are accumulated in generation_data.\n",
    "    \"\"\"\n",
    "    audio, rate = torchaudio.load(audio_path)\n",
    "    chunks = split_audio_into_chunks(audio, rate, chunk_duration)\n",
    "    samples = []\n",
    "    \n",
    "    for i, chunk in enumerate(chunks):\n",
    "        audio_code = encode_audio_chunk(chunk, rate)\n",
    "        if audio_code.shape[1] == int(chunk_duration * fps_positional):  # Valid 10-second sample\n",
    "            sample = {\n",
    "                'generation_data': {},  # Initialize empty dict for generation_data\n",
    "                'chunk_index': i,\n",
    "                'bars': global_attributes.get('bars', 0),\n",
    "                'key_one_hot': key_one_hot,\n",
    "                'track_one_hot': track_one_hot\n",
    "            }\n",
    "            \n",
    "            # Add the current track_class audio code to generation_data\n",
    "            sample['generation_data'][track_class] = np.array(audio_code)\n",
    "            \n",
    "            if is_full_instrumental:  # Compute positional embeddings only for full_instrumental\n",
    "                with tempfile.NamedTemporaryFile(delete=False, suffix='.wav') as temp_audio_file:\n",
    "                    temp_audio_path = temp_audio_file.name\n",
    "                    torchaudio.save(temp_audio_path, chunk, rate)\n",
    "                \n",
    "                # Compute beats and downbeats\n",
    "                downbeats = extract_downbeats(temp_audio_path, fps=fps_beats)\n",
    "                beats = extract_beats(temp_audio_path, fps=fps_beats)\n",
    "                os.remove(temp_audio_path)\n",
    "                \n",
    "                # Compute positional embedding for the chunk\n",
    "                positional_embedding = create_positional_embeddings(beats, downbeats, chunk_duration, fps=fps_positional)\n",
    "                sample['positional_embedding'] = np.array(positional_embedding)\n",
    "            \n",
    "            samples.append(sample)\n",
    "    \n",
    "    return samples\n",
    "\n",
    "\n",
    "\n",
    "def process_folder_with_chunks(folder_path, instrumental_classes, fps_beats=100, fps_positional=75, chunk_duration=10):\n",
    "    \"\"\"\n",
    "    Processes a folder and outputs individual chunks as separate samples with generation_data.\n",
    "    Ensures that all track_classes' audio codes are accumulated in generation_data.\n",
    "    \"\"\"\n",
    "    all_samples = []\n",
    "    track_info_path = os.path.join(folder_path, \"track_info.txt\")\n",
    "    global_attributes = get_global_attributes(track_info_path)\n",
    "    all_keys = define_keys()\n",
    "    \n",
    "    music_key = global_attributes.get('key', \"\").replace(\" \", \"_\").upper()\n",
    "    key_one_hot = one_hot_encode_keys(music_key, all_keys)\n",
    "    \n",
    "    filename_to_track_class = {}\n",
    "    with open(track_info_path, 'r') as f:\n",
    "        for line in f.readlines():\n",
    "            if ',' in line:\n",
    "                file_name, track_class = [part.strip() for part in line.split(',', 1)]\n",
    "                filename_to_track_class[file_name] = track_class\n",
    "\n",
    "    # Process 'full_instrumental' track first to ensure positional_embedding is included\n",
    "    track_classes_ordered = sorted(filename_to_track_class.items(), key=lambda x: 0 if x[1].lower() == 'full_instrumental' else 1)\n",
    "    \n",
    "    # Initialize an empty dictionary to accumulate data for each chunk index\n",
    "    chunk_samples = {}\n",
    "    \n",
    "    for file_name, track_class in track_classes_ordered:\n",
    "        audio_path = os.path.join(folder_path, file_name)\n",
    "        if os.path.exists(audio_path) and file_name.endswith(\".wav\"):\n",
    "            is_full_instrumental = track_class.lower() == 'full_instrumental'\n",
    "            track_one_hot = one_hot_encode_track_classes([track_class], instrumental_classes)\n",
    "            \n",
    "            # Process chunks for the current track_class\n",
    "            samples = process_audio_chunks(\n",
    "                audio_path,\n",
    "                track_class,\n",
    "                is_full_instrumental,\n",
    "                global_attributes,\n",
    "                key_one_hot,\n",
    "                track_one_hot,\n",
    "                fps_beats,\n",
    "                fps_positional,\n",
    "                chunk_duration\n",
    "            )\n",
    "            \n",
    "            # Aggregate data across track_classes for the same chunk index\n",
    "            for sample in samples:\n",
    "                chunk_index = sample['chunk_index']\n",
    "                if chunk_index not in chunk_samples:\n",
    "                    chunk_samples[chunk_index] = sample\n",
    "                else:\n",
    "                    # Merge generation_data from the new sample into existing one\n",
    "                    chunk_samples[chunk_index]['generation_data'].update(sample['generation_data'])\n",
    "                    # Update positional_embedding if present\n",
    "                    if 'positional_embedding' in sample:\n",
    "                        chunk_samples[chunk_index]['positional_embedding'] = sample['positional_embedding']\n",
    "    \n",
    "    # Convert aggregated samples back to a list\n",
    "    all_samples = list(chunk_samples.values())\n",
    "    return all_samples\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def process_tracks_with_chunks(root_dir, fps_beats=100, fps_positional=75, chunk_duration=10):\n",
    "    instrumental_classes = get_instrumental_classes(root_dir)\n",
    "    all_samples = {}\n",
    "    sample_id = 0\n",
    "    \n",
    "    for root, _, files in os.walk(root_dir):\n",
    "        if \"track_info.txt\" in files:\n",
    "            folder_samples = process_folder_with_chunks(root, instrumental_classes, fps_beats, fps_positional, chunk_duration)\n",
    "            \n",
    "            # Assign unique IDs to each sample\n",
    "            for sample in folder_samples:\n",
    "                all_samples[f\"sample_{sample_id}\"] = sample\n",
    "                sample_id += 1\n",
    "    \n",
    "    return all_samples\n",
    "\n",
    "# Initialize Encodec Processor and Model\n",
    "processor = AutoProcessor.from_pretrained(\"facebook/encodec_24khz\")\n",
    "model = EncodecModel.from_pretrained(\"facebook/encodec_24khz\")\n",
    "\n",
    "# Example Usage\n",
    "root_dir = '../processed_dataset'  # Update path as needed\n",
    "all_data = process_tracks_with_chunks(root_dir)\n",
    "\n",
    "all_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b3b19de0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data saved to fulldataset_10sec_positional_embs.npy\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "def convert_to_npy(data):\n",
    "    \"\"\"\n",
    "    Recursively converts the output data structure into a numpy-compatible format\n",
    "    for saving to .npy files.\n",
    "    \"\"\"\n",
    "    if isinstance(data, dict):\n",
    "        # Recursively process dictionary values\n",
    "        return {key: convert_to_npy(value) for key, value in data.items()}\n",
    "    elif isinstance(data, (list, tuple)):\n",
    "        # Convert lists/tuples to numpy arrays\n",
    "        return np.array([convert_to_npy(item) for item in data], dtype=object)\n",
    "    elif isinstance(data, torch.Tensor):\n",
    "        # Convert torch.Tensor to numpy array\n",
    "        return data.detach().cpu().numpy()\n",
    "    elif isinstance(data, np.ndarray):\n",
    "        # If already a numpy array, return as is\n",
    "        return data\n",
    "    else:\n",
    "        # Scalars and other types (e.g., int, float, str)\n",
    "        return data\n",
    "\n",
    "    \n",
    "# Convert the processed data into a numpy-compatible format\n",
    "converted_data = convert_to_npy(all_data)\n",
    "\n",
    "# Save the converted data to a .npy file\n",
    "output_file_name = 'fulldataset_10sec_positional_embs.npy'\n",
    "np.save(output_file_name, converted_data)\n",
    "\n",
    "print(f\"Data saved to {output_file_name}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61f65c86",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
