{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fa8b5327",
   "metadata": {},
   "source": [
    "# Joint Music Track Generation with GPT-2, Weights & Biases Tracking, and Periodic Checkpointing\n",
    "\n",
    "This notebook trains a conditional GPT-2 model for music generation with a joint approach across multiple track classes. Each track type is identified by a unique instrument token embedded periodically throughout the sequence, enabling the model to learn different track classes within a single unified framework. The dataset pre-filters valid samples for each track, ensuring that only meaningful data is used for training. The model is configured to handle track class conditioning, and it combines positional embeddings and instrument tokens with the input embeddings to conditionally generate each track type.\n",
    "\n",
    "The script also integrates Weights & Biases (W&B) for real-time tracking of training metrics, providing an online dashboard for monitoring model performance. The data is split into train and validation sets to allow periodic evaluation, giving a more comprehensive view of model generalization across track types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "91332ab9",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [1], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mwandb\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Dataset\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\__init__.py:960\u001b[0m\n\u001b[0;32m    958\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m sys\u001b[38;5;241m.\u001b[39mexecutable \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtorch_deploy\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m    959\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m library\n\u001b[1;32m--> 960\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _meta_registrations\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\_meta_registrations.py:2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_prims\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m utils\n\u001b[0;32m      4\u001b[0m meta_lib \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mlibrary\u001b[38;5;241m.\u001b[39mLibrary(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maten\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIMPL\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMeta\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcheck\u001b[39m(b, s):\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\_prims\\__init__.py:4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Tensor, _TypedStorage\n\u001b[1;32m----> 4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_prims\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mutils\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_prims\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m      6\u001b[0m     TensorLike,\n\u001b[0;32m      7\u001b[0m     TensorLikeType,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     15\u001b[0m     NumberType,\n\u001b[0;32m     16\u001b[0m )\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01moverrides\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m has_torch_function, handle_torch_function\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\_prims\\utils.py:11\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# nvFuser imports are conditional on CUDA being available\u001b[39;00m\n\u001b[1;32m---> 11\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_available\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m     12\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_C\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_nvfuser\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DataType  \u001b[38;5;66;03m# type: ignore[import]\u001b[39;00m\n\u001b[0;32m     14\u001b[0m     _torch_dtype_to_nvfuser_dtype_map \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m     15\u001b[0m         torch\u001b[38;5;241m.\u001b[39mcdouble: DataType\u001b[38;5;241m.\u001b[39mComplexDouble,\n\u001b[0;32m     16\u001b[0m         torch\u001b[38;5;241m.\u001b[39mcfloat: DataType\u001b[38;5;241m.\u001b[39mComplexFloat,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     23\u001b[0m         torch\u001b[38;5;241m.\u001b[39mbool: DataType\u001b[38;5;241m.\u001b[39mBool,\n\u001b[0;32m     24\u001b[0m     }\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\cuda\\__init__.py:83\u001b[0m, in \u001b[0;36mis_available\u001b[1;34m()\u001b[0m\n\u001b[0;32m     80\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m     81\u001b[0m \u001b[38;5;66;03m# This function never throws and returns 0 if driver is missing or can't\u001b[39;00m\n\u001b[0;32m     82\u001b[0m \u001b[38;5;66;03m# be initialized\u001b[39;00m\n\u001b[1;32m---> 83\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cuda_getDeviceCount\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m>\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import wandb\n",
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset\n",
    "from transformers import GPT2LMHeadModel, GPT2Config, Trainer, TrainingArguments, TrainerCallback\n",
    "import torch.nn as nn\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Set the device to CUDA if available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "\n",
    "print(torch.cuda.is_available())  # Should return True if a GPU is detected\n",
    "print(torch.cuda.device_count())  # Should return the number of GPUs detected\n",
    "print(torch.cuda.get_device_name(0))\n",
    "\n",
    "# Load your saved .npy file\n",
    "data = np.load('fulldataset_10sec_positional_embs.npy', allow_pickle=True).item()\n",
    "\n",
    "VOCAB_SIZE = 1034\n",
    "MAX_LENGTH = 3000  # Adjusted for maximum length\n",
    "track_classes = ['hi_hat', 'kick', 'snare', 'clap', 'bass', 'drums', 'keys', 'full_instrumental']  # Example track classes\n",
    "\n",
    "# Map each track class to a unique token ID for conditioning\n",
    "instrument_token_map = {\n",
    "    'hi_hat': 1027,\n",
    "    'kick': 1028,\n",
    "    'snare': 1029,\n",
    "    'clap': 1030,\n",
    "    'bass': 1031,\n",
    "    'drums': 1032,\n",
    "    'keys': 1032,\n",
    "    'full_instrumental': 1033\n",
    "}\n",
    "\n",
    "wandb_run_name = f'joint_track_generation_gpt2_training_pos_emb_concat_run_fulldataset_10sec_embeddings_final'\n",
    "\n",
    "wandb.init(project=\"music-generation\", name=wandb_run_name, )\n",
    "\n",
    "class MusicDataset(Dataset):\n",
    "    def __init__(self, data, instrument_token_map):\n",
    "        self.data = data\n",
    "        self.instrument_token_map = instrument_token_map\n",
    "\n",
    "        # Initialize a dictionary to store counts of valid samples per track class\n",
    "        self.track_class_counts = {track_class: 0 for track_class in track_classes}\n",
    "\n",
    "        # Precompute a list of (sample_id, track_class) pairs that have valid data\n",
    "        self.valid_pairs = []\n",
    "        for sample_id, sample in self.data.items():\n",
    "            for track_class in track_classes:\n",
    "                if track_class in sample['generation_data'] and np.any(sample['generation_data'][track_class] != 0):\n",
    "                    self.valid_pairs.append((sample_id, track_class))\n",
    "                    self.track_class_counts[track_class] += 1\n",
    "\n",
    "        # Log the number of valid samples for each track class\n",
    "        print(\"Valid sample counts per track class:\")\n",
    "        for track_class, count in self.track_class_counts.items():\n",
    "            print(f\"{track_class}: {count} samples\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.valid_pairs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Get the (sample_id, track_class) pair for this index\n",
    "        sample_id, track_class = self.valid_pairs[idx]\n",
    "        sample = self.data[sample_id]\n",
    "\n",
    "        # Retrieve the specific track data\n",
    "        vocal_audio_codes = sample['generation_data'].get('vocal', np.zeros((4, 750)))\n",
    "        track_data = sample['generation_data'][track_class]\n",
    "\n",
    "        # Retrieve positional embeddings\n",
    "        positional_embedding = sample.get('positional_embedding', np.zeros((4, 750)))\n",
    "\n",
    "        # Ensure vocal_audio_codes is clipped\n",
    "        vocal_audio_codes = np.clip(vocal_audio_codes, 0, VOCAB_SIZE - 1)\n",
    "\n",
    "        # Flatten and pad/truncate sequences\n",
    "        vocal_audio_codes = np.pad(vocal_audio_codes.flatten(), (0, MAX_LENGTH - len(vocal_audio_codes.flatten())), 'constant', constant_values=(0, 0))[:MAX_LENGTH]\n",
    "        track_data = np.pad(track_data.flatten(), (0, MAX_LENGTH - len(track_data.flatten())), 'constant', constant_values=(0, 0))[:MAX_LENGTH]\n",
    "\n",
    "        # Generate attention mask\n",
    "        attention_mask = (vocal_audio_codes != 0).astype(int)\n",
    "\n",
    "        # Pad positional embeddings\n",
    "        padding_length = MAX_LENGTH - positional_embedding.shape[0]\n",
    "        positional_embedding = np.pad(positional_embedding, ((0, padding_length), (0, 0)), mode='constant', constant_values=(0, 0))\n",
    "\n",
    "        # Repeat the instrument token throughout the sequence at regular intervals\n",
    "        interval = 50  # Define the interval at which to repeat the token\n",
    "        instrument_token = np.full(MAX_LENGTH, self.instrument_token_map[track_class], dtype=int)\n",
    "        instrument_token[::interval] = self.instrument_token_map[track_class]\n",
    "\n",
    "        return {\n",
    "            'input_ids': torch.tensor(vocal_audio_codes, dtype=torch.long).to(device),\n",
    "            'attention_mask': torch.tensor(attention_mask, dtype=torch.long).to(device),\n",
    "            'labels': torch.tensor(track_data, dtype=torch.long).to(device),\n",
    "            'positional_embeddings': torch.tensor(positional_embedding, dtype=torch.float).to(device),\n",
    "            'instrument_token': torch.tensor(instrument_token, dtype=torch.long).to(device),\n",
    "            'track_class': track_class,  # Track the class for generation\n",
    "            'sample_id': sample_id  # Track the sample ID for caching\n",
    "        }\n",
    "\n",
    "class CustomGPT2ForConditionalGeneration(GPT2LMHeadModel):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.projection = nn.Linear(config.n_embd * 2, config.n_embd)\n",
    "\n",
    "    def forward(self, input_ids=None, attention_mask=None, labels=None, positional_embeddings=None, instrument_token=None, **kwargs):\n",
    "        # Get input embeddings\n",
    "        input_embeds = self.transformer.wte(input_ids)\n",
    "\n",
    "        # Combine positional embeddings with input embeddings\n",
    "        embs_dim = positional_embeddings.shape[2]\n",
    "        input_embeds = torch.cat((input_embeds[:, :, :-embs_dim], input_embeds[:, :, -embs_dim:] + positional_embeddings), dim=-1)\n",
    "\n",
    "        # Expand instrument token embeddings and add them to the input\n",
    "        instrument_embeds = self.transformer.wte(instrument_token)\n",
    "        combined_input = input_embeds + instrument_embeds\n",
    "\n",
    "        # Forward pass through the GPT model\n",
    "        return super().forward(inputs_embeds=combined_input, attention_mask=attention_mask, labels=labels, **kwargs)\n",
    "\n",
    "# Configuration for the model\n",
    "config = GPT2Config(\n",
    "    vocab_size=VOCAB_SIZE,\n",
    "    n_positions=MAX_LENGTH,\n",
    "    n_ctx=MAX_LENGTH,\n",
    "    n_embd=128,  # Match with your previous d_model\n",
    "    n_layer=6,\n",
    "    n_head=8,\n",
    "    activation_function='gelu',\n",
    "    resid_pdrop=0.1,\n",
    "    embd_pdrop=0.1,\n",
    "    attn_pdrop=0.1,\n",
    ")\n",
    "\n",
    "# Initialize the custom GPT-2 model with regular instrument token conditioning\n",
    "model = CustomGPT2ForConditionalGeneration(config=config).to(device)\n",
    "\n",
    "# Training arguments (set to evaluate/save every 10 epochs)\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=1e-4,\n",
    "    per_device_train_batch_size=1,\n",
    "    per_device_eval_batch_size=1,\n",
    "    num_train_epochs=50,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=3,\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=10,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"loss\",\n",
    "    greater_is_better=False,\n",
    "    fp16=torch.cuda.is_available(),\n",
    "    dataloader_pin_memory=False,\n",
    "    report_to=[\"wandb\"],\n",
    ")\n",
    "\n",
    "# Custom data collator to handle positional embeddings and instrument tokens\n",
    "class DataCollatorWithPositionalEmbeddings:\n",
    "    def __call__(self, batch):\n",
    "        input_ids = torch.stack([item['input_ids'] for item in batch])\n",
    "        attention_mask = torch.stack([item['attention_mask'] for item in batch])\n",
    "        labels = torch.stack([item['labels'] for item in batch])\n",
    "        positional_embeddings = torch.stack([item['positional_embeddings'] for item in batch])\n",
    "        instrument_tokens = torch.stack([item['instrument_token'] for item in batch])\n",
    "\n",
    "        return {\n",
    "            'input_ids': input_ids.to(device),\n",
    "            'attention_mask': attention_mask.to(device),\n",
    "            'labels': labels.to(device),\n",
    "            'positional_embeddings': positional_embeddings.to(device),\n",
    "            'instrument_token': instrument_tokens.to(device),\n",
    "        }\n",
    "\n",
    "data_collator = DataCollatorWithPositionalEmbeddings()\n",
    "\n",
    "# Initialize Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "# Create dataset\n",
    "dataset = MusicDataset(data, instrument_token_map)\n",
    "\n",
    "# Split dataset indices into train and validation sets (e.g., 80% train, 20% validation)\n",
    "train_indices, val_indices = train_test_split(range(len(dataset)), test_size=0.2, random_state=42)\n",
    "\n",
    "# Create separate train and validation datasets\n",
    "train_dataset = torch.utils.data.Subset(dataset, train_indices)\n",
    "val_dataset = torch.utils.data.Subset(dataset, val_indices)\n",
    "\n",
    "# Train the model\n",
    "trainer.train_dataset = train_dataset\n",
    "trainer.eval_dataset = val_dataset\n",
    "trainer.train()\n",
    "\n",
    "# Save the model after training\n",
    "trainer.save_model('./saved_model_joint_standard_full_dataset_final')\n",
    "print(\"Model saved.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d9f74ae",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cf542816",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration. Please open a PR/issue to update `preprocessor_config.json` to use `image_processor_type` instead of `feature_extractor_type`. This warning will be removed in v4.40.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating hi_hat...\n",
      "Generating for hi_hat...\n",
      "Instrument token map for hi_hat: 1027\n",
      "Shapes - audio_codes: torch.Size([1, 3000]), instrument_token: torch.Size([1, 3000]), attention_mask: torch.Size([1, 3000]), positional_embeddings: torch.Size([1, 3000, 128])\n",
      "generated sequence:  torch.Size([1, 3000])\n",
      "hi_hat track saved to generated_joint_model\\generated_hi_hat.wav\n",
      "Generating kick...\n",
      "Generating for kick...\n",
      "Instrument token map for kick: 1028\n",
      "Shapes - audio_codes: torch.Size([1, 3000]), instrument_token: torch.Size([1, 3000]), attention_mask: torch.Size([1, 3000]), positional_embeddings: torch.Size([1, 3000, 128])\n",
      "generated sequence:  torch.Size([1, 3000])\n",
      "kick track saved to generated_joint_model\\generated_kick.wav\n",
      "Generating snare...\n",
      "Generating for snare...\n",
      "Instrument token map for snare: 1029\n",
      "Shapes - audio_codes: torch.Size([1, 3000]), instrument_token: torch.Size([1, 3000]), attention_mask: torch.Size([1, 3000]), positional_embeddings: torch.Size([1, 3000, 128])\n",
      "generated sequence:  torch.Size([1, 3000])\n",
      "snare track saved to generated_joint_model\\generated_snare.wav\n",
      "Generating clap...\n",
      "Generating for clap...\n",
      "Instrument token map for clap: 1030\n",
      "Shapes - audio_codes: torch.Size([1, 3000]), instrument_token: torch.Size([1, 3000]), attention_mask: torch.Size([1, 3000]), positional_embeddings: torch.Size([1, 3000, 128])\n",
      "generated sequence:  torch.Size([1, 3000])\n",
      "clap track saved to generated_joint_model\\generated_clap.wav\n",
      "Generating bass...\n",
      "Generating for bass...\n",
      "Instrument token map for bass: 1031\n",
      "Shapes - audio_codes: torch.Size([1, 3000]), instrument_token: torch.Size([1, 3000]), attention_mask: torch.Size([1, 3000]), positional_embeddings: torch.Size([1, 3000, 128])\n",
      "generated sequence:  torch.Size([1, 3000])\n",
      "bass track saved to generated_joint_model\\generated_bass.wav\n",
      "Generating drums...\n",
      "Generating for drums...\n",
      "Instrument token map for drums: 1032\n",
      "Shapes - audio_codes: torch.Size([1, 3000]), instrument_token: torch.Size([1, 3000]), attention_mask: torch.Size([1, 3000]), positional_embeddings: torch.Size([1, 3000, 128])\n",
      "generated sequence:  torch.Size([1, 3000])\n",
      "drums track saved to generated_joint_model\\generated_drums.wav\n",
      "Generating keys...\n",
      "Generating for keys...\n",
      "Instrument token map for keys: 1032\n",
      "Shapes - audio_codes: torch.Size([1, 3000]), instrument_token: torch.Size([1, 3000]), attention_mask: torch.Size([1, 3000]), positional_embeddings: torch.Size([1, 3000, 128])\n",
      "generated sequence:  torch.Size([1, 3000])\n",
      "keys track saved to generated_joint_model\\generated_keys.wav\n",
      "Generating full_instrumental...\n",
      "Generating for full_instrumental...\n",
      "Instrument token map for full_instrumental: 1033\n",
      "Shapes - audio_codes: torch.Size([1, 3000]), instrument_token: torch.Size([1, 3000]), attention_mask: torch.Size([1, 3000]), positional_embeddings: torch.Size([1, 3000, 128])\n",
      "generated sequence:  torch.Size([1, 3000])\n",
      "full_instrumental track saved to generated_joint_model\\generated_full_instrumental.wav\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torchaudio\n",
    "import numpy as np\n",
    "from transformers import AutoProcessor, EncodecModel, GPT2LMHeadModel\n",
    "import tempfile\n",
    "import madmom\n",
    "import torch.nn as nn\n",
    "from torch.nn.functional import pad as torch_pad\n",
    "\n",
    "# Constants\n",
    "VOCAB_SIZE = 1034\n",
    "MAX_LENGTH = 3000\n",
    "device = torch.device(\"cpu\")  # Set device to CPU for inference\n",
    "\n",
    "# Track classes and corresponding instrument tokens\n",
    "track_classes = ['hi_hat', 'kick', 'snare', 'clap', 'bass', 'drums', 'keys', 'full_instrumental']  # Example track classes\n",
    "\n",
    "# Map each track class to a unique token ID for conditioning\n",
    "instrument_token_map = {\n",
    "    'hi_hat': 1027,\n",
    "    'kick': 1028,\n",
    "    'snare': 1029,\n",
    "    'clap': 1030,\n",
    "    'bass': 1031,\n",
    "    'drums': 1032,\n",
    "    'keys': 1032,\n",
    "    'full_instrumental': 1033\n",
    "}\n",
    "\n",
    "\n",
    "# Initialize Encodec Model and Processor\n",
    "processor = AutoProcessor.from_pretrained(\"facebook/encodec_24khz\")\n",
    "model_encodec = EncodecModel.from_pretrained(\"facebook/encodec_24khz\").to(device)\n",
    "\n",
    "# Function to encode the audio and extract audio codes\n",
    "def encode_audio(audio_path):\n",
    "    audio, rate = torchaudio.load(audio_path)\n",
    "    \n",
    "    # Calculate the number of samples corresponding to 10 seconds\n",
    "    max_length_in_samples = int(rate * 10)\n",
    "    \n",
    "    # Trim or pad the audio to ensure it's exactly 10 seconds long\n",
    "    if audio.shape[1] > max_length_in_samples:\n",
    "        audio = audio[:, :max_length_in_samples]\n",
    "    else:\n",
    "        pad_length = max_length_in_samples - audio.shape[1]\n",
    "        audio = torch.nn.functional.pad(audio, (0, pad_length))\n",
    "    \n",
    "    # Ensure that the audio has a single channel\n",
    "    if audio.ndim > 1:\n",
    "        audio = audio[0]\n",
    "    \n",
    "    audio_np = audio.numpy()\n",
    "    inputs = processor(audio_np, sampling_rate=rate, return_tensors=\"pt\")\n",
    "    \n",
    "    # Move input values to the same device as the model (CPU in this case)\n",
    "    inputs = {key: val.to(device) for key, val in inputs.items()}\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model_encodec.encode(inputs[\"input_values\"], inputs[\"padding_mask\"], 3)\n",
    "    \n",
    "    return outputs.audio_codes.squeeze(), min(audio.shape[0] / rate, 10)\n",
    "\n",
    "# Function to extract downbeats\n",
    "def extract_downbeats(audio_path, fps=100, duration=10):\n",
    "    audio, rate = torchaudio.load(audio_path)\n",
    "    num_samples = int(duration * rate)\n",
    "    \n",
    "    if audio.shape[1] > num_samples:\n",
    "        audio = audio[:, :num_samples]\n",
    "    \n",
    "    with tempfile.NamedTemporaryFile(delete=False, suffix='.wav') as temp_audio_file:\n",
    "        temp_audio_path = temp_audio_file.name\n",
    "        torchaudio.save(temp_audio_path, audio, rate)\n",
    "    \n",
    "    proc = madmom.features.downbeats.DBNDownBeatTrackingProcessor(beats_per_bar=[4], fps=fps)\n",
    "    act = madmom.features.downbeats.RNNDownBeatProcessor(fps=fps)(temp_audio_path)\n",
    "    downbeats = proc(act)\n",
    "    \n",
    "    os.remove(temp_audio_path)\n",
    "    \n",
    "    return downbeats[downbeats[:, 1] == 1, 0]\n",
    "\n",
    "# Function to extract beats\n",
    "def extract_beats(audio_path, fps=100, duration=10):\n",
    "    audio, rate = torchaudio.load(audio_path)\n",
    "    num_samples = int(duration * rate)\n",
    "    \n",
    "    if audio.shape[1] > num_samples:\n",
    "        audio = audio[:, :num_samples]\n",
    "    \n",
    "    with tempfile.NamedTemporaryFile(delete=False, suffix='.wav') as temp_audio_file:\n",
    "        temp_audio_path = temp_audio_file.name\n",
    "        torchaudio.save(temp_audio_path, audio, rate)\n",
    "    \n",
    "    beat_act = madmom.features.beats.RNNBeatProcessor(fps=fps)(temp_audio_path)\n",
    "    beat_proc = madmom.features.beats.BeatDetectionProcessor(fps=fps)\n",
    "    beats = beat_proc(beat_act)\n",
    "    \n",
    "    os.remove(temp_audio_path)\n",
    "    \n",
    "    return beats\n",
    "\n",
    "# Function to create positional embeddings\n",
    "def create_positional_embeddings(beat_times, downbeat_times, audio_duration, fps=75, K=32):\n",
    "    total_frames = int(np.ceil(audio_duration * fps))\n",
    "\n",
    "    def ramps(positions, size):\n",
    "        result = np.zeros(size)\n",
    "        for a, b in zip(positions[:-1], positions[1:]):\n",
    "            result[a:b] = np.linspace(0, 1, b - a, endpoint=False)\n",
    "        missing = positions[0]\n",
    "        if missing:\n",
    "            piece = result[positions[0]:positions[1]]\n",
    "            pieces = np.tile(piece, missing // len(piece) + 1)\n",
    "            result[:missing] = pieces[-missing:]\n",
    "        missing = size - positions[-1]\n",
    "        if missing:\n",
    "            piece = result[positions[-2]:positions[-1]]\n",
    "            pieces = np.tile(piece, missing // len(piece) + 1)\n",
    "            result[-missing:] = pieces[:missing]\n",
    "        return result\n",
    "\n",
    "    vector_downbeat = ramps((downbeat_times * fps).astype(int), total_frames)\n",
    "    vector_beat = ramps((beat_times * fps).astype(int), total_frames)\n",
    "\n",
    "    frequencies = np.arange(1, K + 1)\n",
    "    embeddings_downbeat = []\n",
    "    embeddings_beat = []\n",
    "\n",
    "    for k in frequencies:\n",
    "        embeddings_downbeat.append(np.sin(2 * np.pi * vector_downbeat * k))\n",
    "        embeddings_downbeat.append(np.cos(2 * np.pi * vector_downbeat * k))\n",
    "        embeddings_beat.append(np.sin(2 * np.pi * vector_beat * k))\n",
    "        embeddings_beat.append(np.cos(2 * np.pi * vector_beat * k))\n",
    "\n",
    "    embeddings_downbeat = np.stack(embeddings_downbeat, axis=1)\n",
    "    embeddings_beat = np.stack(embeddings_beat, axis=1)\n",
    "    embeddings = np.hstack((embeddings_downbeat, embeddings_beat))\n",
    "\n",
    "    return torch.from_numpy(embeddings).float()\n",
    "\n",
    "# Initialize the trained GPT-2 model\n",
    "class CustomGPT2ForConditionalGeneration(GPT2LMHeadModel):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.projection = nn.Linear(config.n_embd * 2, config.n_embd)\n",
    "\n",
    "    def forward(self, input_ids=None, attention_mask=None, labels=None, positional_embeddings=None, instrument_token=None, **kwargs):\n",
    "        # Get input embeddings\n",
    "        input_embeds = self.transformer.wte(input_ids)\n",
    "\n",
    "        # Expand instrument token embeddings and add them to the input\n",
    "        instrument_embeds = self.transformer.wte(instrument_token)\n",
    "\n",
    "        # Combine positional embeddings with input embeddings\n",
    "        embs_dim = positional_embeddings.shape[2]\n",
    "        input_embeds = torch.cat((input_embeds[:, :, :-embs_dim], input_embeds[:, :, -embs_dim:] + positional_embeddings), dim=-1)\n",
    "        \n",
    "        # Add instrument embeddings at the defined intervals\n",
    "        combined_input = input_embeds + instrument_embeds\n",
    "\n",
    "        # Forward pass through the GPT model\n",
    "        return super().forward(inputs_embeds=combined_input, attention_mask=attention_mask, labels=labels, **kwargs)\n",
    "\n",
    "# Function to generate each track with debug prints\n",
    "def generate_track(model, audio_codes, positional_embeddings, attention_mask, track_class, device):\n",
    "    # Set the interval for repeating the instrument token\n",
    "    interval = 50  # For example, place the instrument token every 50 positions\n",
    "    \n",
    "    # Create the instrument token array filled with the default class token\n",
    "    instrument_token = np.full(MAX_LENGTH, instrument_token_map[track_class], dtype=int)\n",
    "    \n",
    "    # Repeat the instrument token at regular intervals\n",
    "    instrument_token[::interval] = instrument_token_map[track_class]\n",
    "    \n",
    "    # Convert to tensor and ensure it matches the input shape\n",
    "    instrument_token = torch.tensor(instrument_token, dtype=torch.long).unsqueeze(0).to(device)\n",
    "\n",
    "    # Ensure positional embeddings and attention mask are correctly sized\n",
    "    attention_mask = attention_mask.to(device)\n",
    "    positional_embeddings = positional_embeddings.to(device)\n",
    "\n",
    "    # Debugging prints\n",
    "    print(f\"Generating for {track_class}...\")\n",
    "    print(f\"Instrument token map for {track_class}: {instrument_token_map[track_class]}\")\n",
    "    print(f\"Shapes - audio_codes: {audio_codes.shape}, instrument_token: {instrument_token.shape}, attention_mask: {attention_mask.shape}, positional_embeddings: {positional_embeddings.shape}\")\n",
    "    \n",
    "    # Generate output\n",
    "    with torch.no_grad():  # Ensure no gradient computation\n",
    "        outputs = model(input_ids=audio_codes, attention_mask=attention_mask, positional_embeddings=positional_embeddings, instrument_token=instrument_token)\n",
    "        generated_sequence = outputs.logits.argmax(dim=-1)\n",
    "    \n",
    "    return generated_sequence\n",
    "\n",
    "\n",
    "model_gpt2_path = f'./saved_model_joint_standard_full_dataset_final'\n",
    "model = CustomGPT2ForConditionalGeneration.from_pretrained(model_gpt2_path).to(device)\n",
    "\n",
    "# Set the model to evaluation mode to disable dropout and other training-specific layers\n",
    "model.eval()\n",
    "\n",
    "# Main inference loop for each track class\n",
    "audio_path = \"inference.wav\"\n",
    "reference_beat_path = \"inference_posemb.wav\"\n",
    "\n",
    "# Encode audio to get audio codes\n",
    "audio_codes, length_in_seconds = encode_audio(audio_path)\n",
    "\n",
    "# Move the tensor to the CPU before converting to NumPy and processing\n",
    "audio_codes = audio_codes.cpu().numpy().flatten()\n",
    "\n",
    "# Apply padding to the flattened audio codes and ensure it matches MAX_LENGTH\n",
    "audio_codes = np.pad(audio_codes, (0, MAX_LENGTH - len(audio_codes)), 'constant', constant_values=(0, 0))[:MAX_LENGTH]\n",
    "\n",
    "# Convert back to tensor and move to the appropriate device\n",
    "audio_codes = torch.tensor(audio_codes, dtype=torch.long).unsqueeze(0).to(device)\n",
    "\n",
    "# Extract beats and downbeats\n",
    "reference_beats = extract_beats(reference_beat_path, duration=length_in_seconds)\n",
    "reference_downbeats = extract_downbeats(reference_beat_path, duration=length_in_seconds)\n",
    "\n",
    "# Create positional embeddings\n",
    "positional_embeddings = create_positional_embeddings(reference_beats, reference_downbeats, length_in_seconds)\n",
    "positional_embeddings = torch_pad(positional_embeddings, (0, 0, 0, MAX_LENGTH - positional_embeddings.shape[0])).unsqueeze(0).to(device)\n",
    "\n",
    "# Create attention mask\n",
    "attention_mask = (audio_codes != 0).long()\n",
    "\n",
    "# Generate and save tracks for each instrument\n",
    "output_directory = f'generated_joint_model'\n",
    "os.makedirs(output_directory, exist_ok=True)\n",
    "for track_class in track_classes:\n",
    "    print(f\"Generating {track_class}...\")\n",
    "\n",
    "    # Generate the track\n",
    "    generated_sequence = generate_track(model, audio_codes, positional_embeddings, attention_mask, track_class, device)\n",
    "    print('generated sequence: ', generated_sequence.shape)\n",
    "    # Reshape and decode\n",
    "    reshaped_output = generated_sequence.view(4, 750).unsqueeze(0).unsqueeze(0)\n",
    "    audio_values = model_encodec.decode(reshaped_output, [None])[0]\n",
    "    audio_np = audio_values.cpu().squeeze().detach().numpy()\n",
    "\n",
    "    # Save the generated audio\n",
    "    output_audio_path = os.path.join(output_directory, f\"generated_{track_class}.wav\")\n",
    "    torchaudio.save(output_audio_path, torch.tensor(audio_np).unsqueeze(0), processor.sampling_rate)\n",
    "    print(f\"{track_class} track saved to {output_audio_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
