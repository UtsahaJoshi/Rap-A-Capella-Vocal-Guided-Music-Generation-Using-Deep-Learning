{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "477a9532",
   "metadata": {},
   "source": [
    "# Independent Track Generation with Custom GPT-2 Model and Periodic Checkpointing\n",
    "\n",
    "This script trains a GPT-2-based conditional generation model for independent music track generation using audio embeddings combined with positional embeddings. The dataset, loaded from a .npy file, contains audio codes, positional embeddings, and metadata for various track classes (e.g., bass, full_instrumental). A custom PyTorch dataset (MusicDataset) is used to preprocess and structure the data, ensuring each track class is trained independently without cumulative mixing. Positional embeddings are integrated into the modelâ€™s input embeddings during the forward pass using a modified GPT-2 architecture.\n",
    "\n",
    "Training is performed sequentially for each track class, using the Hugging Face Trainer with cosine learning rate scheduling and mixed precision (FP16) for efficiency. Logging and monitoring are managed through Weights & Biases (WandB), while checkpoints are saved after each epoch, with the best model stored for future use. This approach ensures robust, track-specific generative modeling by leveraging both vocal and positional embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70c60bdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset\n",
    "from transformers import (\n",
    "    GPT2LMHeadModel,\n",
    "    GPT2Config,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    get_cosine_schedule_with_warmup,\n",
    ")\n",
    "import torch.nn as nn\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Import wandb for logging\n",
    "import wandb\n",
    "\n",
    "# Set the device to CUDA if available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Load your saved .npy file\n",
    "data = np.load(\n",
    "    'fulldataset_10sec_positional_vocal_embs.npy',\n",
    "    allow_pickle=True\n",
    ").item()\n",
    "\n",
    "VOCAB_SIZE = 1026\n",
    "MAX_LENGTH = 3000  # Adjusted for maximum length\n",
    "track_classes = ['bass', 'full_instrumental']\n",
    "\n",
    "class MusicDataset(Dataset):\n",
    "    def __init__(self, data, track_class):\n",
    "        self.track_class = track_class\n",
    "        self.data = {\n",
    "            k: v for k, v in data.items()\n",
    "            if self.track_class in v['generation_data']\n",
    "        }\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample_id = list(self.data.keys())[idx]\n",
    "        sample = self.data[sample_id]\n",
    "\n",
    "        # Retrieve the specific track data without cumulative mixing\n",
    "        vocal_audio_codes = sample['generation_data'].get(\n",
    "            'vocal', np.zeros((4, 750))\n",
    "        )\n",
    "        track_data = sample['generation_data'].get(\n",
    "            self.track_class, np.zeros((4, 750))\n",
    "        )  # Track for training\n",
    "        positional_embedding = sample.get(\n",
    "            'positional_embedding', np.zeros((4, 750))\n",
    "        )\n",
    "\n",
    "        # Ensure values are within valid bounds\n",
    "        vocal_audio_codes = np.clip(vocal_audio_codes, 0, VOCAB_SIZE - 1)\n",
    "\n",
    "        # Flatten and pad/truncate sequences\n",
    "        vocal_audio_codes = np.pad(\n",
    "            vocal_audio_codes.flatten(),\n",
    "            (0, MAX_LENGTH - len(vocal_audio_codes.flatten())),\n",
    "            'constant',\n",
    "            constant_values=(0, 0)\n",
    "        )[:MAX_LENGTH]\n",
    "        track_data = np.pad(\n",
    "            track_data.flatten(),\n",
    "            (0, MAX_LENGTH - len(track_data.flatten())),\n",
    "            'constant',\n",
    "            constant_values=(0, 0)\n",
    "        )[:MAX_LENGTH]\n",
    "\n",
    "        attention_mask = (vocal_audio_codes != 0).astype(int)\n",
    "        # Flatten and pad positional embeddings\n",
    "        pos_emb_flat = positional_embedding.flatten()\n",
    "        pos_emb_flat = np.pad(\n",
    "            pos_emb_flat,\n",
    "            (0, MAX_LENGTH * positional_embedding.shape[1] - len(pos_emb_flat)),\n",
    "            'constant',\n",
    "            constant_values=(0, 0)\n",
    "        )[:MAX_LENGTH * positional_embedding.shape[1]]\n",
    "\n",
    "        # Reshape back to (MAX_LENGTH, embedding_dim)\n",
    "        positional_embedding = pos_emb_flat.reshape(MAX_LENGTH, positional_embedding.shape[1])\n",
    "        return {\n",
    "            'input_ids': torch.tensor(vocal_audio_codes, dtype=torch.long),\n",
    "            'attention_mask': torch.tensor(attention_mask, dtype=torch.long),\n",
    "            'labels': torch.tensor(track_data, dtype=torch.long),\n",
    "            'positional_embeddings': torch.tensor(positional_embedding, dtype=torch.float),\n",
    "            'sample_id': sample_id  # Only needed for generation and caching\n",
    "        }\n",
    "\n",
    "class CustomGPT2ForConditionalGeneration(GPT2LMHeadModel):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        # No need to define projection layer if not used\n",
    "\n",
    "    def forward(self, input_ids=None, attention_mask=None, labels=None,\n",
    "                positional_embeddings=None, **kwargs):\n",
    "        # Get input embeddings\n",
    "        input_embeds = self.transformer.wte(input_ids)\n",
    "\n",
    "        # Combine positional embeddings with input embeddings\n",
    "        embs_dim = positional_embeddings.shape[2]\n",
    "        input_embeds = torch.cat((input_embeds[:, :, :-embs_dim], input_embeds[:, :, -embs_dim:] + positional_embeddings), dim=-1)\n",
    "\n",
    "        # Proceed with the standard GPT-2 forward pass\n",
    "        return super().forward(\n",
    "            inputs_embeds=input_embeds,\n",
    "            attention_mask=attention_mask,\n",
    "            labels=labels,\n",
    "            **kwargs\n",
    "        )\n",
    "\n",
    "# Configuration for the model\n",
    "config = GPT2Config(\n",
    "    vocab_size=VOCAB_SIZE,\n",
    "    n_positions=MAX_LENGTH,\n",
    "    n_ctx=MAX_LENGTH,\n",
    "    n_embd=128,  # Match with your previous d_model\n",
    "    n_layer=6,\n",
    "    n_head=8,\n",
    "    activation_function='gelu',\n",
    "    resid_pdrop=0.1,\n",
    "    embd_pdrop=0.1,\n",
    "    attn_pdrop=0.1,\n",
    ")\n",
    "\n",
    "# Initialize the custom GPT-2 model\n",
    "model = CustomGPT2ForConditionalGeneration(config=config).to(device)\n",
    "\n",
    "# Custom data collator to handle positional embeddings\n",
    "class DataCollatorWithPositionalEmbeddings:\n",
    "    def __call__(self, batch):\n",
    "        input_ids = torch.stack([item['input_ids'] for item in batch]).to(device)\n",
    "        attention_mask = torch.stack([item['attention_mask'] for item in batch]).to(device)\n",
    "        labels = torch.stack([item['labels'] for item in batch]).to(device)\n",
    "        positional_embeddings = torch.stack([item['positional_embeddings'] for item in batch]).to(device)\n",
    "\n",
    "        return {\n",
    "            'input_ids': input_ids,\n",
    "            'attention_mask': attention_mask,\n",
    "            'labels': labels,\n",
    "            'positional_embeddings': positional_embeddings\n",
    "        }\n",
    "\n",
    "data_collator = DataCollatorWithPositionalEmbeddings()\n",
    "\n",
    "# Set up the batch size and epoch interval for saving\n",
    "batch_size = 1  # Adjust as needed\n",
    "total_epochs = 120\n",
    "\n",
    "# Sequential training and generation for each track in track_classes\n",
    "for track_idx, track in enumerate(track_classes):\n",
    "    print(f\"Training for {track}...\")\n",
    "\n",
    "    # Create dataset filtered by the current track class (string)\n",
    "    dataset = MusicDataset(data, track)\n",
    "\n",
    "    train_indices, val_indices = train_test_split(\n",
    "        range(len(dataset)), test_size=0.2, random_state=42\n",
    "    )\n",
    "    train_dataset = torch.utils.data.Subset(dataset, train_indices)\n",
    "    val_dataset = torch.utils.data.Subset(dataset, val_indices)\n",
    "\n",
    "    # Set unique output directory for each track\n",
    "    track_output_dir = f'./independent_track_generation_gpt2_checkpointing_{track}_model_pos_emb_concat_fulldataset_10sec_embeddings_final'\n",
    "\n",
    "    # Initialize wandb project name and run name\n",
    "    wandb_project_name = 'music_generation'\n",
    "    wandb_run_name = f'independent_track_generation_gpt2_checkpointing_{track}_training_post_emb_concat_run_fulldataset_10sec_embeddings_final'\n",
    "\n",
    "    # Initialize wandb before training starts\n",
    "    wandb.init(project=wandb_project_name, name=wandb_run_name)\n",
    "\n",
    "    track_training_args = TrainingArguments(\n",
    "        output_dir=track_output_dir,\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        learning_rate=1e-4,  # Lower the learning rate\n",
    "        per_device_train_batch_size=batch_size,\n",
    "        per_device_eval_batch_size=batch_size,\n",
    "        num_train_epochs=total_epochs,\n",
    "        weight_decay=0.01,\n",
    "        save_total_limit=3,\n",
    "        logging_dir=f'./logs_{track}',\n",
    "        logging_steps=10,\n",
    "        metric_for_best_model=\"loss\",\n",
    "        greater_is_better=False,\n",
    "        fp16=True,\n",
    "        dataloader_pin_memory=False,\n",
    "        load_best_model_at_end=True,\n",
    "        lr_scheduler_type='cosine',  # Use cosine learning rate scheduler\n",
    "        warmup_steps=500,            # Number of warmup steps\n",
    "        report_to=['wandb'],         # Report to wandb\n",
    "        run_name=wandb_run_name,     # Set the wandb run name\n",
    "    )\n",
    "\n",
    "    # Calculate total training steps\n",
    "    total_steps = (\n",
    "        len(train_dataset) // track_training_args.per_device_train_batch_size\n",
    "    ) * total_epochs\n",
    "\n",
    "    # Initialize the optimizer\n",
    "    optimizer = torch.optim.AdamW(\n",
    "        model.parameters(),\n",
    "        lr=track_training_args.learning_rate,\n",
    "        weight_decay=track_training_args.weight_decay\n",
    "    )\n",
    "\n",
    "    # Initialize the scheduler\n",
    "    scheduler = get_cosine_schedule_with_warmup(\n",
    "        optimizer,\n",
    "        num_warmup_steps=track_training_args.warmup_steps,\n",
    "        num_training_steps=total_steps,\n",
    "    )\n",
    "\n",
    "    # Initialize the Trainer for each track\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=track_training_args,\n",
    "        data_collator=data_collator,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=val_dataset,\n",
    "        optimizers=(optimizer, scheduler),  # Pass the optimizer and scheduler\n",
    "    )\n",
    "\n",
    "    # Train the model for the current track\n",
    "    trainer.train()\n",
    "\n",
    "    print(f\"Finished training and saved model checkpoints for {track}.\")\n",
    "\n",
    "    # Define the path to save the best model\n",
    "    best_model_output_dir = os.path.join(\n",
    "        'independent_track_generation_gpt2_checkpointing_pos_emb_concat_full_dataset_10secembeddings_final',\n",
    "        f'{track}_model'\n",
    "    )\n",
    "\n",
    "    # Ensure the directory exists\n",
    "    os.makedirs(best_model_output_dir, exist_ok=True)\n",
    "\n",
    "    # Save the best model after training\n",
    "    trainer.save_model(output_dir=best_model_output_dir)\n",
    "\n",
    "    # Finish the wandb run\n",
    "    wandb.finish() \n",
    "\n",
    "print(\"Training completed for all tracks.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d685ca5e",
   "metadata": {},
   "source": [
    "# Inference\n",
    "This script performs track-specific music generation using pre-trained GPT-2-based models and Facebook's EnCodec for audio processing. It encodes input audio files into fixed-length audio codes and extracts positional embeddings using beat and downbeat detection via Madmom. These embeddings, combined with the audio codes, are passed to fine-tuned GPT-2 models for generating sequences specific to various track classes (e.g., hi_hat, kick, snare, bass, etc.).\n",
    "\n",
    "The generated sequences are decoded back into audio using EnCodec, producing distinct audio tracks for each class. The script processes multiple input files and saves the outputs in organized directories, ensuring efficient handling of track-specific generative tasks while maintaining temporal coherence through positional embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "60df1f7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration. Please open a PR/issue to update `preprocessor_config.json` to use `image_processor_type` instead of `feature_extractor_type`. This warning will be removed in v4.40.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing inference.wav -> hi_hat in folder 1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ripple\\AppData\\Local\\Temp\\ipykernel_13748\\3974269274.py:194: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  generated_sequence = generate_track(model, torch.tensor(audio_codes), positional_embeddings, attention_mask)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: ././independent_track_generation_gpt2_checkpointing_hi_hat_model_pos_emb_concat_fulldataset_10sec_embeddings_final/hi_hat_inference.wav_generated.wav\n",
      "Processing inference.wav -> kick in folder 1...\n",
      "Saved: ././independent_track_generation_gpt2_checkpointing_kick_model_pos_emb_concat_fulldataset_10sec_embeddings_final/kick_inference.wav_generated.wav\n",
      "Processing inference.wav -> snare in folder 1...\n",
      "Saved: ././independent_track_generation_gpt2_checkpointing_snare_model_pos_emb_concat_fulldataset_10sec_embeddings_final/snare_inference.wav_generated.wav\n",
      "Processing inference.wav -> clap in folder 1...\n",
      "Saved: ././independent_track_generation_gpt2_checkpointing_clap_model_pos_emb_concat_fulldataset_10sec_embeddings_final/clap_inference.wav_generated.wav\n",
      "Processing inference.wav -> bass in folder 1...\n",
      "Saved: ././independent_track_generation_gpt2_checkpointing_bass_model_pos_emb_concat_fulldataset_10sec_embeddings_final/bass_inference.wav_generated.wav\n",
      "Processing inference.wav -> drums in folder 1...\n",
      "Saved: ././independent_track_generation_gpt2_checkpointing_drums_model_pos_emb_concat_fulldataset_10sec_embeddings_final/drums_inference.wav_generated.wav\n",
      "Processing inference.wav -> keys in folder 1...\n",
      "Saved: ././independent_track_generation_gpt2_checkpointing_keys_model_pos_emb_concat_fulldataset_10sec_embeddings_final/keys_inference.wav_generated.wav\n",
      "Processing inference.wav -> full_instrumental in folder 1...\n",
      "Saved: ././independent_track_generation_gpt2_checkpointing_full_instrumental_model_pos_emb_concat_fulldataset_10sec_embeddings_final/full_instrumental_inference.wav_generated.wav\n",
      "Processing inference2.wav -> hi_hat in folder 2...\n",
      "Saved: ././independent_track_generation_gpt2_checkpointing_hi_hat_model_pos_emb_concat_fulldataset_10sec_embeddings_final/hi_hat_inference2.wav_generated.wav\n",
      "Processing inference2.wav -> kick in folder 2...\n",
      "Saved: ././independent_track_generation_gpt2_checkpointing_kick_model_pos_emb_concat_fulldataset_10sec_embeddings_final/kick_inference2.wav_generated.wav\n",
      "Processing inference2.wav -> snare in folder 2...\n",
      "Saved: ././independent_track_generation_gpt2_checkpointing_snare_model_pos_emb_concat_fulldataset_10sec_embeddings_final/snare_inference2.wav_generated.wav\n",
      "Processing inference2.wav -> clap in folder 2...\n",
      "Saved: ././independent_track_generation_gpt2_checkpointing_clap_model_pos_emb_concat_fulldataset_10sec_embeddings_final/clap_inference2.wav_generated.wav\n",
      "Processing inference2.wav -> bass in folder 2...\n",
      "Saved: ././independent_track_generation_gpt2_checkpointing_bass_model_pos_emb_concat_fulldataset_10sec_embeddings_final/bass_inference2.wav_generated.wav\n",
      "Processing inference2.wav -> drums in folder 2...\n",
      "Saved: ././independent_track_generation_gpt2_checkpointing_drums_model_pos_emb_concat_fulldataset_10sec_embeddings_final/drums_inference2.wav_generated.wav\n",
      "Processing inference2.wav -> keys in folder 2...\n",
      "Saved: ././independent_track_generation_gpt2_checkpointing_keys_model_pos_emb_concat_fulldataset_10sec_embeddings_final/keys_inference2.wav_generated.wav\n",
      "Processing inference2.wav -> full_instrumental in folder 2...\n",
      "Saved: ././independent_track_generation_gpt2_checkpointing_full_instrumental_model_pos_emb_concat_fulldataset_10sec_embeddings_final/full_instrumental_inference2.wav_generated.wav\n",
      "Processing inference3.wav -> hi_hat in folder 3...\n",
      "Saved: ././independent_track_generation_gpt2_checkpointing_hi_hat_model_pos_emb_concat_fulldataset_10sec_embeddings_final/hi_hat_inference3.wav_generated.wav\n",
      "Processing inference3.wav -> kick in folder 3...\n",
      "Saved: ././independent_track_generation_gpt2_checkpointing_kick_model_pos_emb_concat_fulldataset_10sec_embeddings_final/kick_inference3.wav_generated.wav\n",
      "Processing inference3.wav -> snare in folder 3...\n",
      "Saved: ././independent_track_generation_gpt2_checkpointing_snare_model_pos_emb_concat_fulldataset_10sec_embeddings_final/snare_inference3.wav_generated.wav\n",
      "Processing inference3.wav -> clap in folder 3...\n",
      "Saved: ././independent_track_generation_gpt2_checkpointing_clap_model_pos_emb_concat_fulldataset_10sec_embeddings_final/clap_inference3.wav_generated.wav\n",
      "Processing inference3.wav -> bass in folder 3...\n",
      "Saved: ././independent_track_generation_gpt2_checkpointing_bass_model_pos_emb_concat_fulldataset_10sec_embeddings_final/bass_inference3.wav_generated.wav\n",
      "Processing inference3.wav -> drums in folder 3...\n",
      "Saved: ././independent_track_generation_gpt2_checkpointing_drums_model_pos_emb_concat_fulldataset_10sec_embeddings_final/drums_inference3.wav_generated.wav\n",
      "Processing inference3.wav -> keys in folder 3...\n",
      "Saved: ././independent_track_generation_gpt2_checkpointing_keys_model_pos_emb_concat_fulldataset_10sec_embeddings_final/keys_inference3.wav_generated.wav\n",
      "Processing inference3.wav -> full_instrumental in folder 3...\n",
      "Saved: ././independent_track_generation_gpt2_checkpointing_full_instrumental_model_pos_emb_concat_fulldataset_10sec_embeddings_final/full_instrumental_inference3.wav_generated.wav\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torchaudio\n",
    "import numpy as np\n",
    "from transformers import AutoProcessor, EncodecModel, GPT2LMHeadModel\n",
    "import madmom\n",
    "import torch.nn as nn\n",
    "import tempfile\n",
    "\n",
    "# Constants\n",
    "VOCAB_SIZE = 1026\n",
    "MAX_LENGTH = 3000\n",
    "device = torch.device(\"cpu\")  # Use \"cuda\" if GPU is available\n",
    "\n",
    "# Track classes\n",
    "track_classes = ['hi_hat', 'kick', 'snare', 'clap', 'bass', 'drums', 'keys', 'full_instrumental']\n",
    "\n",
    "# Initialize Encodec Model and Processor\n",
    "processor = AutoProcessor.from_pretrained(\"facebook/encodec_24khz\")\n",
    "model_encodec = EncodecModel.from_pretrained(\"facebook/encodec_24khz\").to(device)\n",
    "\n",
    "# Function Definitions (unchanged except added safety checks)\n",
    "def encode_audio(audio_path):\n",
    "    audio, rate = torchaudio.load(audio_path)\n",
    "    max_length_in_samples = int(rate * 10)\n",
    "\n",
    "    if audio.shape[1] > max_length_in_samples:\n",
    "        audio = audio[:, :max_length_in_samples]\n",
    "    else:\n",
    "        pad_length = max_length_in_samples - audio.shape[1]\n",
    "        audio = torch.nn.functional.pad(audio, (0, pad_length))\n",
    "\n",
    "    if audio.shape[0] > 1:\n",
    "        audio = audio.mean(dim=0)\n",
    "    else:\n",
    "        audio = audio.squeeze(0)\n",
    "\n",
    "    inputs = processor(audio.numpy(), sampling_rate=rate, return_tensors=\"pt\")\n",
    "    inputs = {key: val.to(device) for key, val in inputs.items()}\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model_encodec.encode(inputs[\"input_values\"], inputs[\"padding_mask\"], 3)\n",
    "    \n",
    "    duration = audio.shape[0] / rate\n",
    "    return outputs.audio_codes.squeeze(), min(duration, 10.0)\n",
    "\n",
    "def extract_beats_and_downbeats(audio_path, fps=100, duration=10):\n",
    "    audio, rate = torchaudio.load(audio_path)\n",
    "    audio = audio[:, :int(duration * rate)]\n",
    "\n",
    "    with tempfile.NamedTemporaryFile(delete=False, suffix='.wav') as temp_audio_file:\n",
    "        temp_audio_path = temp_audio_file.name\n",
    "        torchaudio.save(temp_audio_path, audio, rate)\n",
    "\n",
    "    proc_downbeats = madmom.features.downbeats.DBNDownBeatTrackingProcessor(beats_per_bar=[4], fps=fps)\n",
    "    act_downbeats = madmom.features.downbeats.RNNDownBeatProcessor(fps=fps)(temp_audio_path)\n",
    "    downbeats = proc_downbeats(act_downbeats)\n",
    "\n",
    "    proc_beats = madmom.features.beats.BeatDetectionProcessor(fps=fps)\n",
    "    act_beats = madmom.features.beats.RNNBeatProcessor(fps=fps)(temp_audio_path)\n",
    "    beats = proc_beats(act_beats)\n",
    "\n",
    "    os.remove(temp_audio_path)\n",
    "\n",
    "    if len(beats) == 0 or len(downbeats) == 0:\n",
    "        raise ValueError(f\"No beats or downbeats detected in {audio_path}\")\n",
    "\n",
    "    return beats, downbeats[downbeats[:, 1] == 1, 0]\n",
    "\n",
    "def create_positional_embeddings(beat_times, downbeat_times, audio_duration, fps=75, K=32):\n",
    "    total_frames = int(np.ceil(audio_duration * fps))\n",
    "\n",
    "    def ramps(positions, size):\n",
    "        result = np.zeros(size)\n",
    "        for a, b in zip(positions[:-1], positions[1:]):\n",
    "            result[a:b] = np.linspace(0, 1, b - a, endpoint=False)\n",
    "        missing = positions[0]\n",
    "        if missing:\n",
    "            piece = result[positions[0]:positions[1]]\n",
    "            pieces = np.tile(piece, missing // len(piece) + 1)\n",
    "            result[:missing] = pieces[-missing:]\n",
    "        missing = size - positions[-1]\n",
    "        if missing:\n",
    "            piece = result[positions[-2]:positions[-1]]\n",
    "            pieces = np.tile(piece, missing // len(piece) + 1)\n",
    "            result[-missing:] = pieces[:missing]\n",
    "        return result\n",
    "\n",
    "    vector_downbeat = ramps((downbeat_times * fps).astype(int), total_frames)\n",
    "    vector_beat = ramps((beat_times * fps).astype(int), total_frames)\n",
    "\n",
    "    frequencies = np.arange(1, K + 1)\n",
    "    embeddings_downbeat = []\n",
    "    embeddings_beat = []\n",
    "\n",
    "    for k in frequencies:\n",
    "        embeddings_downbeat.append(np.sin(2 * np.pi * vector_downbeat * k))\n",
    "        embeddings_downbeat.append(np.cos(2 * np.pi * vector_downbeat * k))\n",
    "        embeddings_beat.append(np.sin(2 * np.pi * vector_beat * k))\n",
    "        embeddings_beat.append(np.cos(2 * np.pi * vector_beat * k))\n",
    "\n",
    "    embeddings_downbeat = np.stack(embeddings_downbeat, axis=1)\n",
    "    embeddings_beat = np.stack(embeddings_beat, axis=1)\n",
    "    embeddings = np.hstack((embeddings_downbeat, embeddings_beat))\n",
    "\n",
    "    return torch.from_numpy(embeddings).float()\n",
    "\n",
    "class CustomGPT2ForConditionalGeneration(GPT2LMHeadModel):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        # No need to define projection layer if not used\n",
    "\n",
    "    def forward(self, input_ids=None, attention_mask=None, labels=None,\n",
    "                positional_embeddings=None, **kwargs):\n",
    "        # Get input embeddings\n",
    "        input_embeds = self.transformer.wte(input_ids)\n",
    "        # Combine positional embeddings with input embeddings\n",
    "        input_embeds = input_embeds + positional_embeddings\n",
    "\n",
    "        # Proceed with the standard GPT-2 forward pass\n",
    "        return super().forward(\n",
    "            inputs_embeds=input_embeds,\n",
    "            attention_mask=attention_mask,\n",
    "            labels=labels,\n",
    "            **kwargs\n",
    "        )\n",
    "\n",
    "def find_highest_checkpoint(folder):\n",
    "    checkpoints = [d for d in os.listdir(folder) if d.startswith(\"checkpoint-\")]\n",
    "    if not checkpoints:\n",
    "        raise ValueError(f\"No checkpoints found in {folder}\")\n",
    "    checkpoints = sorted(checkpoints, key=lambda x: int(x.split(\"-\")[-1]), reverse=True)\n",
    "    return os.path.join(folder, checkpoints[0])\n",
    "\n",
    "def generate_track(model, audio_codes, positional_embeddings, attention_mask):\n",
    "    # Flatten and pad audio_codes to MAX_LENGTH\n",
    "    audio_codes = audio_codes.flatten().to(device)\n",
    "    audio_codes = torch.nn.functional.pad(\n",
    "        audio_codes,\n",
    "        (0, MAX_LENGTH - audio_codes.shape[0]),\n",
    "        value=0\n",
    "    )[:MAX_LENGTH]\n",
    "\n",
    "    # Generate attention_mask from padded audio_codes\n",
    "    attention_mask = (audio_codes != 0).long().to(device)\n",
    "\n",
    "    # Ensure positional_embeddings are padded to MAX_LENGTH\n",
    "    if positional_embeddings.shape[0] < MAX_LENGTH:\n",
    "        padding_length = MAX_LENGTH - positional_embeddings.shape[0]\n",
    "        positional_embeddings = torch.nn.functional.pad(\n",
    "            positional_embeddings,\n",
    "            (0, 0, 0, padding_length),\n",
    "            mode=\"constant\",\n",
    "            value=0\n",
    "        )\n",
    "\n",
    "    positional_embeddings = positional_embeddings[:MAX_LENGTH].to(device)\n",
    "\n",
    "\n",
    "    # Pass inputs to the model\n",
    "    with torch.no_grad():\n",
    "        outputs = model(\n",
    "            input_ids=audio_codes.unsqueeze(0),  # [1, MAX_LENGTH]\n",
    "            attention_mask=attention_mask.unsqueeze(0),  # [1, MAX_LENGTH]\n",
    "            positional_embeddings=positional_embeddings.unsqueeze(0)  # [1, MAX_LENGTH, embedding_dim]\n",
    "        )\n",
    "        return outputs.logits.argmax(dim=-1).squeeze().detach().cpu()\n",
    "\n",
    "\n",
    "\n",
    "# Inference\n",
    "inference_files = [\n",
    "    (\"inference.wav\", \"inference_posemb.wav\", \"1\"),\n",
    "    (\"inference2.wav\", \"inference_posemb2.wav\", \"2\"),\n",
    "    (\"inference3.wav\", \"inference_posemb3.wav\", \"3\"),\n",
    "]\n",
    "\n",
    "for audio_path, posemb_path, folder in inference_files:\n",
    "    for track_class in track_classes:\n",
    "        print(f\"Processing {audio_path} -> {track_class} in folder {folder}...\")\n",
    "        model_folder = f'./independent_track_generation_gpt2_checkpointing_{track_class}_model_pos_emb_concat_fulldataset_10sec_embeddings_final'\n",
    "        highest_checkpoint = find_highest_checkpoint(model_folder)\n",
    "        model = CustomGPT2ForConditionalGeneration.from_pretrained(highest_checkpoint).to(device)\n",
    "        model.eval()\n",
    "\n",
    "        audio_codes, audio_length = encode_audio(audio_path)\n",
    "        beats, downbeats = extract_beats_and_downbeats(posemb_path, duration=audio_length)\n",
    "        positional_embeddings = create_positional_embeddings(beats, downbeats, audio_length)\n",
    "\n",
    "        padding_length = MAX_LENGTH - positional_embeddings.shape[0]\n",
    "        positional_embeddings = torch.nn.functional.pad(positional_embeddings, (0, 0, 0, padding_length))\n",
    "\n",
    "        attention_mask = (audio_codes != 0).long()\n",
    "        generated_sequence = generate_track(model, torch.tensor(audio_codes), positional_embeddings, attention_mask)\n",
    "\n",
    "        reshaped_output = generated_sequence.view(4, 750).unsqueeze(0).unsqueeze(0)\n",
    "        decoded_audio = model_encodec.decode(reshaped_output, [None])[0]\n",
    "        decoded_audio = decoded_audio.detach()\n",
    "        decoded_audio = decoded_audio.squeeze(0).squeeze(0)  # Shape: [samples]\n",
    "        decoded_audio = decoded_audio.unsqueeze(0)\n",
    "        output_audio_path = f\"./{model_folder}/{track_class}_{audio_path}_generated.wav\"\n",
    "        os.makedirs(os.path.dirname(output_audio_path), exist_ok=True)\n",
    "        torchaudio.save(output_audio_path, decoded_audio.cpu(), processor.sampling_rate)\n",
    "        print(f\"Saved: {output_audio_path}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
