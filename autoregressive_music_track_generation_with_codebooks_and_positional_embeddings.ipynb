{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "477a9532",
   "metadata": {},
   "source": [
    "# Autoregressive Music Track Generation with Custom GPT-2 Model Using Multiple Codebooks and Positional Embeddings \n",
    "\n",
    "This script implements an autoregressive model for music track generation using a custom GPT-2 architecture. It processes multiple codebooks per timestep and integrates positional embeddings based on beat and downbeat timings to capture complex musical patterns and temporal dynamics. The model is designed to generate specific music tracks (e.g., 'hi_hat') conditioned on vocal audio codes, leveraging the relationships between different codebooks and the rhythmic structure of the music.\n",
    "\n",
    "Key features include a custom dataset class that structures input data as sequences of tokens across multiple codebooks, ensuring temporal alignment. The model uses separate embedding layers for each codebook and sums them to form input embeddings, which are combined with positional embeddings. Training utilizes a custom Trainer class to handle multiple outputs and computes loss by averaging over all codebooks. The script also incorporates Weights & Biases for logging and monitoring, allowing for real-time tracking of training metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2893070f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model for track class: full_instrumental\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ripple\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\accelerate\\accelerator.py:436: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='4730' max='16320' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 4730/16320 13:35 < 33:20, 5.79 it/s, Epoch 34.77/120]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>26.550300</td>\n",
       "      <td>25.589325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>25.472300</td>\n",
       "      <td>24.957373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>25.122800</td>\n",
       "      <td>24.762791</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>24.941500</td>\n",
       "      <td>24.708237</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>24.896000</td>\n",
       "      <td>24.691833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>25.136900</td>\n",
       "      <td>24.686510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>25.077500</td>\n",
       "      <td>24.678108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>24.758400</td>\n",
       "      <td>24.675783</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>25.045000</td>\n",
       "      <td>24.653824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>24.573800</td>\n",
       "      <td>24.625332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>24.763700</td>\n",
       "      <td>24.506668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>24.782400</td>\n",
       "      <td>24.446945</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>24.503300</td>\n",
       "      <td>24.357513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>24.237100</td>\n",
       "      <td>24.322758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>24.044700</td>\n",
       "      <td>24.215981</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>24.305200</td>\n",
       "      <td>24.141186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>24.270100</td>\n",
       "      <td>24.029348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>24.311400</td>\n",
       "      <td>24.016918</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>23.818800</td>\n",
       "      <td>23.941206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>23.711600</td>\n",
       "      <td>23.943317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>23.556900</td>\n",
       "      <td>23.972042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>23.504300</td>\n",
       "      <td>23.836687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>23.149000</td>\n",
       "      <td>23.918856</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>23.189000</td>\n",
       "      <td>23.798849</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>23.395000</td>\n",
       "      <td>23.685465</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>23.276700</td>\n",
       "      <td>23.633718</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>23.057700</td>\n",
       "      <td>23.742912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>22.908900</td>\n",
       "      <td>23.668341</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>22.708800</td>\n",
       "      <td>23.595085</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>22.964300</td>\n",
       "      <td>23.646608</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31</td>\n",
       "      <td>22.430200</td>\n",
       "      <td>23.623320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>22.720800</td>\n",
       "      <td>23.531548</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33</td>\n",
       "      <td>22.564400</td>\n",
       "      <td>23.533018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34</td>\n",
       "      <td>22.674300</td>\n",
       "      <td>23.547028</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [1], line 198\u001b[0m\n\u001b[0;32m    189\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(\n\u001b[0;32m    190\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[0;32m    191\u001b[0m     args\u001b[38;5;241m=\u001b[39mtraining_args,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    194\u001b[0m     eval_dataset\u001b[38;5;241m=\u001b[39mval_dataset,\n\u001b[0;32m    195\u001b[0m )\n\u001b[0;32m    197\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[1;32m--> 198\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    199\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFinished training for track class: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrack_class\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\transformers\\trainer.py:1626\u001b[0m, in \u001b[0;36mTrainer.train\u001b[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[0;32m   1624\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[0;32m   1625\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1626\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1627\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1628\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1629\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1630\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1631\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\transformers\\trainer.py:1965\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[1;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   1962\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39maccumulate(model):\n\u001b[0;32m   1963\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining_step(model, inputs)\n\u001b[1;32m-> 1965\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m   1966\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[0;32m   1967\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_tpu_available()\n\u001b[0;32m   1968\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[0;32m   1969\u001b[0m ):\n\u001b[0;32m   1970\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[0;32m   1971\u001b[0m     tr_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n\u001b[0;32m   1972\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset\n",
    "from transformers import GPT2Config, Trainer, TrainingArguments\n",
    "import torch.nn as nn\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Set the device to CUDA if available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Load your saved .npy file containing the dataset\n",
    "data = np.load('fulldataset_10sec_positional_embs.npy', allow_pickle=True).item()\n",
    "\n",
    "VOCAB_SIZE = 1026\n",
    "SEQ_LEN = 750  # 10 seconds at 75 FPS\n",
    "NUM_CODEBOOKS = 4\n",
    "MAX_LENGTH = SEQ_LEN  # Sequence length per codebook\n",
    "TRACK_CLASSES = ['full_instrumental']\n",
    "\n",
    "\n",
    "# Dataset class for handling multi-codebook data\n",
    "class MusicDataset(Dataset):\n",
    "    def __init__(self, data, track_class):\n",
    "        self.data = {\n",
    "            k: v for k, v in data.items() if track_class in v['generation_data']\n",
    "        }\n",
    "        self.track_class = track_class\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample_id = list(self.data.keys())[idx]\n",
    "        sample = self.data[sample_id]\n",
    "\n",
    "        # Access the vocal codes (input) directly\n",
    "        input_sequence = sample['generation_data'].get('vocal', np.zeros((NUM_CODEBOOKS, SEQ_LEN)))  # Shape: (4, 750)\n",
    "\n",
    "        # Access the track-specific codes (label) directly\n",
    "        label_sequence = sample['generation_data'].get(self.track_class, np.zeros((NUM_CODEBOOKS, SEQ_LEN)))  # Shape: (4, 750)\n",
    "\n",
    "        # Generate attention mask for the input (vocal)\n",
    "        attention_mask = (input_sequence != 0).astype(int)\n",
    "        \n",
    "        return {\n",
    "            'input_ids': torch.tensor(input_sequence, dtype=torch.long),  # Vocal input (4, 750)\n",
    "            'attention_mask': torch.tensor(attention_mask, dtype=torch.long),  # Mask for vocal input\n",
    "            'labels': torch.tensor(label_sequence, dtype=torch.long),  # Track-specific labels (4, 750)\n",
    "        }\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Model definition\n",
    "class CustomGPT2ForMusicGen(nn.Module):\n",
    "    def __init__(self, config, num_codebooks=NUM_CODEBOOKS):\n",
    "        super().__init__()\n",
    "        self.num_codebooks = num_codebooks\n",
    "\n",
    "        # Shared embedding layer for input tokens\n",
    "        self.codebook_embeddings = nn.Embedding(config.vocab_size, config.n_embd)\n",
    "\n",
    "        # Positional embeddings per codebook\n",
    "        self.positional_embeddings = nn.Parameter(\n",
    "            torch.zeros(num_codebooks, config.n_positions, config.n_embd)\n",
    "        )\n",
    "\n",
    "        # Transformer encoder (with causal masking for autoregressive modeling)\n",
    "        self.encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=config.n_embd,\n",
    "            nhead=config.n_head,\n",
    "            dim_feedforward=config.n_embd * 4,\n",
    "            dropout=config.resid_pdrop,\n",
    "        )\n",
    "        self.transformer = nn.TransformerEncoder(self.encoder_layer, num_layers=config.n_layer)\n",
    "\n",
    "        # Output heads for each codebook\n",
    "        self.codebook_heads = nn.ModuleList([\n",
    "            nn.Linear(config.n_embd, config.vocab_size) for _ in range(num_codebooks)\n",
    "        ])\n",
    "\n",
    "    def forward(self, input_ids=None, attention_mask=None, labels=None, **kwargs):\n",
    "        batch_size, num_codebooks, seq_len = input_ids.shape  # (B, 4, 750)\n",
    "\n",
    "        # Embed tokens for all codebooks\n",
    "        input_embeds = self.codebook_embeddings(input_ids)  # Shape: (B, 4, 750, D)\n",
    "\n",
    "        # Add positional embeddings for each codebook\n",
    "        input_embeds = input_embeds + self.positional_embeddings[:, :seq_len, :]\n",
    "\n",
    "        # Flatten codebooks for Transformer input\n",
    "        input_embeds = input_embeds.view(batch_size, num_codebooks * seq_len, -1).permute(1, 0, 2)  # (seq_len, B, D)\n",
    "        attention_mask = attention_mask.view(batch_size, num_codebooks * seq_len)\n",
    "\n",
    "        # Create causal mask for autoregressive generation\n",
    "        seq_len_flat = num_codebooks * seq_len\n",
    "        causal_mask = torch.triu(torch.ones(seq_len_flat, seq_len_flat), diagonal=1).bool()\n",
    "        causal_mask = causal_mask.to(input_embeds.device)\n",
    "\n",
    "        # Transformer forward pass\n",
    "        transformer_outputs = self.transformer(\n",
    "            input_embeds, mask=causal_mask\n",
    "        )  # Shape: (seq_len, B, D)\n",
    "        hidden_states = transformer_outputs.permute(1, 0, 2)  # Back to (B, seq_len, D)\n",
    "\n",
    "        # Reshape back to (B, 4, 750, D)\n",
    "        hidden_states = hidden_states.view(batch_size, num_codebooks, seq_len, -1)\n",
    "\n",
    "        # Compute logits for each codebook\n",
    "        logits = torch.stack([\n",
    "            head(hidden_states[:, i, :, :]) for i, head in enumerate(self.codebook_heads)\n",
    "        ], dim=1)  # Shape: (B, 4, 750, vocab_size)\n",
    "\n",
    "        # Loss computation (if labels are provided)\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            loss_fct = nn.CrossEntropyLoss()\n",
    "            losses = [\n",
    "                loss_fct(logits[:, i, :, :].view(-1, logits.size(-1)), labels[:, i, :].view(-1))\n",
    "                for i in range(num_codebooks)\n",
    "            ]\n",
    "            loss = sum(losses)\n",
    "\n",
    "        return {\n",
    "            \"loss\": loss,\n",
    "            \"logits\": logits,\n",
    "        }\n",
    "\n",
    "\n",
    "# Loop through each track class and train a separate model\n",
    "for track_class in TRACK_CLASSES:\n",
    "    print(f\"Training model for track class: {track_class}\")\n",
    "\n",
    "    # Create dataset for the current track class\n",
    "    dataset = MusicDataset(data, track_class)\n",
    "    train_indices, val_indices = train_test_split(range(len(dataset)), test_size=0.2, random_state=42)\n",
    "    train_dataset = torch.utils.data.Subset(dataset, train_indices)\n",
    "    val_dataset = torch.utils.data.Subset(dataset, val_indices)\n",
    "\n",
    "    # Model configuration\n",
    "    config = GPT2Config(\n",
    "        vocab_size=VOCAB_SIZE,\n",
    "        n_positions=SEQ_LEN,\n",
    "        n_ctx=SEQ_LEN,\n",
    "        n_embd=128,  # Adjust based on available GPU memory\n",
    "        n_layer=6,\n",
    "        n_head=8,\n",
    "        resid_pdrop=0.1,\n",
    "        embd_pdrop=0.1,\n",
    "        attn_pdrop=0.1,\n",
    "    )\n",
    "    model = CustomGPT2ForMusicGen(config=config).to(device)\n",
    "\n",
    "    # Data collator\n",
    "    class DataCollatorWithMultiCodebooks:\n",
    "        def __call__(self, batch):\n",
    "            input_ids = torch.stack([item['input_ids'] for item in batch])  # Shape: (B, 4, 750)\n",
    "            attention_mask = torch.stack([item['attention_mask'] for item in batch])  # Shape: (B, 4, 750)\n",
    "            labels = torch.stack([item['labels'] for item in batch])  # Shape: (B, 4, 750)\n",
    "\n",
    "            return {\n",
    "                'input_ids': input_ids,\n",
    "                'attention_mask': attention_mask,\n",
    "                'labels': labels,\n",
    "            }\n",
    "\n",
    "    data_collator = DataCollatorWithMultiCodebooks()\n",
    "\n",
    "    # Training arguments\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=f\"./checkpoints_{track_class}\",\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        learning_rate=1e-4,\n",
    "        per_device_train_batch_size=1,  # Adjust based on GPU memory\n",
    "        per_device_eval_batch_size=1,\n",
    "        num_train_epochs=120,\n",
    "        weight_decay=0.01,\n",
    "        logging_dir=f\"./logs_{track_class}\",\n",
    "        logging_steps=50,\n",
    "        save_total_limit=3,\n",
    "        load_best_model_at_end=True,\n",
    "        report_to=\"none\",\n",
    "        fp16=True,\n",
    "    )\n",
    "\n",
    "    # Trainer\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        data_collator=data_collator,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=val_dataset,\n",
    "    )\n",
    "\n",
    "    # Train the model\n",
    "    trainer.train()\n",
    "    print(f\"Finished training for track class: {track_class}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "1b9c0d35",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration. Please open a PR/issue to update `preprocessor_config.json` to use `image_processor_type` instead of `feature_extractor_type`. This warning will be removed in v4.40.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of audio_codes before unsqueeze: torch.Size([1, 4, 750])\n",
      "Shape of audio_codes after squeeze: torch.Size([4, 750])\n",
      "Generating track for: hi_hat\n",
      "checkpoints_hi_hat\\checkpoint-3531\n",
      "Config: vocab_size=1026, n_embd=128, n_positions=750\n",
      "Missing keys: []\n",
      "Unexpected keys: []\n",
      "CustomGPT2ForMusicGen(\n",
      "  (codebook_embeddings): Embedding(1026, 128)\n",
      "  (encoder_layer): TransformerEncoderLayer(\n",
      "    (self_attn): MultiheadAttention(\n",
      "      (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
      "    )\n",
      "    (linear1): Linear(in_features=128, out_features=512, bias=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "    (linear2): Linear(in_features=512, out_features=128, bias=True)\n",
      "    (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "    (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "    (dropout1): Dropout(p=0.1, inplace=False)\n",
      "    (dropout2): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (transformer): TransformerEncoder(\n",
      "    (layers): ModuleList(\n",
      "      (0): TransformerEncoderLayer(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
      "        )\n",
      "        (linear1): Linear(in_features=128, out_features=512, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (linear2): Linear(in_features=512, out_features=128, bias=True)\n",
      "        (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout1): Dropout(p=0.1, inplace=False)\n",
      "        (dropout2): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (1): TransformerEncoderLayer(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
      "        )\n",
      "        (linear1): Linear(in_features=128, out_features=512, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (linear2): Linear(in_features=512, out_features=128, bias=True)\n",
      "        (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout1): Dropout(p=0.1, inplace=False)\n",
      "        (dropout2): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (2): TransformerEncoderLayer(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
      "        )\n",
      "        (linear1): Linear(in_features=128, out_features=512, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (linear2): Linear(in_features=512, out_features=128, bias=True)\n",
      "        (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout1): Dropout(p=0.1, inplace=False)\n",
      "        (dropout2): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (3): TransformerEncoderLayer(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
      "        )\n",
      "        (linear1): Linear(in_features=128, out_features=512, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (linear2): Linear(in_features=512, out_features=128, bias=True)\n",
      "        (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout1): Dropout(p=0.1, inplace=False)\n",
      "        (dropout2): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (4): TransformerEncoderLayer(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
      "        )\n",
      "        (linear1): Linear(in_features=128, out_features=512, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (linear2): Linear(in_features=512, out_features=128, bias=True)\n",
      "        (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout1): Dropout(p=0.1, inplace=False)\n",
      "        (dropout2): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (5): TransformerEncoderLayer(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
      "        )\n",
      "        (linear1): Linear(in_features=128, out_features=512, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (linear2): Linear(in_features=512, out_features=128, bias=True)\n",
      "        (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout1): Dropout(p=0.1, inplace=False)\n",
      "        (dropout2): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (codebook_heads): ModuleList(\n",
      "    (0): Linear(in_features=128, out_features=1026, bias=True)\n",
      "    (1): Linear(in_features=128, out_features=1026, bias=True)\n",
      "    (2): Linear(in_features=128, out_features=1026, bias=True)\n",
      "    (3): Linear(in_features=128, out_features=1026, bias=True)\n",
      "  )\n",
      ")\n",
      "Shape of input_ids after unsqueeze: torch.Size([1, 4, 750])\n",
      "Shape of attention_mask: torch.Size([1, 4, 750])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating tokens for hi_hat: 100%|████████████████████████████████████████████████| 750/750 [00:52<00:00, 14.22step/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated sequence for hi_hat: (1, 750, 4)\n",
      "Reshaped output for decoding: torch.Size([1, 1, 4, 750])\n",
      "Saved generated audio for hi_hat to: ./hi_hat_generated.wav\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torchaudio\n",
    "import numpy as np\n",
    "from transformers import GPT2Config, AutoProcessor, EncodecModel\n",
    "import soundfile as sf\n",
    "from tqdm import tqdm\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Load EnCodec model and processor\n",
    "processor = AutoProcessor.from_pretrained(\"facebook/encodec_24khz\")\n",
    "encodec_model = EncodecModel.from_pretrained(\"facebook/encodec_24khz\").eval().to(DEVICE)\n",
    "\n",
    "# List of track classes\n",
    "TRACK_CLASSES = [\"hi_hat\"]\n",
    "\n",
    "\n",
    "# Model definition\n",
    "class CustomGPT2ForMusicGen(nn.Module):\n",
    "    def __init__(self, config, num_codebooks=NUM_CODEBOOKS):\n",
    "        super().__init__()\n",
    "        self.num_codebooks = num_codebooks\n",
    "\n",
    "        # Shared embedding layer for input tokens\n",
    "        self.codebook_embeddings = nn.Embedding(config.vocab_size, config.n_embd)\n",
    "\n",
    "        # Positional embeddings per codebook\n",
    "        self.positional_embeddings = nn.Parameter(\n",
    "            torch.zeros(num_codebooks, config.n_positions, config.n_embd)\n",
    "        )\n",
    "\n",
    "        # Transformer encoder (with causal masking for autoregressive modeling)\n",
    "        self.encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=config.n_embd,\n",
    "            nhead=config.n_head,\n",
    "            dim_feedforward=config.n_embd * 4,\n",
    "            dropout=config.resid_pdrop,\n",
    "        )\n",
    "        self.transformer = nn.TransformerEncoder(self.encoder_layer, num_layers=config.n_layer)\n",
    "\n",
    "        # Output heads for each codebook\n",
    "        self.codebook_heads = nn.ModuleList([\n",
    "            nn.Linear(config.n_embd, config.vocab_size) for _ in range(num_codebooks)\n",
    "        ])\n",
    "\n",
    "    def forward(self, input_ids=None, attention_mask=None, labels=None, **kwargs):\n",
    "        batch_size, num_codebooks, seq_len = input_ids.shape  # (B, 4, 750)\n",
    "\n",
    "        # Embed tokens for all codebooks\n",
    "        input_embeds = self.codebook_embeddings(input_ids)  # Shape: (B, 4, 750, D)\n",
    "\n",
    "        # Add positional embeddings for each codebook\n",
    "        input_embeds = input_embeds + self.positional_embeddings[:, :seq_len, :]\n",
    "\n",
    "        # Flatten codebooks for Transformer input\n",
    "        input_embeds = input_embeds.view(batch_size, num_codebooks * seq_len, -1).permute(1, 0, 2)  # (seq_len, B, D)\n",
    "        attention_mask = attention_mask.view(batch_size, num_codebooks * seq_len)\n",
    "\n",
    "        # Create causal mask for autoregressive generation\n",
    "        seq_len_flat = num_codebooks * seq_len\n",
    "        causal_mask = torch.triu(torch.ones(seq_len_flat, seq_len_flat), diagonal=1).bool()\n",
    "        causal_mask = causal_mask.to(input_embeds.device)\n",
    "\n",
    "        # Transformer forward pass\n",
    "        transformer_outputs = self.transformer(\n",
    "            input_embeds, mask=causal_mask\n",
    "        )  # Shape: (seq_len, B, D)\n",
    "        hidden_states = transformer_outputs.permute(1, 0, 2)  # Back to (B, seq_len, D)\n",
    "\n",
    "        # Reshape back to (B, 4, 750, D)\n",
    "        hidden_states = hidden_states.view(batch_size, num_codebooks, seq_len, -1)\n",
    "\n",
    "        # Compute logits for each codebook\n",
    "        logits = torch.stack([\n",
    "            head(hidden_states[:, i, :, :]) for i, head in enumerate(self.codebook_heads)\n",
    "        ], dim=1)  # Shape: (B, 4, 750, vocab_size)\n",
    "\n",
    "        # Loss computation (if labels are provided)\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            loss_fct = nn.CrossEntropyLoss()\n",
    "            losses = [\n",
    "                loss_fct(logits[:, i, :, :].view(-1, logits.size(-1)), labels[:, i, :].view(-1))\n",
    "                for i in range(num_codebooks)\n",
    "            ]\n",
    "            loss = sum(losses)\n",
    "\n",
    "        return {\n",
    "            \"loss\": loss,\n",
    "            \"logits\": logits,\n",
    "        }\n",
    "\n",
    "# Function to encode input audio\n",
    "def encode_audio(audio_path, model, processor, device, sequence_length=750):\n",
    "    \"\"\"\n",
    "    Encodes a vocal audio file into audio codes using Facebook's EnCodec.\n",
    "\n",
    "    Returns:\n",
    "    - torch.Tensor: Encoded audio codes of shape (4, 750).\n",
    "    \"\"\"\n",
    "    # Load and preprocess audio\n",
    "    audio, rate = torchaudio.load(audio_path)\n",
    "    if rate != 24000:\n",
    "        audio = torchaudio.transforms.Resample(orig_freq=rate, new_freq=24000)(audio)\n",
    "    audio = torch.nn.functional.pad(audio, (0, int(24000 * 10 - audio.size(1))), \"constant\")\n",
    "    if audio.size(0) > 1:\n",
    "        audio = audio.mean(dim=0)  # Convert stereo to mono\n",
    "\n",
    "    # Process and encode audio\n",
    "    inputs = processor(audio.numpy(), sampling_rate=24000, return_tensors=\"pt\")\n",
    "    inputs = {key: val.to(device) for key, val in inputs.items()}\n",
    "    with torch.no_grad():\n",
    "        encoded = model.encode(inputs[\"input_values\"], inputs[\"padding_mask\"], 3)\n",
    "\n",
    "    # Truncate or pad to match the expected sequence length\n",
    "    codes = encoded.audio_codes.squeeze(0)  # Shape: (4, ?)\n",
    "    if codes.size(-1) < sequence_length:\n",
    "        codes = torch.nn.functional.pad(codes, (0, sequence_length - codes.size(-1)), \"constant\")\n",
    "    elif codes.size(-1) > sequence_length:\n",
    "        codes = codes[:, :sequence_length]\n",
    "\n",
    "    return codes\n",
    "\n",
    "\n",
    "# Function to decode model output\n",
    "def decode_generated_sequence(logits, temperature=1.0, top_k=10):\n",
    "    \"\"\"\n",
    "    Decode logits into discrete codebook values using sampling.\n",
    "    \"\"\"\n",
    "    # Ensure logits have the correct shape: (batch_size, vocab_size)\n",
    "    probabilities = torch.nn.functional.softmax(logits / temperature, dim=-1)  # Shape: (batch_size, vocab_size)\n",
    "    \n",
    "    # Get top_k probabilities and indices\n",
    "    top_prob, top_idx = probabilities.topk(top_k, dim=-1)  # Shape: (batch_size, top_k)\n",
    "\n",
    "    # Sample from the top_k probabilities\n",
    "    sampled_indices = torch.multinomial(top_prob, num_samples=1)  # Shape: (batch_size, 1)\n",
    "\n",
    "    # Map sampled indices back to original vocab indices\n",
    "    decoded_tokens = top_idx.gather(-1, sampled_indices).squeeze(-1)  # Shape: (batch_size)\n",
    "\n",
    "    return decoded_tokens.cpu().numpy()\n",
    "\n",
    "\n",
    "\n",
    "# Function to save generated output as audio\n",
    "def save_as_audio(sequence, output_path):\n",
    "    \"\"\"\n",
    "    Convert generated sequence into an audio file and save.\n",
    "    \"\"\"\n",
    "    # Placeholder: In real implementation, map codebooks back to audio\n",
    "    reconstructed_audio = np.zeros((75000,))  # Dummy audio data\n",
    "    sf.write(output_path, reconstructed_audio, samplerate=24000)\n",
    "\n",
    "from safetensors.torch import load_file as load_safetensors\n",
    "\n",
    "def load_trained_model(checkpoint_dir, vocab_size=1026, seq_len=750, num_codebooks=4):\n",
    "    \"\"\"\n",
    "    Load the trained CustomGPT2ForMusicGen model from a checkpoint.\n",
    "\n",
    "    Parameters:\n",
    "    - checkpoint_dir (str): Directory containing model checkpoints.\n",
    "    - vocab_size (int): Vocabulary size used in the model.\n",
    "    - seq_len (int): Sequence length used during training.\n",
    "    - num_codebooks (int): Number of codebooks.\n",
    "\n",
    "    Returns:\n",
    "    - CustomGPT2ForMusicGen: The loaded model instance.\n",
    "    \"\"\"\n",
    "    # Define the configuration\n",
    "    config = GPT2Config(\n",
    "        vocab_size=vocab_size,\n",
    "        n_positions=seq_len,\n",
    "        n_ctx=seq_len,\n",
    "        n_embd=128,\n",
    "        n_layer=6,\n",
    "        n_head=8,\n",
    "        resid_pdrop=0.1,\n",
    "        embd_pdrop=0.1,\n",
    "        attn_pdrop=0.1,\n",
    "    )\n",
    "\n",
    "    # Debugging: Print configuration details\n",
    "    print(f\"Config: vocab_size={config.vocab_size}, n_embd={config.n_embd}, n_positions={config.n_positions}\")\n",
    "\n",
    "    # Initialize the model\n",
    "    model = CustomGPT2ForMusicGen(config=config, num_codebooks=num_codebooks)\n",
    "\n",
    "    # Load weights from model.safetensors\n",
    "    model_weights_path = os.path.join(checkpoint_dir, \"model.safetensors\")\n",
    "    if not os.path.exists(model_weights_path):\n",
    "        raise FileNotFoundError(f\"model.safetensors not found in {checkpoint_dir}\")\n",
    "\n",
    "    # Load the state dictionary from the safetensors file\n",
    "    state_dict = load_safetensors(model_weights_path)\n",
    "\n",
    "    # Load weights into the model\n",
    "    model.load_state_dict(state_dict)\n",
    "    missing_keys, unexpected_keys = model.load_state_dict(state_dict, strict=False)\n",
    "    print(\"Missing keys:\", missing_keys)\n",
    "    print(\"Unexpected keys:\", unexpected_keys)\n",
    "    print(model)\n",
    "    model.eval()\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def find_highest_checkpoint(folder):\n",
    "    \"\"\"\n",
    "    Find the highest-numbered checkpoint in a given folder.\n",
    "    \n",
    "    Parameters:\n",
    "    - folder (str): The directory containing the checkpoints.\n",
    "\n",
    "    Returns:\n",
    "    - str: The path to the highest-numbered checkpoint.\n",
    "    \"\"\"\n",
    "    checkpoints = [d for d in os.listdir(folder) if d.startswith(\"checkpoint-\")]\n",
    "    if not checkpoints:\n",
    "        raise ValueError(f\"No checkpoints found in {folder}\")\n",
    "    checkpoints = sorted(checkpoints, key=lambda x: int(x.split(\"-\")[-1]), reverse=True)\n",
    "    return checkpoints[0]  # Return only the folder name of the highest checkpoint\n",
    "\n",
    "\n",
    "# Main inference function\n",
    "def generate_tracks(vocal_path, output_dir):\n",
    "    \"\"\"\n",
    "    Generate instrumental tracks for all track classes from a given vocal input.\n",
    "    \"\"\"\n",
    "    # Encode vocal input\n",
    "    audio_codes = encode_audio(vocal_path, encodec_model, processor, DEVICE)  # Shape: (4, 750)\n",
    "    \n",
    "    print(f\"Shape of audio_codes before unsqueeze: {audio_codes.shape}\")\n",
    "    \n",
    "    # Remove extra batch dimension if present\n",
    "    if len(audio_codes.shape) == 3 and audio_codes.shape[0] == 1:\n",
    "        audio_codes = audio_codes.squeeze(0)  # Shape becomes (4, 750)\n",
    "        \n",
    "    print(f\"Shape of audio_codes after squeeze: {audio_codes.shape}\")\n",
    "\n",
    "    for track_class in TRACK_CLASSES:\n",
    "        print(f\"Generating track for: {track_class}\")\n",
    "\n",
    "        # Locate the highest checkpoint\n",
    "        checkpoint_folder = f\"checkpoints_{track_class}\"\n",
    "        try:\n",
    "            checkpoint_dir = find_highest_checkpoint(checkpoint_folder)\n",
    "        except ValueError as e:\n",
    "            print(f\"Skipping {track_class} due to error: {e}\")\n",
    "            continue\n",
    "            \n",
    "        checkpoint_dir = os.path.join(checkpoint_folder, checkpoint_dir) \n",
    "        print(checkpoint_dir)\n",
    "        # Load the model\n",
    "        model = load_trained_model(checkpoint_dir, vocab_size=VOCAB_SIZE, seq_len=SEQ_LEN, num_codebooks=NUM_CODEBOOKS)\n",
    "        model = model.to(DEVICE)\n",
    "\n",
    "        # Prepare input for the model\n",
    "        input_ids = audio_codes.unsqueeze(0)  # Add batch dimension again, final shape: (1, 4, 750)\n",
    "        print(f\"Shape of input_ids after unsqueeze: {input_ids.shape}\")\n",
    "        attention_mask = (input_ids != 0).long()\n",
    "        print(f\"Shape of attention_mask: {attention_mask.shape}\")\n",
    "\n",
    "        # Generate sequence autoregressively\n",
    "        generated_sequence = []\n",
    "        with torch.no_grad():\n",
    "            for step in tqdm(range(input_ids.size(-1)), desc=f\"Generating tokens for {track_class}\", unit=\"step\"):\n",
    "                # Forward pass through the model\n",
    "                outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "\n",
    "                # Extract logits for the current step\n",
    "                logits = outputs[\"logits\"][:, :, step, :]  # Shape: (batch_size, num_codebooks, vocab_size)\n",
    "                logits = logits.view(-1, logits.size(-1))  # Flatten to (batch_size * num_codebooks, vocab_size)\n",
    "\n",
    "                # Decode the logits to get next tokens\n",
    "                next_tokens = decode_generated_sequence(logits)\n",
    "\n",
    "                # Reshape next_tokens to match the expected input_ids structure\n",
    "                next_tokens = torch.tensor(next_tokens).view(input_ids.size(0), input_ids.size(1))\n",
    "\n",
    "                # Append tokens to the generated sequence\n",
    "                generated_sequence.append(next_tokens)\n",
    "\n",
    "                # Update input_ids for the next step\n",
    "                if step < input_ids.size(-1) - 1:\n",
    "                    input_ids[:, :, step + 1] = next_tokens\n",
    "\n",
    "\n",
    "        # Convert generated sequence to audio\n",
    "        generated_sequence = np.stack(generated_sequence, axis=1)  # Shape: (4, 750)\n",
    "        print(f\"Generated sequence for {track_class}:\", generated_sequence.shape)\n",
    "        reshaped_output = torch.tensor(generated_sequence, dtype=torch.float32).squeeze(0)  # Shape: (750, 4)\n",
    "        reshaped_output = reshaped_output.permute(1, 0).unsqueeze(0).unsqueeze(0)  # Final shape: (1, 1, 4, 750)\n",
    "        print(f\"Reshaped output for decoding: {reshaped_output.shape}\")\n",
    "        reshaped_output = reshaped_output.to(torch.long).to(DEVICE)\n",
    "        # Decode the audio using encodec_model\n",
    "        decoded_audio = encodec_model.decode(reshaped_output, [None])[0]  # Shape: (samples,)\n",
    "        decoded_audio = decoded_audio.detach()\n",
    "        decoded_audio = decoded_audio.squeeze(0).squeeze(0)  # Shape: [samples]\n",
    "        decoded_audio = decoded_audio.unsqueeze(0)\n",
    "\n",
    "        # Save the decoded audio\n",
    "        output_audio_path = f\"./{track_class}_generated.wav\"\n",
    "        os.makedirs(os.path.dirname(output_audio_path), exist_ok=True)\n",
    "        torchaudio.save(output_audio_path, decoded_audio.cpu(), processor.sampling_rate)\n",
    "        print(f\"Saved generated audio for {track_class} to: {output_audio_path}\")\n",
    "\n",
    "\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    vocal_path = \"inference.wav\"  # Path to input vocal file\n",
    "    output_dir = \"./generated_tracks_codebooks\"  # Directory to save generated tracks\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    generate_tracks(vocal_path, output_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e221c57",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
