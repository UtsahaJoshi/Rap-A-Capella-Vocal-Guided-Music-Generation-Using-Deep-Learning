{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "477a9532",
   "metadata": {},
   "source": [
    "# Training a GPT-2 Model for Music Generation Using 1-Second Audio Chunks\n",
    "\n",
    "This script trains a GPT-2-based model for music track generation using 1-second audio chunks, offering finer granularity compared to the usual 10-second training chunks. The dataset comprises 10-second audio embeddings, which are split into smaller 1-second sequential chunks (75 tokens each) for training. This approach enables the model to learn short-term musical patterns more effectively while still leveraging positional embeddings to capture temporal dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "70c60bdf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:s3mmf0yi) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <style>\n",
       "        .wandb-row {\n",
       "            display: flex;\n",
       "            flex-direction: row;\n",
       "            flex-wrap: wrap;\n",
       "            justify-content: flex-start;\n",
       "            width: 100%;\n",
       "        }\n",
       "        .wandb-col {\n",
       "            display: flex;\n",
       "            flex-direction: column;\n",
       "            flex-basis: 100%;\n",
       "            flex: 1;\n",
       "            padding: 10px;\n",
       "        }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>██▄▂▂▁▁▁▁▁</td></tr><tr><td>eval/perplexity</td><td>██▃▂▁▁▁▁▁▁</td></tr><tr><td>eval/runtime</td><td>▆▅▃▆▅▄█▂▁▆</td></tr><tr><td>eval/samples_per_second</td><td>▂▃▆▂▃▄▁▇█▃</td></tr><tr><td>eval/steps_per_second</td><td>▂▃▆▂▃▄▁▇█▃</td></tr><tr><td>train/epoch</td><td>▁▁▁▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇▇▇█</td></tr><tr><td>train/global_step</td><td>▁▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▃▃▄▄▅▅▅▅▅▆▆▆▆▆▆▆▆▆▇▇▇██</td></tr><tr><td>train/grad_norm</td><td>▁▂▂▄▃▂▄▃▅▄▄▃▃▅▅▆▃▁▆▂▁▄▅▄██▃▁▄▅▄▂▂▁▁▁▅▂▅▁</td></tr><tr><td>train/learning_rate</td><td>██████▇▇▇▇▆▆▆▆▆▆▅▅▅▅▅▅▅▅▄▄▄▄▄▃▃▂▂▂▂▁▁▁▁▁</td></tr><tr><td>train/loss</td><td>█▆▄▄▄▃▄▄▄▄▃▄▃▃▃▃▂▂▂▂▁▂▂▂▁▁▁▁▂▁▁▂▁▁▁▂▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>2.87806</td></tr><tr><td>eval/perplexity</td><td>17.77974</td></tr><tr><td>eval/runtime</td><td>2.5182</td></tr><tr><td>eval/samples_per_second</td><td>101.658</td></tr><tr><td>eval/steps_per_second</td><td>101.658</td></tr><tr><td>train/epoch</td><td>10.16</td></tr><tr><td>train/global_step</td><td>10400</td></tr><tr><td>train/grad_norm</td><td>5.89472</td></tr><tr><td>train/learning_rate</td><td>9e-05</td></tr><tr><td>train/loss</td><td>2.6429</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">sequential_memory_training_hi_hat_run</strong> at: <a href='https://wandb.ai/uniqlabs/music_generation_with_memory/runs/s3mmf0yi' target=\"_blank\">https://wandb.ai/uniqlabs/music_generation_with_memory/runs/s3mmf0yi</a><br/> View project at: <a href='https://wandb.ai/uniqlabs/music_generation_with_memory' target=\"_blank\">https://wandb.ai/uniqlabs/music_generation_with_memory</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20241123_143833-s3mmf0yi\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:s3mmf0yi). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>C:\\Users\\Ripple\\Desktop\\5th sem\\practical work\\code\\wandb\\run-20241123_151902-kygorb20</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/uniqlabs/music_generation_with_memory/runs/kygorb20' target=\"_blank\">sunny-eon-24</a></strong> to <a href='https://wandb.ai/uniqlabs/music_generation_with_memory' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/uniqlabs/music_generation_with_memory' target=\"_blank\">https://wandb.ai/uniqlabs/music_generation_with_memory</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/uniqlabs/music_generation_with_memory/runs/kygorb20' target=\"_blank\">https://wandb.ai/uniqlabs/music_generation_with_memory/runs/kygorb20</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for hi_hat with sequential memory...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:kygorb20) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">sunny-eon-24</strong> at: <a href='https://wandb.ai/uniqlabs/music_generation_with_memory/runs/kygorb20' target=\"_blank\">https://wandb.ai/uniqlabs/music_generation_with_memory/runs/kygorb20</a><br/> View project at: <a href='https://wandb.ai/uniqlabs/music_generation_with_memory' target=\"_blank\">https://wandb.ai/uniqlabs/music_generation_with_memory</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20241123_151902-kygorb20\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:kygorb20). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>C:\\Users\\Ripple\\Desktop\\5th sem\\practical work\\code\\wandb\\run-20241123_151905-kh02h6d9</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/uniqlabs/music_generation_with_memory/runs/kh02h6d9' target=\"_blank\">sequential_memory_training_hi_hat_run</a></strong> to <a href='https://wandb.ai/uniqlabs/music_generation_with_memory' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/uniqlabs/music_generation_with_memory' target=\"_blank\">https://wandb.ai/uniqlabs/music_generation_with_memory</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/uniqlabs/music_generation_with_memory/runs/kh02h6d9' target=\"_blank\">https://wandb.ai/uniqlabs/music_generation_with_memory/runs/kh02h6d9</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ripple\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\accelerate\\accelerator.py:436: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='44379' max='122880' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 44379/122880 28:27 < 50:20, 25.99 it/s, Epoch 43.34/120]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Perplexity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>3.793000</td>\n",
       "      <td>3.980779</td>\n",
       "      <td>53.558750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>3.844400</td>\n",
       "      <td>3.770371</td>\n",
       "      <td>43.396179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>3.423200</td>\n",
       "      <td>3.348563</td>\n",
       "      <td>28.461796</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>3.054300</td>\n",
       "      <td>3.102402</td>\n",
       "      <td>22.251339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>2.810000</td>\n",
       "      <td>2.964585</td>\n",
       "      <td>19.386648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>2.737900</td>\n",
       "      <td>2.939131</td>\n",
       "      <td>18.899406</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>2.559700</td>\n",
       "      <td>2.912565</td>\n",
       "      <td>18.403934</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>2.971000</td>\n",
       "      <td>2.862559</td>\n",
       "      <td>17.506273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>2.792500</td>\n",
       "      <td>2.876948</td>\n",
       "      <td>17.759989</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>2.578400</td>\n",
       "      <td>2.870871</td>\n",
       "      <td>17.652384</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>2.380300</td>\n",
       "      <td>2.849771</td>\n",
       "      <td>17.283823</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>2.785100</td>\n",
       "      <td>2.876434</td>\n",
       "      <td>17.750858</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>2.434700</td>\n",
       "      <td>2.835967</td>\n",
       "      <td>17.046883</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>2.598500</td>\n",
       "      <td>2.892608</td>\n",
       "      <td>18.040297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>2.397700</td>\n",
       "      <td>2.874147</td>\n",
       "      <td>17.710306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>2.609600</td>\n",
       "      <td>2.874198</td>\n",
       "      <td>17.711214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>2.455500</td>\n",
       "      <td>2.848510</td>\n",
       "      <td>17.262047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>2.376000</td>\n",
       "      <td>2.814721</td>\n",
       "      <td>16.688513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>2.544900</td>\n",
       "      <td>2.886373</td>\n",
       "      <td>17.928171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>2.327900</td>\n",
       "      <td>2.841629</td>\n",
       "      <td>17.143671</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>2.191800</td>\n",
       "      <td>2.854716</td>\n",
       "      <td>17.369501</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>2.327200</td>\n",
       "      <td>2.868309</td>\n",
       "      <td>17.607224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>2.366300</td>\n",
       "      <td>2.851029</td>\n",
       "      <td>17.305574</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>2.053900</td>\n",
       "      <td>2.852224</td>\n",
       "      <td>17.326271</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>2.347700</td>\n",
       "      <td>2.871464</td>\n",
       "      <td>17.662867</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>2.167200</td>\n",
       "      <td>2.890430</td>\n",
       "      <td>18.001057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>2.090000</td>\n",
       "      <td>2.945209</td>\n",
       "      <td>19.014641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>2.212100</td>\n",
       "      <td>2.902758</td>\n",
       "      <td>18.224333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>1.967200</td>\n",
       "      <td>2.900717</td>\n",
       "      <td>18.187189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>2.226200</td>\n",
       "      <td>2.850047</td>\n",
       "      <td>17.288601</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31</td>\n",
       "      <td>2.252000</td>\n",
       "      <td>2.850778</td>\n",
       "      <td>17.301231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>1.989900</td>\n",
       "      <td>2.896238</td>\n",
       "      <td>18.105904</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33</td>\n",
       "      <td>2.126400</td>\n",
       "      <td>2.889415</td>\n",
       "      <td>17.982786</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34</td>\n",
       "      <td>2.067000</td>\n",
       "      <td>2.922828</td>\n",
       "      <td>18.593796</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>2.089100</td>\n",
       "      <td>2.917475</td>\n",
       "      <td>18.494520</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36</td>\n",
       "      <td>2.023400</td>\n",
       "      <td>2.909298</td>\n",
       "      <td>18.343920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37</td>\n",
       "      <td>1.937500</td>\n",
       "      <td>2.854228</td>\n",
       "      <td>17.361031</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38</td>\n",
       "      <td>1.951100</td>\n",
       "      <td>2.928850</td>\n",
       "      <td>18.706100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39</td>\n",
       "      <td>1.935700</td>\n",
       "      <td>2.929221</td>\n",
       "      <td>18.713041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>1.812900</td>\n",
       "      <td>2.915750</td>\n",
       "      <td>18.462646</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41</td>\n",
       "      <td>2.081000</td>\n",
       "      <td>2.919161</td>\n",
       "      <td>18.525743</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42</td>\n",
       "      <td>1.711400</td>\n",
       "      <td>2.905466</td>\n",
       "      <td>18.273764</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43</td>\n",
       "      <td>1.980300</td>\n",
       "      <td>2.963628</td>\n",
       "      <td>19.368109</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [21], line 248\u001b[0m\n\u001b[0;32m    238\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(\n\u001b[0;32m    239\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[0;32m    240\u001b[0m     args\u001b[38;5;241m=\u001b[39mtraining_args,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    244\u001b[0m     compute_metrics\u001b[38;5;241m=\u001b[39mcompute_metrics,  \u001b[38;5;66;03m# Optional: Remove if not using\u001b[39;00m\n\u001b[0;32m    245\u001b[0m )\n\u001b[0;32m    247\u001b[0m \u001b[38;5;66;03m# Start training\u001b[39;00m\n\u001b[1;32m--> 248\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    250\u001b[0m \u001b[38;5;66;03m# Evaluate the model\u001b[39;00m\n\u001b[0;32m    251\u001b[0m eval_results \u001b[38;5;241m=\u001b[39m trainer\u001b[38;5;241m.\u001b[39mevaluate()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\transformers\\trainer.py:1626\u001b[0m, in \u001b[0;36mTrainer.train\u001b[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[0;32m   1624\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[0;32m   1625\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1626\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1627\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1628\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1629\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1630\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1631\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\transformers\\trainer.py:1963\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[1;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   1960\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_step_begin(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n\u001b[0;32m   1962\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39maccumulate(model):\n\u001b[1;32m-> 1963\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1965\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m   1966\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[0;32m   1967\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_tpu_available()\n\u001b[0;32m   1968\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[0;32m   1969\u001b[0m ):\n\u001b[0;32m   1970\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[0;32m   1971\u001b[0m     tr_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\transformers\\trainer.py:2886\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[1;34m(self, model, inputs)\u001b[0m\n\u001b[0;32m   2884\u001b[0m         scaled_loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m   2885\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 2886\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maccelerator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2888\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\u001b[38;5;241m.\u001b[39mdetach() \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mgradient_accumulation_steps\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\accelerate\\accelerator.py:2011\u001b[0m, in \u001b[0;36mAccelerator.backward\u001b[1;34m(self, loss, **kwargs)\u001b[0m\n\u001b[0;32m   2009\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m   2010\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscaler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 2011\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscaler\u001b[38;5;241m.\u001b[39mscale(loss)\u001b[38;5;241m.\u001b[39mbackward(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   2012\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   2013\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\_tensor.py:396\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    387\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    388\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    389\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    390\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    394\u001b[0m         create_graph\u001b[38;5;241m=\u001b[39mcreate_graph,\n\u001b[0;32m    395\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs)\n\u001b[1;32m--> 396\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\autograd\\__init__.py:173\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    168\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    170\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[0;32m    171\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    172\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 173\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    174\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    175\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset\n",
    "from transformers import GPT2LMHeadModel, GPT2Config, Trainer, TrainingArguments\n",
    "from torch.optim import AdamW\n",
    "from transformers import get_cosine_schedule_with_warmup\n",
    "from sklearn.model_selection import train_test_split\n",
    "import wandb\n",
    "import torch.nn as nn\n",
    "\n",
    "VOCAB_SIZE = 1026\n",
    "CHUNK_LENGTH = 75  # 1 second = 75 tokens\n",
    "track_classes = ['hi_hat']\n",
    "\n",
    "# Load your saved .npy file\n",
    "data = np.load(\n",
    "    'processed_tracks_data_final_10secs_embeddings_standardized.npy',\n",
    "    allow_pickle=True\n",
    ").item()\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "class SequentialMusicDataset(Dataset):\n",
    "    def __init__(self, data, track_class, chunk_length=CHUNK_LENGTH):\n",
    "        \"\"\"\n",
    "        Initializes the dataset.\n",
    "\n",
    "        Args:\n",
    "            data (dict): Loaded data dictionary.\n",
    "            track_class (str): The track class to process.\n",
    "            chunk_length (int): Length of each chunk.\n",
    "        \"\"\"\n",
    "        self.track_class = track_class\n",
    "        self.chunk_length = chunk_length\n",
    "        self.data = {\n",
    "            k: v for k, v in data.items()\n",
    "            if self.track_class in v['generation_data']\n",
    "        }\n",
    "        self.samples = []\n",
    "\n",
    "        # Preprocess all samples to include positional embeddings\n",
    "        for sample_id, sample in self.data.items():\n",
    "            # Retrieve precomputed positional embeddings\n",
    "            positional_embedding = sample.get('positional_embedding')\n",
    "            if positional_embedding is None:\n",
    "                print(f\"Warning: No 'positional_embedding' for sample_id {sample_id}. Skipping.\")\n",
    "                continue\n",
    "\n",
    "            # Get track data and split into chunks\n",
    "            track_data = sample['generation_data'][self.track_class].flatten()\n",
    "            vocal_audio_codes = sample['generation_data'].get('vocal', np.zeros((4, chunk_length)))\n",
    "\n",
    "            # Clip values in audio codes to ensure valid range\n",
    "            track_data = np.clip(track_data, 0, VOCAB_SIZE - 1)\n",
    "            vocal_audio_codes = np.clip(vocal_audio_codes, 0, VOCAB_SIZE - 1)\n",
    "\n",
    "            # Split data into chunks\n",
    "            num_chunks = int(np.ceil(len(track_data) / self.chunk_length))\n",
    "            track_chunks = np.array_split(track_data, num_chunks)\n",
    "            vocal_chunks = np.array_split(vocal_audio_codes.flatten(), num_chunks)\n",
    "\n",
    "            # Pad each chunk to CHUNK_LENGTH\n",
    "            track_chunks = [\n",
    "                np.pad(chunk, (0, self.chunk_length - len(chunk)), 'constant', constant_values=0)\n",
    "                for chunk in track_chunks\n",
    "            ]\n",
    "            vocal_chunks = [\n",
    "                np.pad(chunk, (0, self.chunk_length - len(chunk)), 'constant', constant_values=0)\n",
    "                for chunk in vocal_chunks\n",
    "            ]\n",
    "\n",
    "            # Convert lists of arrays to single NumPy arrays for efficient tensor creation\n",
    "            track_chunks = np.array(track_chunks)\n",
    "            vocal_chunks = np.array(vocal_chunks)\n",
    "\n",
    "            # Split positional_embeddings into chunks\n",
    "            # Assuming positional_embedding has shape [MAX_LENGTH, embedding_dim]\n",
    "            # and chunk_length corresponds to sequential segments\n",
    "            # Here, we assume each chunk corresponds to 'CHUNK_LENGTH' tokens\n",
    "            # Thus, positional_embeddings for a chunk are positional_embedding[i*CHUNK_LENGTH : (i+1)*CHUNK_LENGTH]\n",
    "            positional_chunks = np.array_split(positional_embedding, num_chunks)\n",
    "            positional_chunks = [\n",
    "                np.pad(chunk, ((0, self.chunk_length - chunk.shape[0]), (0,0)), 'constant', constant_values=0)\n",
    "                for chunk in positional_chunks\n",
    "            ]\n",
    "            positional_chunks = np.array(positional_chunks)\n",
    "            positional_chunks = torch.tensor(positional_chunks, dtype=torch.float)  # [num_chunks, CHUNK_LENGTH, embedding_dim]\n",
    "\n",
    "            # Append to samples\n",
    "            for input_ids, labels, pos_emb in zip(vocal_chunks, track_chunks, positional_chunks):\n",
    "                self.samples.append({\n",
    "                    'input_ids': torch.tensor(input_ids, dtype=torch.long),\n",
    "                    'labels': torch.tensor(labels, dtype=torch.long),\n",
    "                    'positional_embeddings': pos_emb\n",
    "                })\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.samples[idx]\n",
    "\n",
    "\n",
    "# Custom GPT-2 model with positional embeddings\n",
    "class CustomGPT2ForConditionalGeneration(GPT2LMHeadModel):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        # Projection layer to match positional embeddings to model's embedding size\n",
    "        self.positional_proj = nn.Linear(128, config.n_embd)  # Assuming positional_embeddings have dim 64\n",
    "\n",
    "    def forward(self, input_ids=None, attention_mask=None, labels=None,\n",
    "                past_key_values=None, positional_embeddings=None, **kwargs):\n",
    "        \"\"\"\n",
    "        Forward pass that incorporates positional embeddings.\n",
    "\n",
    "        Args:\n",
    "            input_ids (torch.Tensor): Input token IDs.\n",
    "            attention_mask (torch.Tensor): Attention mask.\n",
    "            labels (torch.Tensor): Labels for language modeling.\n",
    "            past_key_values (tuple): Past key and value states.\n",
    "            positional_embeddings (torch.Tensor): Positional embeddings to add to input embeddings.\n",
    "\n",
    "        Returns:\n",
    "            CausalLMOutputWithCrossAttentions: Model outputs.\n",
    "        \"\"\"\n",
    "        # Get input embeddings\n",
    "        input_embeds = self.transformer.wte(input_ids)  # [batch, seq_length, n_embd]\n",
    "\n",
    "        if positional_embeddings is not None:\n",
    "            # Project positional embeddings to match n_embd\n",
    "            pos_emb_proj = self.positional_proj(positional_embeddings)  # [batch, seq_length, n_embd]\n",
    "            input_embeds = input_embeds + pos_emb_proj  # [batch, seq_length, n_embd]\n",
    "\n",
    "        # Proceed with the standard GPT-2 forward pass using inputs_embeds\n",
    "        return super().forward(\n",
    "            inputs_embeds=input_embeds,\n",
    "            attention_mask=attention_mask,\n",
    "            labels=labels,\n",
    "            past_key_values=past_key_values,\n",
    "            **kwargs\n",
    "        )\n",
    "\n",
    "\n",
    "# Custom data collator to handle batches\n",
    "class DataCollatorWithPositionalEmbeddings:\n",
    "    def __call__(self, batch):\n",
    "        input_ids = torch.stack([item['input_ids'] for item in batch])  # [batch_size, CHUNK_LENGTH]\n",
    "        labels = torch.stack([item['labels'] for item in batch])        # [batch_size, CHUNK_LENGTH]\n",
    "        positional_embeddings = torch.stack([item['positional_embeddings'] for item in batch])  # [batch_size, CHUNK_LENGTH, embedding_dim]\n",
    "        \n",
    "        # Create attention mask: 1 where input_ids != 0, else 0\n",
    "        attention_mask = (input_ids != 0).long()  # [batch_size, CHUNK_LENGTH]\n",
    "        \n",
    "        return {\n",
    "            'input_ids': input_ids,\n",
    "            'attention_mask': attention_mask,\n",
    "            'labels': labels,\n",
    "            'positional_embeddings': positional_embeddings\n",
    "        }\n",
    "\n",
    "\n",
    "# Initialize the model configuration\n",
    "config = GPT2Config(\n",
    "    vocab_size=VOCAB_SIZE,\n",
    "    n_positions=3000,  # Adjusted for maximum sequence length\n",
    "    n_ctx=3000,         # Adjusted for maximum context length\n",
    "    n_embd=128,         # Embedding size\n",
    "    n_layer=6,          # Number of transformer layers\n",
    "    n_head=8,           # Number of attention heads\n",
    "    activation_function='gelu',\n",
    "    resid_pdrop=0.1,\n",
    "    embd_pdrop=0.1,\n",
    "    attn_pdrop=0.1,\n",
    ")\n",
    "\n",
    "# Initialize the custom GPT-2 model\n",
    "model = CustomGPT2ForConditionalGeneration(config=config).to(device)\n",
    "\n",
    "# Initialize Weights & Biases\n",
    "wandb.init(project='music_generation_with_memory')\n",
    "\n",
    "# Define a simple compute_metrics function (optional)\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    shift_logits = torch.tensor(logits).transpose(0, 1)\n",
    "    shift_labels = torch.tensor(labels).transpose(0, 1)\n",
    "    \n",
    "    loss_fct = nn.CrossEntropyLoss(ignore_index=0, reduction='mean')\n",
    "    loss = loss_fct(shift_logits.reshape(-1, shift_logits.size(-1)), shift_labels.reshape(-1))\n",
    "    \n",
    "    perplexity = torch.exp(loss)\n",
    "    return {\"perplexity\": perplexity.item(), \"loss\": loss.item()}\n",
    "\n",
    "\n",
    "# Training loop with sequential memory using Trainer\n",
    "for track_idx, track in enumerate(track_classes):\n",
    "    print(f\"Training for {track} with sequential memory...\")\n",
    "\n",
    "    # Create dataset for the current track\n",
    "    dataset = SequentialMusicDataset(data, track, chunk_length=CHUNK_LENGTH)\n",
    "    train_indices, val_indices = train_test_split(\n",
    "        range(len(dataset)), test_size=0.2, random_state=42\n",
    "    )\n",
    "    train_dataset = torch.utils.data.Subset(dataset, train_indices)\n",
    "    val_dataset = torch.utils.data.Subset(dataset, val_indices)\n",
    "\n",
    "    # Output and logging directories\n",
    "    track_output_dir = f'./sequential_memory_checkpointing_{track}_model'\n",
    "    os.makedirs(track_output_dir, exist_ok=True)\n",
    "    wandb_run_name = f'sequential_memory_training_{track}_run'\n",
    "\n",
    "    # Initialize wandb for the current track\n",
    "    wandb.init(project='music_generation_with_memory', name=wandb_run_name)\n",
    "\n",
    "    # Training arguments\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=track_output_dir,\n",
    "        overwrite_output_dir=True,\n",
    "        num_train_epochs=120,\n",
    "        per_device_train_batch_size=1,  # Adjust based on GPU memory\n",
    "        per_device_eval_batch_size=1,\n",
    "        gradient_accumulation_steps=1,   # Adjust as needed\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        learning_rate=1e-4,\n",
    "        weight_decay=0.01,\n",
    "        logging_dir=f'./logs_{track}',\n",
    "        logging_steps=100,\n",
    "        report_to=[\"wandb\"],             # Enable logging to wandb\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=\"loss\",\n",
    "        greater_is_better=False,\n",
    "        fp16=True,                       # Enable mixed precision if supported\n",
    "    )\n",
    "\n",
    "    # Initialize the Trainer\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=val_dataset,\n",
    "        data_collator=DataCollatorWithPositionalEmbeddings(),\n",
    "        compute_metrics=compute_metrics,  # Optional: Remove if not using\n",
    "    )\n",
    "\n",
    "    # Start training\n",
    "    trainer.train()\n",
    "\n",
    "    # Evaluate the model\n",
    "    eval_results = trainer.evaluate()\n",
    "    print(f\"Evaluation results for {track}: {eval_results}\")\n",
    "\n",
    "    # Save the best model\n",
    "    trainer.save_model(track_output_dir)\n",
    "\n",
    "    # Finish the current wandb run\n",
    "    wandb.finish()\n",
    "\n",
    "print(\"Training completed for all tracks.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "60df1f7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration. Please open a PR/issue to update `preprocessor_config.json` to use `image_processor_type` instead of `feature_extractor_type`. This warning will be removed in v4.40.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing inference.wav -> hi_hat in folder 1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ripple\\AppData\\Local\\Temp\\ipykernel_18572\\3888548476.py:252: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  generated_sequence = generate_track_in_chunks(model, torch.tensor(audio_codes), positional_embeddings)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: ././sequential_memory_checkpointing_hi_hat_model/1/hi_hat_generated.wav\n",
      "Processing inference2.wav -> hi_hat in folder 2...\n",
      "Saved: ././sequential_memory_checkpointing_hi_hat_model/2/hi_hat_generated.wav\n",
      "Processing inference3.wav -> hi_hat in folder 3...\n",
      "Saved: ././sequential_memory_checkpointing_hi_hat_model/3/hi_hat_generated.wav\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torchaudio\n",
    "import numpy as np\n",
    "from transformers import AutoProcessor, EncodecModel, GPT2LMHeadModel\n",
    "import madmom\n",
    "import torch.nn as nn\n",
    "import tempfile\n",
    "\n",
    "# Constants\n",
    "VOCAB_SIZE = 1026\n",
    "MAX_LENGTH = 3000\n",
    "CHUNK_LENGTH = 75  # Default chunk size matching training\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Track classes\n",
    "track_classes = ['hi_hat']\n",
    "\n",
    "# Initialize Encodec Model and Processor\n",
    "processor = AutoProcessor.from_pretrained(\"facebook/encodec_24khz\")\n",
    "model_encodec = EncodecModel.from_pretrained(\"facebook/encodec_24khz\").to(device)\n",
    "\n",
    "# Function Definitions\n",
    "def encode_audio(audio_path):\n",
    "    audio, rate = torchaudio.load(audio_path)\n",
    "    max_length_in_samples = int(rate * 10)\n",
    "\n",
    "    if audio.shape[1] > max_length_in_samples:\n",
    "        audio = audio[:, :max_length_in_samples]\n",
    "    else:\n",
    "        pad_length = max_length_in_samples - audio.shape[1]\n",
    "        audio = torch.nn.functional.pad(audio, (0, pad_length))\n",
    "\n",
    "    if audio.shape[0] > 1:\n",
    "        audio = audio.mean(dim=0)\n",
    "    else:\n",
    "        audio = audio.squeeze(0)\n",
    "\n",
    "    inputs = processor(audio.cpu().numpy(), sampling_rate=rate, return_tensors=\"pt\")\n",
    "    inputs = {key: val.to(device) for key, val in inputs.items()}\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model_encodec.encode(inputs[\"input_values\"], inputs[\"padding_mask\"], 3)\n",
    "    \n",
    "    duration = audio.shape[0] / rate\n",
    "    return outputs.audio_codes.squeeze(), min(duration, 10.0)\n",
    "\n",
    "def extract_beats_and_downbeats(audio_path, fps=100, duration=10):\n",
    "    audio, rate = torchaudio.load(audio_path)\n",
    "    audio = audio[:, :int(duration * rate)]\n",
    "\n",
    "    with tempfile.NamedTemporaryFile(delete=False, suffix='.wav') as temp_audio_file:\n",
    "        temp_audio_path = temp_audio_file.name\n",
    "        torchaudio.save(temp_audio_path, audio, rate)\n",
    "\n",
    "    proc_downbeats = madmom.features.downbeats.DBNDownBeatTrackingProcessor(beats_per_bar=[4], fps=fps)\n",
    "    act_downbeats = madmom.features.downbeats.RNNDownBeatProcessor(fps=fps)(temp_audio_path)\n",
    "    downbeats = proc_downbeats(act_downbeats)\n",
    "\n",
    "    proc_beats = madmom.features.beats.BeatDetectionProcessor(fps=fps)\n",
    "    act_beats = madmom.features.beats.RNNBeatProcessor(fps=fps)(temp_audio_path)\n",
    "    beats = proc_beats(act_beats)\n",
    "\n",
    "    os.remove(temp_audio_path)\n",
    "\n",
    "    if len(beats) == 0 or len(downbeats) == 0:\n",
    "        raise ValueError(f\"No beats or downbeats detected in {audio_path}\")\n",
    "\n",
    "    return beats, downbeats[downbeats[:, 1] == 1, 0]\n",
    "\n",
    "def create_positional_embeddings(beat_times, downbeat_times, audio_duration, fps=75, K=32):\n",
    "    total_frames = int(np.ceil(audio_duration * fps))\n",
    "\n",
    "    def ramps(positions, size):\n",
    "        result = np.zeros(size)\n",
    "        for a, b in zip(positions[:-1], positions[1:]):\n",
    "            result[a:b] = np.linspace(0, 1, b - a, endpoint=False)\n",
    "        missing = positions[0]\n",
    "        if missing:\n",
    "            piece = result[positions[0]:positions[1]]\n",
    "            pieces = np.tile(piece, missing // len(piece) + 1)\n",
    "            result[:missing] = pieces[-missing:]\n",
    "        missing = size - positions[-1]\n",
    "        if missing:\n",
    "            piece = result[positions[-2]:positions[-1]]\n",
    "            pieces = np.tile(piece, missing // len(piece) + 1)\n",
    "            result[-missing:] = pieces[:missing]\n",
    "        return result\n",
    "\n",
    "    vector_downbeat = ramps((downbeat_times * fps).astype(int), total_frames)\n",
    "    vector_beat = ramps((beat_times * fps).astype(int), total_frames)\n",
    "\n",
    "    frequencies = np.arange(1, K + 1)\n",
    "    embeddings_downbeat = []\n",
    "    embeddings_beat = []\n",
    "\n",
    "    for k in frequencies:\n",
    "        embeddings_downbeat.append(np.sin(2 * np.pi * vector_downbeat * k))\n",
    "        embeddings_downbeat.append(np.cos(2 * np.pi * vector_downbeat * k))\n",
    "        embeddings_beat.append(np.sin(2 * np.pi * vector_beat * k))\n",
    "        embeddings_beat.append(np.cos(2 * np.pi * vector_beat * k))\n",
    "\n",
    "    embeddings_downbeat = np.stack(embeddings_downbeat, axis=1)\n",
    "    embeddings_beat = np.stack(embeddings_beat, axis=1)\n",
    "    embeddings = np.hstack((embeddings_downbeat, embeddings_beat))\n",
    "\n",
    "    return torch.from_numpy(embeddings).float()\n",
    "\n",
    "# Define the custom GPT-2 model class\n",
    "class CustomGPT2ForConditionalGeneration(GPT2LMHeadModel):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        # Projection layer to match positional embeddings to model's embedding size\n",
    "        self.positional_proj = nn.Linear(128, config.n_embd)  # Assuming positional_embeddings have dim 64\n",
    "\n",
    "    def forward(self, input_ids=None, attention_mask=None, labels=None,\n",
    "                past_key_values=None, positional_embeddings=None, **kwargs):\n",
    "        \"\"\"\n",
    "        Forward pass that incorporates positional embeddings.\n",
    "\n",
    "        Args:\n",
    "            input_ids (torch.Tensor): Input token IDs.\n",
    "            attention_mask (torch.Tensor): Attention mask.\n",
    "            labels (torch.Tensor): Labels for language modeling.\n",
    "            past_key_values (tuple): Past key and value states.\n",
    "            positional_embeddings (torch.Tensor): Positional embeddings to add to input embeddings.\n",
    "\n",
    "        Returns:\n",
    "            CausalLMOutputWithCrossAttentions: Model outputs.\n",
    "        \"\"\"\n",
    "        # Get input embeddings\n",
    "        input_embeds = self.transformer.wte(input_ids)  # [batch, seq_length, n_embd]\n",
    "\n",
    "        if positional_embeddings is not None:\n",
    "            # Project positional embeddings to match n_embd\n",
    "            pos_emb_proj = self.positional_proj(positional_embeddings)  # [batch, seq_length, n_embd]\n",
    "            input_embeds = input_embeds + pos_emb_proj  # [batch, seq_length, n_embd]\n",
    "\n",
    "        # Proceed with the standard GPT-2 forward pass using inputs_embeds\n",
    "        return super().forward(\n",
    "            inputs_embeds=input_embeds,\n",
    "            attention_mask=attention_mask,\n",
    "            labels=labels,\n",
    "            past_key_values=past_key_values,\n",
    "            **kwargs\n",
    "        )\n",
    "\n",
    "def generate_track_in_chunks_with_memory(model, audio_codes, positional_embeddings, chunk_length=CHUNK_LENGTH):\n",
    "    \"\"\"\n",
    "    Generates a track in chunks with sequential dependence using past_key_values.\n",
    "    \n",
    "    Args:\n",
    "        model: The trained model.\n",
    "        audio_codes: Tensor containing audio codes.\n",
    "        positional_embeddings: Tensor containing positional embeddings.\n",
    "        chunk_length: Length of each chunk.\n",
    "\n",
    "    Returns:\n",
    "        generated_sequence: Concatenated generated sequence from all chunks.\n",
    "    \"\"\"\n",
    "    # Ensure audio_codes is 1D\n",
    "    if len(audio_codes.shape) > 1:\n",
    "        audio_codes = audio_codes.flatten()\n",
    "\n",
    "    # Split audio_codes and positional_embeddings into chunks\n",
    "    num_chunks = int(np.ceil(audio_codes.shape[0] / chunk_length))\n",
    "    audio_chunks = np.array_split(audio_codes.cpu().numpy(), num_chunks)\n",
    "    positional_chunks = np.array_split(positional_embeddings.cpu().numpy(), num_chunks)\n",
    "\n",
    "    generated_chunks = []\n",
    "    past_key_values = None  # Initialize past_key_values for sequential dependence\n",
    "\n",
    "    for i in range(num_chunks):\n",
    "        # Pad each audio chunk to the chunk length\n",
    "        audio_chunk = torch.tensor(audio_chunks[i], dtype=torch.long).to(device)\n",
    "        audio_chunk = torch.nn.functional.pad(\n",
    "            audio_chunk,\n",
    "            (0, chunk_length - audio_chunk.shape[0]),\n",
    "            value=0\n",
    "        )\n",
    "\n",
    "        # Pad each positional embedding chunk to the chunk length\n",
    "        pos_chunk = torch.tensor(positional_chunks[i], dtype=torch.float).to(device)\n",
    "        pos_chunk = torch.nn.functional.pad(\n",
    "            pos_chunk,\n",
    "            (0, 0, 0, chunk_length - pos_chunk.shape[0]),\n",
    "            value=0\n",
    "        )\n",
    "\n",
    "        # Ensure dimensions match\n",
    "        if audio_chunk.shape[0] != pos_chunk.shape[0]:\n",
    "            raise ValueError(f\"Dimension mismatch in chunk {i}. \"\n",
    "                             f\"Audio chunk length: {audio_chunk.shape[0]}, \"\n",
    "                             f\"Positional embedding chunk length: {pos_chunk.shape[0]}.\")\n",
    "\n",
    "        # Generate attention mask\n",
    "        attention_mask = (audio_chunk != 0).long().to(device)\n",
    "\n",
    "        # Pass inputs to the model\n",
    "        with torch.no_grad():\n",
    "            outputs = model(\n",
    "                input_ids=audio_chunk.unsqueeze(0),  # [1, chunk_length]\n",
    "                attention_mask=attention_mask.unsqueeze(0),  # [1, chunk_length]\n",
    "                positional_embeddings=pos_chunk.unsqueeze(0),  # [1, chunk_length, embedding_dim]\n",
    "                past_key_values=past_key_values  # Include past_key_values for memory\n",
    "            )\n",
    "            logits = outputs.logits.argmax(dim=-1).squeeze().detach().cpu()\n",
    "            past_key_values = outputs.past_key_values  # Update past_key_values for the next chunk\n",
    "            generated_chunks.append(logits)\n",
    "\n",
    "    # Concatenate all generated chunks\n",
    "    generated_sequence = torch.cat(generated_chunks, dim=0)\n",
    "    return generated_sequence\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Inference\n",
    "inference_files = [\n",
    "    (\"inference.wav\", \"inference_posemb.wav\", \"1\"),\n",
    "    (\"inference2.wav\", \"inference_posemb2.wav\", \"2\"),\n",
    "    (\"inference3.wav\", \"inference_posemb3.wav\", \"3\"),\n",
    "]\n",
    "\n",
    "for audio_path, posemb_path, folder in inference_files:\n",
    "    for track_class in track_classes:\n",
    "        print(f\"Processing {audio_path} -> {track_class} in folder {folder}...\")\n",
    "        model_folder = f'./sequential_memory_checkpointing_{track_class}_model'\n",
    "        checkpoint_path = os.path.join(model_folder, 'checkpoint-44032')  # Specify the checkpoint\n",
    "\n",
    "        if not os.path.exists(checkpoint_path):\n",
    "            raise FileNotFoundError(f\"Checkpoint {checkpoint_path} not found.\")\n",
    "\n",
    "        # Load the model from the specified checkpoint\n",
    "        model = CustomGPT2ForConditionalGeneration.from_pretrained(checkpoint_path).to(device)\n",
    "        model.eval()\n",
    "\n",
    "        # Encode the audio\n",
    "        audio_codes, audio_length = encode_audio(audio_path)\n",
    "\n",
    "        # Extract beats and downbeats\n",
    "        try:\n",
    "            beats, downbeats = extract_beats_and_downbeats(posemb_path, duration=audio_length)\n",
    "        except ValueError as e:\n",
    "            print(e)\n",
    "            continue  # Skip this audio file if beats/downbeats are not detected\n",
    "\n",
    "        # Create positional embeddings\n",
    "        positional_embeddings = create_positional_embeddings(beats, downbeats, audio_length)\n",
    "\n",
    "        # Generate the sequence in chunks\n",
    "        generated_sequence = generate_track_in_chunks(model, torch.tensor(audio_codes), positional_embeddings)\n",
    "\n",
    "        # Reshape and decode the generated sequence\n",
    "        reshaped_output = generated_sequence.view(4, 750).unsqueeze(0).unsqueeze(0).to(device)  # Move to device\n",
    "        decoded_audio = model_encodec.decode(reshaped_output, [None])[0]\n",
    "        decoded_audio = decoded_audio.detach()\n",
    "        decoded_audio = decoded_audio.squeeze(0).squeeze(0)  # Shape: [samples]\n",
    "        decoded_audio = decoded_audio.unsqueeze(0)\n",
    "        output_audio_path = f\"./{model_folder}/{folder}/{track_class}_generated.wav\"\n",
    "        os.makedirs(os.path.dirname(output_audio_path), exist_ok=True)\n",
    "        torchaudio.save(output_audio_path, decoded_audio.cpu(), processor.sampling_rate)\n",
    "        print(f\"Saved: {output_audio_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "280ff567",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
