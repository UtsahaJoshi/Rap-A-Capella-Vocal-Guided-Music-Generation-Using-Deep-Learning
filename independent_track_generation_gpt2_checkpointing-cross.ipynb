{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "477a9532",
   "metadata": {},
   "source": [
    "# Cross-Attention GPT-2 Model for Independent Music Track Generation\n",
    "\n",
    "This script trains a GPT-2-based model enhanced with cross-attention mechanisms for independent music track generation. The model processes audio data encoded as 10-second segments, leveraging positional embeddings and track-specific conditioning inputs for improved contextual learning. A custom dataset framework is used to integrate positional and conditioning inputs into the training pipeline, ensuring track classes such as hi_hat, kick, snare, and bass are modeled independently.\n",
    "\n",
    "The model introduces a cross-attention layer that allows it to focus on specific track data during sequence generation, enhancing its ability to generate accurate and context-aware outputs. Training is performed sequentially for each track class using Hugging Face's Trainer with mixed precision and epoch-based checkpointing. The script uses Weights & Biases (WandB) for tracking and logging, saving the best models for each track class for future inference tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "70c60bdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for hi_hat...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33muniqlabs\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>C:\\Users\\Ripple\\Desktop\\5th sem\\practical work\\code\\wandb\\run-20241128_191217-ptsf9xn2</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/uniqlabs/music_generation/runs/ptsf9xn2' target=\"_blank\">hi_hat_training_run_gpt2_cross_10secsdataset_final</a></strong> to <a href='https://wandb.ai/uniqlabs/music_generation' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/uniqlabs/music_generation' target=\"_blank\">https://wandb.ai/uniqlabs/music_generation</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/uniqlabs/music_generation/runs/ptsf9xn2' target=\"_blank\">https://wandb.ai/uniqlabs/music_generation/runs/ptsf9xn2</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ripple\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\accelerate\\accelerator.py:436: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='12840' max='12840' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [12840/12840 46:39, Epoch 120/120]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>5.008100</td>\n",
       "      <td>4.776132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2.629500</td>\n",
       "      <td>2.958881</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>3.110000</td>\n",
       "      <td>2.730720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>3.142400</td>\n",
       "      <td>2.666206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>2.708500</td>\n",
       "      <td>2.617558</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>3.090700</td>\n",
       "      <td>2.591557</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>2.673600</td>\n",
       "      <td>2.584707</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>2.575900</td>\n",
       "      <td>2.549710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>2.381700</td>\n",
       "      <td>2.540797</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>2.319500</td>\n",
       "      <td>2.525972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>2.047700</td>\n",
       "      <td>2.516229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>3.197300</td>\n",
       "      <td>2.514301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>2.881000</td>\n",
       "      <td>2.522957</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>2.556900</td>\n",
       "      <td>2.506491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>2.266700</td>\n",
       "      <td>2.488896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>2.725600</td>\n",
       "      <td>2.480652</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>2.184000</td>\n",
       "      <td>2.477894</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>2.159100</td>\n",
       "      <td>2.471114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>2.222500</td>\n",
       "      <td>2.467499</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>2.907900</td>\n",
       "      <td>2.473921</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>2.179900</td>\n",
       "      <td>2.448214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>2.214400</td>\n",
       "      <td>2.451962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>2.218000</td>\n",
       "      <td>2.451831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>2.319900</td>\n",
       "      <td>2.431619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>2.277100</td>\n",
       "      <td>2.426973</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>2.118900</td>\n",
       "      <td>2.432178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>2.655100</td>\n",
       "      <td>2.417574</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>2.779700</td>\n",
       "      <td>2.416650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>2.392700</td>\n",
       "      <td>2.409067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>1.958100</td>\n",
       "      <td>2.409715</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31</td>\n",
       "      <td>2.929000</td>\n",
       "      <td>2.406677</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>2.132600</td>\n",
       "      <td>2.379462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33</td>\n",
       "      <td>2.559700</td>\n",
       "      <td>2.379824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34</td>\n",
       "      <td>2.597800</td>\n",
       "      <td>2.374617</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>1.964500</td>\n",
       "      <td>2.369001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36</td>\n",
       "      <td>2.273700</td>\n",
       "      <td>2.375031</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37</td>\n",
       "      <td>1.912700</td>\n",
       "      <td>2.369480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38</td>\n",
       "      <td>2.099400</td>\n",
       "      <td>2.365590</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39</td>\n",
       "      <td>2.108800</td>\n",
       "      <td>2.355146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>2.452600</td>\n",
       "      <td>2.366122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41</td>\n",
       "      <td>1.803700</td>\n",
       "      <td>2.347689</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42</td>\n",
       "      <td>1.727300</td>\n",
       "      <td>2.349032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43</td>\n",
       "      <td>1.905900</td>\n",
       "      <td>2.346961</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44</td>\n",
       "      <td>2.078500</td>\n",
       "      <td>2.350829</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45</td>\n",
       "      <td>2.232200</td>\n",
       "      <td>2.337359</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46</td>\n",
       "      <td>1.876200</td>\n",
       "      <td>2.355460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47</td>\n",
       "      <td>1.682400</td>\n",
       "      <td>2.324316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48</td>\n",
       "      <td>2.305800</td>\n",
       "      <td>2.328992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49</td>\n",
       "      <td>2.153100</td>\n",
       "      <td>2.333126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>1.660100</td>\n",
       "      <td>2.328711</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>51</td>\n",
       "      <td>2.088800</td>\n",
       "      <td>2.328161</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>52</td>\n",
       "      <td>2.457700</td>\n",
       "      <td>2.325438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>53</td>\n",
       "      <td>1.842200</td>\n",
       "      <td>2.326191</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>54</td>\n",
       "      <td>2.883500</td>\n",
       "      <td>2.327177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>55</td>\n",
       "      <td>2.367000</td>\n",
       "      <td>2.324181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>56</td>\n",
       "      <td>2.392300</td>\n",
       "      <td>2.320734</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>57</td>\n",
       "      <td>2.269500</td>\n",
       "      <td>2.324378</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>58</td>\n",
       "      <td>2.339500</td>\n",
       "      <td>2.303445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>59</td>\n",
       "      <td>1.530900</td>\n",
       "      <td>2.320111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>2.010000</td>\n",
       "      <td>2.302083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>61</td>\n",
       "      <td>2.130900</td>\n",
       "      <td>2.302784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>62</td>\n",
       "      <td>1.899800</td>\n",
       "      <td>2.314134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>63</td>\n",
       "      <td>1.850300</td>\n",
       "      <td>2.302387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>64</td>\n",
       "      <td>2.212600</td>\n",
       "      <td>2.318471</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>65</td>\n",
       "      <td>2.183800</td>\n",
       "      <td>2.304621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>66</td>\n",
       "      <td>2.018300</td>\n",
       "      <td>2.304244</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>67</td>\n",
       "      <td>1.751600</td>\n",
       "      <td>2.313678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>68</td>\n",
       "      <td>1.884800</td>\n",
       "      <td>2.284638</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>69</td>\n",
       "      <td>1.482000</td>\n",
       "      <td>2.306027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>1.780200</td>\n",
       "      <td>2.302052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>71</td>\n",
       "      <td>2.175000</td>\n",
       "      <td>2.301246</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>72</td>\n",
       "      <td>2.372000</td>\n",
       "      <td>2.295337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>73</td>\n",
       "      <td>1.630700</td>\n",
       "      <td>2.298225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>74</td>\n",
       "      <td>1.557000</td>\n",
       "      <td>2.300177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75</td>\n",
       "      <td>1.994200</td>\n",
       "      <td>2.302774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>76</td>\n",
       "      <td>1.968500</td>\n",
       "      <td>2.293515</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>77</td>\n",
       "      <td>1.937800</td>\n",
       "      <td>2.300501</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>78</td>\n",
       "      <td>1.870400</td>\n",
       "      <td>2.293296</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>79</td>\n",
       "      <td>2.274500</td>\n",
       "      <td>2.301934</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>2.121800</td>\n",
       "      <td>2.300395</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>81</td>\n",
       "      <td>2.306600</td>\n",
       "      <td>2.304577</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>82</td>\n",
       "      <td>1.507300</td>\n",
       "      <td>2.305795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>83</td>\n",
       "      <td>2.129400</td>\n",
       "      <td>2.296486</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>84</td>\n",
       "      <td>1.965800</td>\n",
       "      <td>2.304059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>85</td>\n",
       "      <td>2.010800</td>\n",
       "      <td>2.297685</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>86</td>\n",
       "      <td>2.325200</td>\n",
       "      <td>2.303324</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>87</td>\n",
       "      <td>1.994000</td>\n",
       "      <td>2.301695</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>88</td>\n",
       "      <td>1.768300</td>\n",
       "      <td>2.302369</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>89</td>\n",
       "      <td>2.174200</td>\n",
       "      <td>2.312665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>1.078800</td>\n",
       "      <td>2.302347</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>91</td>\n",
       "      <td>2.065600</td>\n",
       "      <td>2.312213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>92</td>\n",
       "      <td>1.573300</td>\n",
       "      <td>2.317359</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>93</td>\n",
       "      <td>2.454800</td>\n",
       "      <td>2.303980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>94</td>\n",
       "      <td>1.400100</td>\n",
       "      <td>2.318462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>95</td>\n",
       "      <td>1.871800</td>\n",
       "      <td>2.311604</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>96</td>\n",
       "      <td>1.903400</td>\n",
       "      <td>2.304373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>97</td>\n",
       "      <td>1.985700</td>\n",
       "      <td>2.304627</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>98</td>\n",
       "      <td>1.851800</td>\n",
       "      <td>2.305866</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>99</td>\n",
       "      <td>1.621600</td>\n",
       "      <td>2.307858</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>1.843400</td>\n",
       "      <td>2.311111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>101</td>\n",
       "      <td>1.631900</td>\n",
       "      <td>2.307536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>102</td>\n",
       "      <td>2.420300</td>\n",
       "      <td>2.311582</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>103</td>\n",
       "      <td>1.779800</td>\n",
       "      <td>2.307956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>104</td>\n",
       "      <td>1.474800</td>\n",
       "      <td>2.315241</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>105</td>\n",
       "      <td>1.752300</td>\n",
       "      <td>2.311826</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>106</td>\n",
       "      <td>2.115300</td>\n",
       "      <td>2.314242</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>107</td>\n",
       "      <td>2.131000</td>\n",
       "      <td>2.307153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>108</td>\n",
       "      <td>1.434200</td>\n",
       "      <td>2.308588</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>109</td>\n",
       "      <td>2.339700</td>\n",
       "      <td>2.310870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>2.079300</td>\n",
       "      <td>2.319165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>111</td>\n",
       "      <td>1.811800</td>\n",
       "      <td>2.316120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>112</td>\n",
       "      <td>1.851900</td>\n",
       "      <td>2.318365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>113</td>\n",
       "      <td>2.068000</td>\n",
       "      <td>2.315367</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>114</td>\n",
       "      <td>1.696700</td>\n",
       "      <td>2.313151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>115</td>\n",
       "      <td>1.705800</td>\n",
       "      <td>2.316541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>116</td>\n",
       "      <td>1.793200</td>\n",
       "      <td>2.313802</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>117</td>\n",
       "      <td>1.820900</td>\n",
       "      <td>2.318325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>118</td>\n",
       "      <td>1.544200</td>\n",
       "      <td>2.316938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>119</td>\n",
       "      <td>1.553600</td>\n",
       "      <td>2.316941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>1.885200</td>\n",
       "      <td>2.317782</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <style>\n",
       "        .wandb-row {\n",
       "            display: flex;\n",
       "            flex-direction: row;\n",
       "            flex-wrap: wrap;\n",
       "            justify-content: flex-start;\n",
       "            width: 100%;\n",
       "        }\n",
       "        .wandb-col {\n",
       "            display: flex;\n",
       "            flex-direction: column;\n",
       "            flex-basis: 100%;\n",
       "            flex: 1;\n",
       "            padding: 10px;\n",
       "        }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>█▇▇▆▅▄▄▃▃▃▃▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>eval/runtime</td><td>▁▃▃▄▅▅▅▅▅▅▅▆▅▆▆▆▅▆▆▆▅▅▆▅▅▅▅▅▅▅▅▇█▇▆▆▆▆▆▆</td></tr><tr><td>eval/samples_per_second</td><td>█▆▅▅▄▄▄▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▂▁▃▃▃▃▃▃▃</td></tr><tr><td>eval/steps_per_second</td><td>█▇▅▅▄▄▄▄▄▄▃▃▃▃▃▄▃▃▃▃▃▄▃▃▃▄▃▃▄▄▂▁▃▃▃▃▃▄▃▃</td></tr><tr><td>train/epoch</td><td>▁▁▁▂▂▂▂▂▂▂▂▂▂▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇▇███</td></tr><tr><td>train/global_step</td><td>▁▁▁▁▂▂▂▂▂▂▃▃▄▄▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▇▇▇▇▇█████</td></tr><tr><td>train/grad_norm</td><td>▃▁▄▃█▂▃▂▂▄▅▁▂▂▁▆▃▃▁▆▁▅▁▃▂▄▂▆▃▃▃▁▂▁▂▄▃▅▅▁</td></tr><tr><td>train/learning_rate</td><td>███▇▇▇▇▇▇▇▇▆▆▆▆▆▆▆▆▆▅▅▅▅▅▄▄▄▄▄▃▃▃▃▃▂▂▂▁▁</td></tr><tr><td>train/loss</td><td>██▄▄▄▄▄▃▄▆▆▃▃▇▃▃▃▃▃▃▆▄▁▃▂▃▃▄▁▂▄▃▁▄▄▂▃▃▂▃</td></tr><tr><td>train/total_flos</td><td>▁</td></tr><tr><td>train/train_loss</td><td>▁</td></tr><tr><td>train/train_runtime</td><td>▁</td></tr><tr><td>train/train_samples_per_second</td><td>▁</td></tr><tr><td>train/train_steps_per_second</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>2.31778</td></tr><tr><td>eval/runtime</td><td>1.7463</td></tr><tr><td>eval/samples_per_second</td><td>15.461</td></tr><tr><td>eval/steps_per_second</td><td>15.461</td></tr><tr><td>train/epoch</td><td>120</td></tr><tr><td>train/global_step</td><td>12840</td></tr><tr><td>train/grad_norm</td><td>1.04571</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>1.8852</td></tr><tr><td>train/total_flos</td><td>294088181760000.0</td></tr><tr><td>train/train_loss</td><td>2.12982</td></tr><tr><td>train/train_runtime</td><td>2803.3706</td></tr><tr><td>train/train_samples_per_second</td><td>4.58</td></tr><tr><td>train/train_steps_per_second</td><td>4.58</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">hi_hat_training_run_gpt2_cross_10secsdataset_final</strong> at: <a href='https://wandb.ai/uniqlabs/music_generation/runs/ptsf9xn2' target=\"_blank\">https://wandb.ai/uniqlabs/music_generation/runs/ptsf9xn2</a><br/> View project at: <a href='https://wandb.ai/uniqlabs/music_generation' target=\"_blank\">https://wandb.ai/uniqlabs/music_generation</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20241128_191217-ptsf9xn2\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for kick...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>C:\\Users\\Ripple\\Desktop\\5th sem\\practical work\\code\\wandb\\run-20241128_195906-qphvasim</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/uniqlabs/music_generation/runs/qphvasim' target=\"_blank\">kick_training_run_gpt2_cross_10secsdataset_final</a></strong> to <a href='https://wandb.ai/uniqlabs/music_generation' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/uniqlabs/music_generation' target=\"_blank\">https://wandb.ai/uniqlabs/music_generation</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/uniqlabs/music_generation/runs/qphvasim' target=\"_blank\">https://wandb.ai/uniqlabs/music_generation/runs/qphvasim</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ripple\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\accelerate\\accelerator.py:436: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='12840' max='12840' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [12840/12840 47:05, Epoch 120/120]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.344200</td>\n",
       "      <td>2.178631</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.609500</td>\n",
       "      <td>2.054694</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>2.211200</td>\n",
       "      <td>2.004590</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>2.109400</td>\n",
       "      <td>1.960391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>2.391100</td>\n",
       "      <td>1.925263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>2.209000</td>\n",
       "      <td>1.915276</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>2.015000</td>\n",
       "      <td>1.902902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>1.654700</td>\n",
       "      <td>1.887194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>1.423700</td>\n",
       "      <td>1.898422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>2.071300</td>\n",
       "      <td>1.884959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>1.320800</td>\n",
       "      <td>1.894585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>1.310900</td>\n",
       "      <td>1.883229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>1.855500</td>\n",
       "      <td>1.872563</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>1.914900</td>\n",
       "      <td>1.877306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>1.703100</td>\n",
       "      <td>1.868335</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>2.163500</td>\n",
       "      <td>1.860185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>1.404600</td>\n",
       "      <td>1.875003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>1.568900</td>\n",
       "      <td>1.861438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>1.961600</td>\n",
       "      <td>1.861204</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>1.597800</td>\n",
       "      <td>1.857375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>1.752900</td>\n",
       "      <td>1.852699</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>1.975700</td>\n",
       "      <td>1.851222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>1.737200</td>\n",
       "      <td>1.848920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>1.656400</td>\n",
       "      <td>1.863315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>1.944000</td>\n",
       "      <td>1.852251</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>1.695800</td>\n",
       "      <td>1.839874</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>1.445300</td>\n",
       "      <td>1.853044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>1.877500</td>\n",
       "      <td>1.870029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>1.541500</td>\n",
       "      <td>1.861032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>1.339900</td>\n",
       "      <td>1.849669</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31</td>\n",
       "      <td>1.903800</td>\n",
       "      <td>1.836193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>1.348900</td>\n",
       "      <td>1.860929</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33</td>\n",
       "      <td>1.469100</td>\n",
       "      <td>1.839264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34</td>\n",
       "      <td>1.488300</td>\n",
       "      <td>1.848655</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>1.674800</td>\n",
       "      <td>1.842354</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36</td>\n",
       "      <td>1.632900</td>\n",
       "      <td>1.853840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37</td>\n",
       "      <td>1.736400</td>\n",
       "      <td>1.851516</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38</td>\n",
       "      <td>1.231900</td>\n",
       "      <td>1.878496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39</td>\n",
       "      <td>1.795300</td>\n",
       "      <td>1.851009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>1.837600</td>\n",
       "      <td>1.847677</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41</td>\n",
       "      <td>1.182300</td>\n",
       "      <td>1.851785</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42</td>\n",
       "      <td>1.240500</td>\n",
       "      <td>1.852381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43</td>\n",
       "      <td>1.596700</td>\n",
       "      <td>1.859715</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44</td>\n",
       "      <td>1.151700</td>\n",
       "      <td>1.859959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45</td>\n",
       "      <td>1.695500</td>\n",
       "      <td>1.854738</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46</td>\n",
       "      <td>1.391400</td>\n",
       "      <td>1.860960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47</td>\n",
       "      <td>1.408000</td>\n",
       "      <td>1.861850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48</td>\n",
       "      <td>1.650400</td>\n",
       "      <td>1.872684</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49</td>\n",
       "      <td>1.412100</td>\n",
       "      <td>1.876445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>1.543200</td>\n",
       "      <td>1.863184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>51</td>\n",
       "      <td>1.144900</td>\n",
       "      <td>1.885328</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>52</td>\n",
       "      <td>1.448200</td>\n",
       "      <td>1.866569</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>53</td>\n",
       "      <td>1.360300</td>\n",
       "      <td>1.869408</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>54</td>\n",
       "      <td>1.438600</td>\n",
       "      <td>1.882044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>55</td>\n",
       "      <td>1.426800</td>\n",
       "      <td>1.875474</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>56</td>\n",
       "      <td>1.186300</td>\n",
       "      <td>1.880862</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>57</td>\n",
       "      <td>1.735400</td>\n",
       "      <td>1.880620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>58</td>\n",
       "      <td>1.276600</td>\n",
       "      <td>1.886768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>59</td>\n",
       "      <td>1.439500</td>\n",
       "      <td>1.878380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.912400</td>\n",
       "      <td>1.885976</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>61</td>\n",
       "      <td>1.767700</td>\n",
       "      <td>1.891844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>62</td>\n",
       "      <td>1.683600</td>\n",
       "      <td>1.871275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>63</td>\n",
       "      <td>1.458100</td>\n",
       "      <td>1.869679</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>64</td>\n",
       "      <td>1.339600</td>\n",
       "      <td>1.878825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>65</td>\n",
       "      <td>1.590200</td>\n",
       "      <td>1.872564</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>66</td>\n",
       "      <td>1.161800</td>\n",
       "      <td>1.891719</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>67</td>\n",
       "      <td>1.510200</td>\n",
       "      <td>1.880749</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>68</td>\n",
       "      <td>1.209900</td>\n",
       "      <td>1.910661</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>69</td>\n",
       "      <td>1.796700</td>\n",
       "      <td>1.905186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>1.095100</td>\n",
       "      <td>1.895578</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>71</td>\n",
       "      <td>1.600100</td>\n",
       "      <td>1.911880</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>72</td>\n",
       "      <td>1.459600</td>\n",
       "      <td>1.895202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>73</td>\n",
       "      <td>1.477400</td>\n",
       "      <td>1.907837</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>74</td>\n",
       "      <td>1.153600</td>\n",
       "      <td>1.904246</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75</td>\n",
       "      <td>1.579000</td>\n",
       "      <td>1.907934</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>76</td>\n",
       "      <td>1.544500</td>\n",
       "      <td>1.899258</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>77</td>\n",
       "      <td>1.233800</td>\n",
       "      <td>1.891547</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>78</td>\n",
       "      <td>0.912800</td>\n",
       "      <td>1.908875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>79</td>\n",
       "      <td>1.092100</td>\n",
       "      <td>1.920469</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>1.330200</td>\n",
       "      <td>1.907065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>81</td>\n",
       "      <td>1.627100</td>\n",
       "      <td>1.903430</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>82</td>\n",
       "      <td>1.725100</td>\n",
       "      <td>1.920119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>83</td>\n",
       "      <td>1.489800</td>\n",
       "      <td>1.914785</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>84</td>\n",
       "      <td>1.377400</td>\n",
       "      <td>1.907853</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>85</td>\n",
       "      <td>1.268400</td>\n",
       "      <td>1.927425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>86</td>\n",
       "      <td>1.588000</td>\n",
       "      <td>1.909232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>87</td>\n",
       "      <td>1.073500</td>\n",
       "      <td>1.915920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>88</td>\n",
       "      <td>1.546500</td>\n",
       "      <td>1.931336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>89</td>\n",
       "      <td>1.065300</td>\n",
       "      <td>1.928720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>1.404000</td>\n",
       "      <td>1.928746</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>91</td>\n",
       "      <td>1.167200</td>\n",
       "      <td>1.913912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>92</td>\n",
       "      <td>1.233200</td>\n",
       "      <td>1.927623</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>93</td>\n",
       "      <td>1.552500</td>\n",
       "      <td>1.921739</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>94</td>\n",
       "      <td>1.344200</td>\n",
       "      <td>1.921339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>95</td>\n",
       "      <td>1.416900</td>\n",
       "      <td>1.933025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>96</td>\n",
       "      <td>1.444700</td>\n",
       "      <td>1.928394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>97</td>\n",
       "      <td>1.017000</td>\n",
       "      <td>1.923106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>98</td>\n",
       "      <td>1.108500</td>\n",
       "      <td>1.935632</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>99</td>\n",
       "      <td>1.483300</td>\n",
       "      <td>1.940906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>1.285200</td>\n",
       "      <td>1.938145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>101</td>\n",
       "      <td>1.167100</td>\n",
       "      <td>1.933649</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>102</td>\n",
       "      <td>1.421400</td>\n",
       "      <td>1.934700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>103</td>\n",
       "      <td>1.276000</td>\n",
       "      <td>1.941869</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>104</td>\n",
       "      <td>1.234000</td>\n",
       "      <td>1.938741</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>105</td>\n",
       "      <td>1.255100</td>\n",
       "      <td>1.940163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>106</td>\n",
       "      <td>1.396300</td>\n",
       "      <td>1.938142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>107</td>\n",
       "      <td>1.316400</td>\n",
       "      <td>1.935812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>108</td>\n",
       "      <td>1.390700</td>\n",
       "      <td>1.942393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>109</td>\n",
       "      <td>1.619900</td>\n",
       "      <td>1.939866</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>1.350400</td>\n",
       "      <td>1.940990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>111</td>\n",
       "      <td>1.410600</td>\n",
       "      <td>1.943559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>112</td>\n",
       "      <td>1.209200</td>\n",
       "      <td>1.946490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>113</td>\n",
       "      <td>1.248900</td>\n",
       "      <td>1.946888</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>114</td>\n",
       "      <td>0.916200</td>\n",
       "      <td>1.947582</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>115</td>\n",
       "      <td>1.165700</td>\n",
       "      <td>1.943881</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>116</td>\n",
       "      <td>1.196800</td>\n",
       "      <td>1.947473</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>117</td>\n",
       "      <td>1.385700</td>\n",
       "      <td>1.954121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>118</td>\n",
       "      <td>0.857400</td>\n",
       "      <td>1.948211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>119</td>\n",
       "      <td>1.437200</td>\n",
       "      <td>1.950651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>1.355800</td>\n",
       "      <td>1.951264</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <style>\n",
       "        .wandb-row {\n",
       "            display: flex;\n",
       "            flex-direction: row;\n",
       "            flex-wrap: wrap;\n",
       "            justify-content: flex-start;\n",
       "            width: 100%;\n",
       "        }\n",
       "        .wandb-col {\n",
       "            display: flex;\n",
       "            flex-direction: column;\n",
       "            flex-basis: 100%;\n",
       "            flex: 1;\n",
       "            padding: 10px;\n",
       "        }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>█▆▅▄▃▂▂▁▂▂▁▂▁▂▂▂▂▂▂▂▂▃▃▃▃▃▃▃▃▄▃▄▄▄▄▄▄▄▄▅</td></tr><tr><td>eval/runtime</td><td>▂▁▁▄▂▃▃█▁▂▂▃▂▂▂▂▅▃▇▂▅▄▃▃▃▃▃▃▅▄▄▃▃▃▄▃▃▄▆▂</td></tr><tr><td>eval/samples_per_second</td><td>██▇██▆▄▆▆▆▇▅▇▇▆▇▇▇▇▇▅▅▁▃▆▆▅▅▆▆▄▅▄▆▅▅▆▄▂▇</td></tr><tr><td>eval/steps_per_second</td><td>▇█▇▅▇▇▁▆▇▄▇▆▇▇▇▅▆▇▆▇▆▆▅▆▇▆▆▄▇▆▆▆▆▆▆▆▆▆▅▆</td></tr><tr><td>train/epoch</td><td>▁▁▁▁▂▂▂▂▂▂▂▂▂▂▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▅▆▆▆▆▆▆▆▇▇█</td></tr><tr><td>train/global_step</td><td>▁▁▂▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▆▆▆▆▆▆▆▇▇▇▇▇▇▇███</td></tr><tr><td>train/grad_norm</td><td>▄▂▂▁▂▁▂▁▁▂▂▁▂▁▁▂█▂▂▂▅▂▂▃▃▄▃▂▂▂▃▂▄▂▂▂▂▂▃▂</td></tr><tr><td>train/learning_rate</td><td>█████▇▇▇▇▇▆▆▆▆▅▅▅▅▄▄▄▃▃▃▃▃▃▃▃▃▃▂▂▂▂▂▂▁▁▁</td></tr><tr><td>train/loss</td><td>▃▇▃▆▅▃▇▃▃▅▇█▅▆▃▂▄▅▇▆▆▅▄▄▅▄▃▃▁▃▂▂▄▂▂▂▁▂▄▂</td></tr><tr><td>train/total_flos</td><td>▁</td></tr><tr><td>train/train_loss</td><td>▁</td></tr><tr><td>train/train_runtime</td><td>▁</td></tr><tr><td>train/train_samples_per_second</td><td>▁</td></tr><tr><td>train/train_steps_per_second</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>1.95126</td></tr><tr><td>eval/runtime</td><td>1.7469</td></tr><tr><td>eval/samples_per_second</td><td>15.456</td></tr><tr><td>eval/steps_per_second</td><td>15.456</td></tr><tr><td>train/epoch</td><td>120</td></tr><tr><td>train/global_step</td><td>12840</td></tr><tr><td>train/grad_norm</td><td>0.59535</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>1.3558</td></tr><tr><td>train/total_flos</td><td>294088181760000.0</td></tr><tr><td>train/train_loss</td><td>1.5298</td></tr><tr><td>train/train_runtime</td><td>2825.7904</td></tr><tr><td>train/train_samples_per_second</td><td>4.544</td></tr><tr><td>train/train_steps_per_second</td><td>4.544</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">kick_training_run_gpt2_cross_10secsdataset_final</strong> at: <a href='https://wandb.ai/uniqlabs/music_generation/runs/qphvasim' target=\"_blank\">https://wandb.ai/uniqlabs/music_generation/runs/qphvasim</a><br/> View project at: <a href='https://wandb.ai/uniqlabs/music_generation' target=\"_blank\">https://wandb.ai/uniqlabs/music_generation</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20241128_195906-qphvasim\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for snare...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>C:\\Users\\Ripple\\Desktop\\5th sem\\practical work\\code\\wandb\\run-20241128_204615-gu3tmbuo</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/uniqlabs/music_generation/runs/gu3tmbuo' target=\"_blank\">snare_training_run_gpt2_cross_10secsdataset_final</a></strong> to <a href='https://wandb.ai/uniqlabs/music_generation' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/uniqlabs/music_generation' target=\"_blank\">https://wandb.ai/uniqlabs/music_generation</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/uniqlabs/music_generation/runs/gu3tmbuo' target=\"_blank\">https://wandb.ai/uniqlabs/music_generation/runs/gu3tmbuo</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ripple\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\accelerate\\accelerator.py:436: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='7800' max='7800' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [7800/7800 28:38, Epoch 120/120]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.637800</td>\n",
       "      <td>2.131308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2.753200</td>\n",
       "      <td>2.022117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>2.082900</td>\n",
       "      <td>1.990611</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>2.191700</td>\n",
       "      <td>1.972749</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>2.633600</td>\n",
       "      <td>1.947277</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>1.883100</td>\n",
       "      <td>1.933614</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>1.732900</td>\n",
       "      <td>1.906582</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>2.103400</td>\n",
       "      <td>1.906523</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>2.253500</td>\n",
       "      <td>1.897117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>1.822300</td>\n",
       "      <td>1.881855</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>1.969200</td>\n",
       "      <td>1.853306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>1.855300</td>\n",
       "      <td>1.865491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>1.988900</td>\n",
       "      <td>1.846015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>1.632800</td>\n",
       "      <td>1.847732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>2.119100</td>\n",
       "      <td>1.823871</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>2.475800</td>\n",
       "      <td>1.840935</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>1.782600</td>\n",
       "      <td>1.833583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>2.074800</td>\n",
       "      <td>1.835170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>1.778000</td>\n",
       "      <td>1.830109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>1.915700</td>\n",
       "      <td>1.829939</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>1.940600</td>\n",
       "      <td>1.819469</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>1.698600</td>\n",
       "      <td>1.830367</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>1.860000</td>\n",
       "      <td>1.816594</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>1.668000</td>\n",
       "      <td>1.817662</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>1.649300</td>\n",
       "      <td>1.827124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>1.660300</td>\n",
       "      <td>1.807826</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>1.782200</td>\n",
       "      <td>1.822526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>1.670300</td>\n",
       "      <td>1.831260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>1.376200</td>\n",
       "      <td>1.816539</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>1.490900</td>\n",
       "      <td>1.817575</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31</td>\n",
       "      <td>1.492000</td>\n",
       "      <td>1.817749</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>1.486300</td>\n",
       "      <td>1.832705</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33</td>\n",
       "      <td>2.031800</td>\n",
       "      <td>1.840862</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34</td>\n",
       "      <td>1.614500</td>\n",
       "      <td>1.840312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>1.579300</td>\n",
       "      <td>1.828366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36</td>\n",
       "      <td>1.803600</td>\n",
       "      <td>1.834025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37</td>\n",
       "      <td>1.187200</td>\n",
       "      <td>1.826147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38</td>\n",
       "      <td>1.425600</td>\n",
       "      <td>1.883292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39</td>\n",
       "      <td>1.529100</td>\n",
       "      <td>1.844870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>1.557900</td>\n",
       "      <td>1.845947</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41</td>\n",
       "      <td>1.613900</td>\n",
       "      <td>1.847040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42</td>\n",
       "      <td>1.107100</td>\n",
       "      <td>1.851807</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43</td>\n",
       "      <td>1.870700</td>\n",
       "      <td>1.851767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44</td>\n",
       "      <td>1.701700</td>\n",
       "      <td>1.862899</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45</td>\n",
       "      <td>1.917700</td>\n",
       "      <td>1.865452</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46</td>\n",
       "      <td>1.505400</td>\n",
       "      <td>1.857165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47</td>\n",
       "      <td>1.276700</td>\n",
       "      <td>1.888633</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48</td>\n",
       "      <td>1.471100</td>\n",
       "      <td>1.884892</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49</td>\n",
       "      <td>1.805800</td>\n",
       "      <td>1.873261</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>1.580200</td>\n",
       "      <td>1.859123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>51</td>\n",
       "      <td>1.512300</td>\n",
       "      <td>1.897565</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>52</td>\n",
       "      <td>1.378500</td>\n",
       "      <td>1.861086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>53</td>\n",
       "      <td>1.058700</td>\n",
       "      <td>1.869403</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>54</td>\n",
       "      <td>1.529200</td>\n",
       "      <td>1.899982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>55</td>\n",
       "      <td>1.306500</td>\n",
       "      <td>1.873839</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>56</td>\n",
       "      <td>1.151100</td>\n",
       "      <td>1.912531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>57</td>\n",
       "      <td>1.618700</td>\n",
       "      <td>1.896591</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>58</td>\n",
       "      <td>1.157000</td>\n",
       "      <td>1.901273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>59</td>\n",
       "      <td>1.437900</td>\n",
       "      <td>1.898644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>1.601500</td>\n",
       "      <td>1.920012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>61</td>\n",
       "      <td>1.209300</td>\n",
       "      <td>1.913213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>62</td>\n",
       "      <td>1.340000</td>\n",
       "      <td>1.919108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>63</td>\n",
       "      <td>1.300400</td>\n",
       "      <td>1.919822</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>64</td>\n",
       "      <td>1.343200</td>\n",
       "      <td>1.931336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>65</td>\n",
       "      <td>1.304300</td>\n",
       "      <td>1.895778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>66</td>\n",
       "      <td>0.940200</td>\n",
       "      <td>1.920764</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>67</td>\n",
       "      <td>1.016000</td>\n",
       "      <td>1.930340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>68</td>\n",
       "      <td>1.304400</td>\n",
       "      <td>1.932395</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>69</td>\n",
       "      <td>1.420700</td>\n",
       "      <td>1.930248</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>1.110200</td>\n",
       "      <td>1.946146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>71</td>\n",
       "      <td>1.412700</td>\n",
       "      <td>1.944586</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>72</td>\n",
       "      <td>1.425600</td>\n",
       "      <td>1.946292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>73</td>\n",
       "      <td>1.476300</td>\n",
       "      <td>1.940813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>74</td>\n",
       "      <td>1.314300</td>\n",
       "      <td>1.952857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75</td>\n",
       "      <td>1.432500</td>\n",
       "      <td>1.965991</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>76</td>\n",
       "      <td>1.419800</td>\n",
       "      <td>1.946914</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>77</td>\n",
       "      <td>1.077800</td>\n",
       "      <td>1.969163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>78</td>\n",
       "      <td>1.609600</td>\n",
       "      <td>1.956470</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>79</td>\n",
       "      <td>1.447000</td>\n",
       "      <td>1.970763</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>1.145400</td>\n",
       "      <td>1.970303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>81</td>\n",
       "      <td>1.022900</td>\n",
       "      <td>1.978365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>82</td>\n",
       "      <td>1.457100</td>\n",
       "      <td>1.962831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>83</td>\n",
       "      <td>1.013000</td>\n",
       "      <td>1.976046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>84</td>\n",
       "      <td>1.097100</td>\n",
       "      <td>1.986464</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>85</td>\n",
       "      <td>1.425100</td>\n",
       "      <td>1.994300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>86</td>\n",
       "      <td>1.444500</td>\n",
       "      <td>1.999308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>87</td>\n",
       "      <td>0.853300</td>\n",
       "      <td>1.990387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>88</td>\n",
       "      <td>1.352600</td>\n",
       "      <td>2.010614</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>89</td>\n",
       "      <td>1.359500</td>\n",
       "      <td>1.995295</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>1.183100</td>\n",
       "      <td>2.013429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>91</td>\n",
       "      <td>1.442700</td>\n",
       "      <td>1.999189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>92</td>\n",
       "      <td>1.097300</td>\n",
       "      <td>2.010491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>93</td>\n",
       "      <td>1.187800</td>\n",
       "      <td>2.016085</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>94</td>\n",
       "      <td>1.062100</td>\n",
       "      <td>2.007502</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>95</td>\n",
       "      <td>1.163900</td>\n",
       "      <td>2.026667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>96</td>\n",
       "      <td>0.949600</td>\n",
       "      <td>2.018004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>97</td>\n",
       "      <td>1.477200</td>\n",
       "      <td>2.019857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>98</td>\n",
       "      <td>1.262900</td>\n",
       "      <td>2.020405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>99</td>\n",
       "      <td>1.663200</td>\n",
       "      <td>2.032882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>1.177400</td>\n",
       "      <td>2.014693</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>101</td>\n",
       "      <td>1.110000</td>\n",
       "      <td>2.029292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>102</td>\n",
       "      <td>1.338900</td>\n",
       "      <td>2.042284</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>103</td>\n",
       "      <td>1.399700</td>\n",
       "      <td>2.024164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>104</td>\n",
       "      <td>1.201800</td>\n",
       "      <td>2.039836</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>105</td>\n",
       "      <td>1.440200</td>\n",
       "      <td>2.040109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>106</td>\n",
       "      <td>1.355100</td>\n",
       "      <td>2.026923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>107</td>\n",
       "      <td>1.443500</td>\n",
       "      <td>2.037115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>108</td>\n",
       "      <td>1.263700</td>\n",
       "      <td>2.025132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>109</td>\n",
       "      <td>1.737500</td>\n",
       "      <td>2.036068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>1.108900</td>\n",
       "      <td>2.031248</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>111</td>\n",
       "      <td>1.387800</td>\n",
       "      <td>2.036581</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>112</td>\n",
       "      <td>1.352900</td>\n",
       "      <td>2.042408</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>113</td>\n",
       "      <td>0.936900</td>\n",
       "      <td>2.037603</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>114</td>\n",
       "      <td>1.176000</td>\n",
       "      <td>2.036394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>115</td>\n",
       "      <td>1.467400</td>\n",
       "      <td>2.039055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>116</td>\n",
       "      <td>1.538300</td>\n",
       "      <td>2.042707</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>117</td>\n",
       "      <td>1.652700</td>\n",
       "      <td>2.039830</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>118</td>\n",
       "      <td>0.992400</td>\n",
       "      <td>2.042417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>119</td>\n",
       "      <td>1.355000</td>\n",
       "      <td>2.043315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>1.207500</td>\n",
       "      <td>2.044528</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <style>\n",
       "        .wandb-row {\n",
       "            display: flex;\n",
       "            flex-direction: row;\n",
       "            flex-wrap: wrap;\n",
       "            justify-content: flex-start;\n",
       "            width: 100%;\n",
       "        }\n",
       "        .wandb-col {\n",
       "            display: flex;\n",
       "            flex-direction: column;\n",
       "            flex-basis: 100%;\n",
       "            flex: 1;\n",
       "            padding: 10px;\n",
       "        }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>▇▆▄▄▃▂▂▁▁▁▂▂▂▂▂▄▃▃▄▄▅▅▅▆▅▆▇▆▆▇▇▇██▇█████</td></tr><tr><td>eval/runtime</td><td>▄▄▃▄▂▂▃▅▃▄▃▆▂▃▃▄▃▃▁▃▄▃▃▂▄▃▃▄▃▄█▅▅▆▆▅▅▄▆▆</td></tr><tr><td>eval/samples_per_second</td><td>█▇▅▅▆▆▆▇▆▅▇▇▆▄▆▆▆▆▆▆█▆▆▆▇▇▆█▆▆▆▆▁▅▄▃▃▅▃▄</td></tr><tr><td>eval/steps_per_second</td><td>▇█▄▇▄▅▆▆▆▅▄▇▅▅▅█▅▅▅█▅▅▄▅▇▆▅▆▇▄▄▃▂▃▃▃▂▁▄▂</td></tr><tr><td>train/epoch</td><td>▁▁▁▁▁▂▂▂▃▃▃▃▃▃▃▄▄▄▄▅▅▅▅▅▅▆▆▇▇▇▇▇▇▇▇█████</td></tr><tr><td>train/global_step</td><td>▁▁▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▆▆▆▆▆▆▇▇▇████</td></tr><tr><td>train/grad_norm</td><td>█▃▃▂▁▂▃▂▂▄▃▃▃▂▂▃▂▂▃▃▃▃▂▂▄▃▆▃▁▂▁▄▃▂▃▂▃▃▂▃</td></tr><tr><td>train/learning_rate</td><td>███▇▇▇▆▆▆▆▆▆▅▅▅▅▅▅▅▅▄▄▄▄▄▄▄▄▃▃▂▂▂▂▂▂▂▂▁▁</td></tr><tr><td>train/loss</td><td>▆▆▅█▅█▇▅▄▅▄▆▄▄▅▂▅▅▄▃▄▅▃▃▃▂▃▂▃▃▃▄▂▃▂▁▂▄▃▂</td></tr><tr><td>train/total_flos</td><td>▁</td></tr><tr><td>train/train_loss</td><td>▁</td></tr><tr><td>train/train_runtime</td><td>▁</td></tr><tr><td>train/train_samples_per_second</td><td>▁</td></tr><tr><td>train/train_steps_per_second</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>2.04453</td></tr><tr><td>eval/runtime</td><td>1.1027</td></tr><tr><td>eval/samples_per_second</td><td>15.417</td></tr><tr><td>eval/steps_per_second</td><td>15.417</td></tr><tr><td>train/epoch</td><td>120</td></tr><tr><td>train/global_step</td><td>7800</td></tr><tr><td>train/grad_norm</td><td>1.89957</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>1.2075</td></tr><tr><td>train/total_flos</td><td>178651699200000.0</td></tr><tr><td>train/train_loss</td><td>1.51674</td></tr><tr><td>train/train_runtime</td><td>1719.0866</td></tr><tr><td>train/train_samples_per_second</td><td>4.537</td></tr><tr><td>train/train_steps_per_second</td><td>4.537</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">snare_training_run_gpt2_cross_10secsdataset_final</strong> at: <a href='https://wandb.ai/uniqlabs/music_generation/runs/gu3tmbuo' target=\"_blank\">https://wandb.ai/uniqlabs/music_generation/runs/gu3tmbuo</a><br/> View project at: <a href='https://wandb.ai/uniqlabs/music_generation' target=\"_blank\">https://wandb.ai/uniqlabs/music_generation</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20241128_204615-gu3tmbuo\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for clap...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>C:\\Users\\Ripple\\Desktop\\5th sem\\practical work\\code\\wandb\\run-20241128_211457-79ortedk</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/uniqlabs/music_generation/runs/79ortedk' target=\"_blank\">clap_training_run_gpt2_cross_10secsdataset_final</a></strong> to <a href='https://wandb.ai/uniqlabs/music_generation' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/uniqlabs/music_generation' target=\"_blank\">https://wandb.ai/uniqlabs/music_generation</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/uniqlabs/music_generation/runs/79ortedk' target=\"_blank\">https://wandb.ai/uniqlabs/music_generation/runs/79ortedk</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ripple\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\accelerate\\accelerator.py:436: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='8760' max='8760' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [8760/8760 32:19, Epoch 120/120]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.569200</td>\n",
       "      <td>1.424866</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.389800</td>\n",
       "      <td>1.366259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.573100</td>\n",
       "      <td>1.332663</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.424800</td>\n",
       "      <td>1.294087</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.238200</td>\n",
       "      <td>1.271235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>1.666400</td>\n",
       "      <td>1.238858</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.921200</td>\n",
       "      <td>1.228014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>1.426000</td>\n",
       "      <td>1.207181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>1.126700</td>\n",
       "      <td>1.197819</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.790800</td>\n",
       "      <td>1.198611</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>1.187500</td>\n",
       "      <td>1.210220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>1.295900</td>\n",
       "      <td>1.175455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>1.147800</td>\n",
       "      <td>1.171923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.939800</td>\n",
       "      <td>1.165072</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>1.296000</td>\n",
       "      <td>1.163652</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>1.182700</td>\n",
       "      <td>1.155836</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.826300</td>\n",
       "      <td>1.159821</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.869800</td>\n",
       "      <td>1.142154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.867100</td>\n",
       "      <td>1.161921</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>1.020400</td>\n",
       "      <td>1.167852</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>1.119000</td>\n",
       "      <td>1.124355</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>1.041000</td>\n",
       "      <td>1.133402</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>1.022800</td>\n",
       "      <td>1.135021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>0.855300</td>\n",
       "      <td>1.136674</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.635700</td>\n",
       "      <td>1.147385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>0.490800</td>\n",
       "      <td>1.129789</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>0.896000</td>\n",
       "      <td>1.116591</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>0.943100</td>\n",
       "      <td>1.135271</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>0.874600</td>\n",
       "      <td>1.124627</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.571300</td>\n",
       "      <td>1.126712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31</td>\n",
       "      <td>0.758300</td>\n",
       "      <td>1.117175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>1.097400</td>\n",
       "      <td>1.136196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33</td>\n",
       "      <td>0.935000</td>\n",
       "      <td>1.124852</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34</td>\n",
       "      <td>1.012300</td>\n",
       "      <td>1.143592</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>1.031000</td>\n",
       "      <td>1.131993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36</td>\n",
       "      <td>0.786500</td>\n",
       "      <td>1.123438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37</td>\n",
       "      <td>1.079300</td>\n",
       "      <td>1.159217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38</td>\n",
       "      <td>0.904100</td>\n",
       "      <td>1.137133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39</td>\n",
       "      <td>0.983300</td>\n",
       "      <td>1.121537</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>1.111600</td>\n",
       "      <td>1.107688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41</td>\n",
       "      <td>0.765400</td>\n",
       "      <td>1.125210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42</td>\n",
       "      <td>0.718400</td>\n",
       "      <td>1.127450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43</td>\n",
       "      <td>0.705200</td>\n",
       "      <td>1.130255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44</td>\n",
       "      <td>0.824200</td>\n",
       "      <td>1.128133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45</td>\n",
       "      <td>0.791100</td>\n",
       "      <td>1.134243</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46</td>\n",
       "      <td>0.868100</td>\n",
       "      <td>1.119798</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47</td>\n",
       "      <td>0.824000</td>\n",
       "      <td>1.122815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48</td>\n",
       "      <td>0.773600</td>\n",
       "      <td>1.166983</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49</td>\n",
       "      <td>0.810800</td>\n",
       "      <td>1.124805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.803000</td>\n",
       "      <td>1.128401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>51</td>\n",
       "      <td>0.639000</td>\n",
       "      <td>1.137190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>52</td>\n",
       "      <td>0.531800</td>\n",
       "      <td>1.148805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>53</td>\n",
       "      <td>0.975400</td>\n",
       "      <td>1.137628</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>54</td>\n",
       "      <td>0.773100</td>\n",
       "      <td>1.166178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>55</td>\n",
       "      <td>0.575800</td>\n",
       "      <td>1.153478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>56</td>\n",
       "      <td>0.931000</td>\n",
       "      <td>1.126730</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>57</td>\n",
       "      <td>0.653100</td>\n",
       "      <td>1.140421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>58</td>\n",
       "      <td>0.749300</td>\n",
       "      <td>1.144541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>59</td>\n",
       "      <td>0.867500</td>\n",
       "      <td>1.141349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.496700</td>\n",
       "      <td>1.136618</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>61</td>\n",
       "      <td>1.190300</td>\n",
       "      <td>1.141113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>62</td>\n",
       "      <td>0.526300</td>\n",
       "      <td>1.142960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>63</td>\n",
       "      <td>0.937700</td>\n",
       "      <td>1.159460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>64</td>\n",
       "      <td>0.565200</td>\n",
       "      <td>1.142953</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>65</td>\n",
       "      <td>0.845000</td>\n",
       "      <td>1.176569</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>66</td>\n",
       "      <td>1.001200</td>\n",
       "      <td>1.149668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>67</td>\n",
       "      <td>0.760900</td>\n",
       "      <td>1.129177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>68</td>\n",
       "      <td>0.602600</td>\n",
       "      <td>1.183697</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>69</td>\n",
       "      <td>0.700900</td>\n",
       "      <td>1.150737</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>0.645100</td>\n",
       "      <td>1.131249</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>71</td>\n",
       "      <td>0.732300</td>\n",
       "      <td>1.149683</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>72</td>\n",
       "      <td>0.598200</td>\n",
       "      <td>1.171227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>73</td>\n",
       "      <td>0.681700</td>\n",
       "      <td>1.170591</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>74</td>\n",
       "      <td>0.707600</td>\n",
       "      <td>1.170373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75</td>\n",
       "      <td>0.547500</td>\n",
       "      <td>1.166132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>76</td>\n",
       "      <td>0.498800</td>\n",
       "      <td>1.182685</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>77</td>\n",
       "      <td>0.426500</td>\n",
       "      <td>1.172657</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>78</td>\n",
       "      <td>0.571000</td>\n",
       "      <td>1.166660</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>79</td>\n",
       "      <td>0.581200</td>\n",
       "      <td>1.144355</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.706400</td>\n",
       "      <td>1.173896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>81</td>\n",
       "      <td>0.762400</td>\n",
       "      <td>1.160354</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>82</td>\n",
       "      <td>0.402600</td>\n",
       "      <td>1.179994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>83</td>\n",
       "      <td>0.625400</td>\n",
       "      <td>1.144478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>84</td>\n",
       "      <td>1.180400</td>\n",
       "      <td>1.157973</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>85</td>\n",
       "      <td>0.783500</td>\n",
       "      <td>1.158046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>86</td>\n",
       "      <td>0.656400</td>\n",
       "      <td>1.176533</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>87</td>\n",
       "      <td>0.481000</td>\n",
       "      <td>1.170855</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>88</td>\n",
       "      <td>0.659700</td>\n",
       "      <td>1.179018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>89</td>\n",
       "      <td>0.522100</td>\n",
       "      <td>1.178176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>0.625200</td>\n",
       "      <td>1.165163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>91</td>\n",
       "      <td>0.774000</td>\n",
       "      <td>1.160994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>92</td>\n",
       "      <td>0.895500</td>\n",
       "      <td>1.166132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>93</td>\n",
       "      <td>0.548100</td>\n",
       "      <td>1.197673</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>94</td>\n",
       "      <td>0.500600</td>\n",
       "      <td>1.192716</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>95</td>\n",
       "      <td>0.448700</td>\n",
       "      <td>1.176293</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>96</td>\n",
       "      <td>0.667400</td>\n",
       "      <td>1.180747</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>97</td>\n",
       "      <td>0.677400</td>\n",
       "      <td>1.188155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>98</td>\n",
       "      <td>0.776100</td>\n",
       "      <td>1.171326</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>99</td>\n",
       "      <td>0.852000</td>\n",
       "      <td>1.202105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.328600</td>\n",
       "      <td>1.180133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>101</td>\n",
       "      <td>0.632700</td>\n",
       "      <td>1.170072</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>102</td>\n",
       "      <td>0.752800</td>\n",
       "      <td>1.188981</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>103</td>\n",
       "      <td>0.779900</td>\n",
       "      <td>1.173448</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>104</td>\n",
       "      <td>0.599200</td>\n",
       "      <td>1.189847</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>105</td>\n",
       "      <td>0.582100</td>\n",
       "      <td>1.178668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>106</td>\n",
       "      <td>0.794200</td>\n",
       "      <td>1.171691</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>107</td>\n",
       "      <td>0.792300</td>\n",
       "      <td>1.190729</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>108</td>\n",
       "      <td>0.508900</td>\n",
       "      <td>1.175635</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>109</td>\n",
       "      <td>0.463400</td>\n",
       "      <td>1.187552</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>0.803100</td>\n",
       "      <td>1.186921</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>111</td>\n",
       "      <td>0.543800</td>\n",
       "      <td>1.196599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>112</td>\n",
       "      <td>0.559200</td>\n",
       "      <td>1.187456</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>113</td>\n",
       "      <td>0.497800</td>\n",
       "      <td>1.178571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>114</td>\n",
       "      <td>0.598000</td>\n",
       "      <td>1.189786</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>115</td>\n",
       "      <td>0.655300</td>\n",
       "      <td>1.193322</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>116</td>\n",
       "      <td>0.619000</td>\n",
       "      <td>1.190282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>117</td>\n",
       "      <td>0.529500</td>\n",
       "      <td>1.191589</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>118</td>\n",
       "      <td>0.624800</td>\n",
       "      <td>1.191190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>119</td>\n",
       "      <td>0.669700</td>\n",
       "      <td>1.190942</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>0.662800</td>\n",
       "      <td>1.190383</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <style>\n",
       "        .wandb-row {\n",
       "            display: flex;\n",
       "            flex-direction: row;\n",
       "            flex-wrap: wrap;\n",
       "            justify-content: flex-start;\n",
       "            width: 100%;\n",
       "        }\n",
       "        .wandb-col {\n",
       "            display: flex;\n",
       "            flex-direction: column;\n",
       "            flex-basis: 100%;\n",
       "            flex: 1;\n",
       "            padding: 10px;\n",
       "        }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>█▆▅▃▃▂▂▃▂▂▂▂▂▂▁▁▂▁▂▃▂▂▂▂▃▂▂▃▃▂▂▃▃▂▃▃▃▃▃▃</td></tr><tr><td>eval/runtime</td><td>▁▃▂▄▂▃▂▃▄▃▂▄▃▃▃▂▅▃▂▁▂▄▄▃▃▃▄▅▃▃▃▂▁█▃▂▂▃▃▁</td></tr><tr><td>eval/samples_per_second</td><td>█▄▆▆▅▅▄▅▄▄▄▃▃▄▄▄▃▁▆█▂▅▃▂▁▄▆▃▅▄▄▆▄▅▆▅▄▇▃▆</td></tr><tr><td>eval/steps_per_second</td><td>█▆▅▆▇▆▄▆▇▇▄▅▇█▆▅▆▆▆▆▄▆▅▇▅▆█▆▁▇▆▆▆▃█▆▆▆▇▇</td></tr><tr><td>train/epoch</td><td>▁▁▁▁▁▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇▇███</td></tr><tr><td>train/global_step</td><td>▁▁▂▂▂▂▂▃▃▃▃▃▃▃▃▄▄▄▅▅▅▅▅▅▅▅▆▆▆▆▆▆▆▇▇▇▇▇██</td></tr><tr><td>train/grad_norm</td><td>▇▄▇▅▄▁▇▄▅▁▅▄▇▆▇▄▅▅▅▄▅▅▄▃▄▄▅▅▃█▅▅▁▃▃▃▁▅▁▁</td></tr><tr><td>train/learning_rate</td><td>███▇▇▇▇▆▆▆▆▆▅▅▅▅▅▅▅▅▄▄▄▄▄▄▄▃▃▃▃▃▃▃▃▂▂▁▁▁</td></tr><tr><td>train/loss</td><td>▇█▇▅▁▃▄▂▄▅▅▆▄▃▃▄▃▄▆▂▃▂▅▂▃▃▂▁▂▂▂▁▃▂▁▁▂▂▃▂</td></tr><tr><td>train/total_flos</td><td>▁</td></tr><tr><td>train/train_loss</td><td>▁</td></tr><tr><td>train/train_runtime</td><td>▁</td></tr><tr><td>train/train_samples_per_second</td><td>▁</td></tr><tr><td>train/train_steps_per_second</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>1.19038</td></tr><tr><td>eval/runtime</td><td>1.2333</td></tr><tr><td>eval/samples_per_second</td><td>15.406</td></tr><tr><td>eval/steps_per_second</td><td>15.406</td></tr><tr><td>train/epoch</td><td>120</td></tr><tr><td>train/global_step</td><td>8760</td></tr><tr><td>train/grad_norm</td><td>2.07013</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>0.6628</td></tr><tr><td>train/total_flos</td><td>200639600640000.0</td></tr><tr><td>train/train_loss</td><td>0.81983</td></tr><tr><td>train/train_runtime</td><td>1939.8588</td></tr><tr><td>train/train_samples_per_second</td><td>4.516</td></tr><tr><td>train/train_steps_per_second</td><td>4.516</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">clap_training_run_gpt2_cross_10secsdataset_final</strong> at: <a href='https://wandb.ai/uniqlabs/music_generation/runs/79ortedk' target=\"_blank\">https://wandb.ai/uniqlabs/music_generation/runs/79ortedk</a><br/> View project at: <a href='https://wandb.ai/uniqlabs/music_generation' target=\"_blank\">https://wandb.ai/uniqlabs/music_generation</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20241128_211457-79ortedk\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for bass...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>C:\\Users\\Ripple\\Desktop\\5th sem\\practical work\\code\\wandb\\run-20241128_214720-6wj98j1o</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/uniqlabs/music_generation/runs/6wj98j1o' target=\"_blank\">bass_training_run_gpt2_cross_10secsdataset_final</a></strong> to <a href='https://wandb.ai/uniqlabs/music_generation' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/uniqlabs/music_generation' target=\"_blank\">https://wandb.ai/uniqlabs/music_generation</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/uniqlabs/music_generation/runs/6wj98j1o' target=\"_blank\">https://wandb.ai/uniqlabs/music_generation/runs/6wj98j1o</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ripple\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\accelerate\\accelerator.py:436: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='14400' max='14400' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [14400/14400 52:51, Epoch 120/120]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>3.735300</td>\n",
       "      <td>3.878748</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>4.573700</td>\n",
       "      <td>3.757229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>3.555200</td>\n",
       "      <td>3.687701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>3.995100</td>\n",
       "      <td>3.624350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>3.584000</td>\n",
       "      <td>3.582553</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>3.608500</td>\n",
       "      <td>3.554781</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>3.707200</td>\n",
       "      <td>3.520705</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>3.250800</td>\n",
       "      <td>3.480429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>3.549400</td>\n",
       "      <td>3.483246</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>3.510200</td>\n",
       "      <td>3.458662</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>3.353200</td>\n",
       "      <td>3.479765</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>3.741100</td>\n",
       "      <td>3.462106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>3.272500</td>\n",
       "      <td>3.461218</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>3.999200</td>\n",
       "      <td>3.419217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>3.551100</td>\n",
       "      <td>3.430103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>3.176100</td>\n",
       "      <td>3.413760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>2.687500</td>\n",
       "      <td>3.407651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>3.158600</td>\n",
       "      <td>3.406158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>3.360000</td>\n",
       "      <td>3.395537</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>3.802200</td>\n",
       "      <td>3.413460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>3.176900</td>\n",
       "      <td>3.390396</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>3.720800</td>\n",
       "      <td>3.406471</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>3.173900</td>\n",
       "      <td>3.408134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>2.952800</td>\n",
       "      <td>3.398875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>3.237600</td>\n",
       "      <td>3.390431</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>2.637700</td>\n",
       "      <td>3.410331</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>3.192400</td>\n",
       "      <td>3.404486</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>3.017300</td>\n",
       "      <td>3.390433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>3.252500</td>\n",
       "      <td>3.402696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>2.921900</td>\n",
       "      <td>3.388780</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31</td>\n",
       "      <td>3.662400</td>\n",
       "      <td>3.401459</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>3.192900</td>\n",
       "      <td>3.395953</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33</td>\n",
       "      <td>3.441300</td>\n",
       "      <td>3.397278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34</td>\n",
       "      <td>3.669200</td>\n",
       "      <td>3.394725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>3.117000</td>\n",
       "      <td>3.409279</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36</td>\n",
       "      <td>3.658600</td>\n",
       "      <td>3.394008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37</td>\n",
       "      <td>3.430900</td>\n",
       "      <td>3.392711</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38</td>\n",
       "      <td>3.748400</td>\n",
       "      <td>3.401871</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39</td>\n",
       "      <td>2.914500</td>\n",
       "      <td>3.403777</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>3.505700</td>\n",
       "      <td>3.405625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41</td>\n",
       "      <td>3.649700</td>\n",
       "      <td>3.406659</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42</td>\n",
       "      <td>3.628900</td>\n",
       "      <td>3.395031</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43</td>\n",
       "      <td>3.034100</td>\n",
       "      <td>3.411264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44</td>\n",
       "      <td>2.902600</td>\n",
       "      <td>3.393687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45</td>\n",
       "      <td>3.411000</td>\n",
       "      <td>3.396449</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46</td>\n",
       "      <td>3.051900</td>\n",
       "      <td>3.409664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47</td>\n",
       "      <td>3.114000</td>\n",
       "      <td>3.396990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48</td>\n",
       "      <td>2.841900</td>\n",
       "      <td>3.401863</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49</td>\n",
       "      <td>3.162500</td>\n",
       "      <td>3.393491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>2.908800</td>\n",
       "      <td>3.405942</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>51</td>\n",
       "      <td>2.895500</td>\n",
       "      <td>3.410300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>52</td>\n",
       "      <td>2.901300</td>\n",
       "      <td>3.410239</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>53</td>\n",
       "      <td>2.420500</td>\n",
       "      <td>3.399055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>54</td>\n",
       "      <td>3.478000</td>\n",
       "      <td>3.404901</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>55</td>\n",
       "      <td>2.898200</td>\n",
       "      <td>3.405997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>56</td>\n",
       "      <td>3.460400</td>\n",
       "      <td>3.401000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>57</td>\n",
       "      <td>2.856000</td>\n",
       "      <td>3.423455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>58</td>\n",
       "      <td>3.397100</td>\n",
       "      <td>3.419856</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>59</td>\n",
       "      <td>3.535600</td>\n",
       "      <td>3.422663</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>3.659200</td>\n",
       "      <td>3.424404</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>61</td>\n",
       "      <td>3.334400</td>\n",
       "      <td>3.421075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>62</td>\n",
       "      <td>3.029400</td>\n",
       "      <td>3.412634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>63</td>\n",
       "      <td>3.308000</td>\n",
       "      <td>3.418211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>64</td>\n",
       "      <td>3.172500</td>\n",
       "      <td>3.415405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>65</td>\n",
       "      <td>3.595300</td>\n",
       "      <td>3.419000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>66</td>\n",
       "      <td>3.013600</td>\n",
       "      <td>3.415543</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>67</td>\n",
       "      <td>3.275200</td>\n",
       "      <td>3.415604</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>68</td>\n",
       "      <td>3.047600</td>\n",
       "      <td>3.422913</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>69</td>\n",
       "      <td>3.138800</td>\n",
       "      <td>3.433138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>3.123100</td>\n",
       "      <td>3.424624</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>71</td>\n",
       "      <td>3.022400</td>\n",
       "      <td>3.421611</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>72</td>\n",
       "      <td>2.935100</td>\n",
       "      <td>3.450374</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>73</td>\n",
       "      <td>3.361400</td>\n",
       "      <td>3.431030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>74</td>\n",
       "      <td>2.782700</td>\n",
       "      <td>3.431429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75</td>\n",
       "      <td>2.898300</td>\n",
       "      <td>3.428257</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>76</td>\n",
       "      <td>2.913100</td>\n",
       "      <td>3.442968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>77</td>\n",
       "      <td>2.639100</td>\n",
       "      <td>3.436224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>78</td>\n",
       "      <td>3.003600</td>\n",
       "      <td>3.430235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>79</td>\n",
       "      <td>2.755600</td>\n",
       "      <td>3.423389</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>3.001600</td>\n",
       "      <td>3.432907</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>81</td>\n",
       "      <td>2.967600</td>\n",
       "      <td>3.436783</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>82</td>\n",
       "      <td>2.727300</td>\n",
       "      <td>3.444255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>83</td>\n",
       "      <td>2.947400</td>\n",
       "      <td>3.455693</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>84</td>\n",
       "      <td>2.678500</td>\n",
       "      <td>3.437575</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>85</td>\n",
       "      <td>3.572600</td>\n",
       "      <td>3.453400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>86</td>\n",
       "      <td>2.545200</td>\n",
       "      <td>3.442535</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>87</td>\n",
       "      <td>2.962800</td>\n",
       "      <td>3.446033</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>88</td>\n",
       "      <td>2.692700</td>\n",
       "      <td>3.447556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>89</td>\n",
       "      <td>2.784500</td>\n",
       "      <td>3.442391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>2.799100</td>\n",
       "      <td>3.450309</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>91</td>\n",
       "      <td>3.015300</td>\n",
       "      <td>3.450475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>92</td>\n",
       "      <td>3.031900</td>\n",
       "      <td>3.458417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>93</td>\n",
       "      <td>2.813200</td>\n",
       "      <td>3.459700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>94</td>\n",
       "      <td>2.933500</td>\n",
       "      <td>3.463820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>95</td>\n",
       "      <td>2.654500</td>\n",
       "      <td>3.456813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>96</td>\n",
       "      <td>3.313400</td>\n",
       "      <td>3.466598</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>97</td>\n",
       "      <td>3.220100</td>\n",
       "      <td>3.462917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>98</td>\n",
       "      <td>2.423500</td>\n",
       "      <td>3.467546</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>99</td>\n",
       "      <td>2.650500</td>\n",
       "      <td>3.465798</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>2.595000</td>\n",
       "      <td>3.462814</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>101</td>\n",
       "      <td>2.639300</td>\n",
       "      <td>3.459618</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>102</td>\n",
       "      <td>2.842300</td>\n",
       "      <td>3.458915</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>103</td>\n",
       "      <td>3.017200</td>\n",
       "      <td>3.472732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>104</td>\n",
       "      <td>3.413900</td>\n",
       "      <td>3.466588</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>105</td>\n",
       "      <td>2.771500</td>\n",
       "      <td>3.470099</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>106</td>\n",
       "      <td>2.983500</td>\n",
       "      <td>3.467613</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>107</td>\n",
       "      <td>2.696400</td>\n",
       "      <td>3.474712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>108</td>\n",
       "      <td>2.814800</td>\n",
       "      <td>3.473093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>109</td>\n",
       "      <td>2.768200</td>\n",
       "      <td>3.464183</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>2.987800</td>\n",
       "      <td>3.478517</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>111</td>\n",
       "      <td>2.523500</td>\n",
       "      <td>3.479254</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>112</td>\n",
       "      <td>2.243400</td>\n",
       "      <td>3.471073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>113</td>\n",
       "      <td>2.561100</td>\n",
       "      <td>3.472000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>114</td>\n",
       "      <td>3.129000</td>\n",
       "      <td>3.474607</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>115</td>\n",
       "      <td>2.582500</td>\n",
       "      <td>3.475106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>116</td>\n",
       "      <td>3.412800</td>\n",
       "      <td>3.475654</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>117</td>\n",
       "      <td>2.506200</td>\n",
       "      <td>3.474310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>118</td>\n",
       "      <td>2.370000</td>\n",
       "      <td>3.478043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>119</td>\n",
       "      <td>2.776600</td>\n",
       "      <td>3.477912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>3.176300</td>\n",
       "      <td>3.477299</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <style>\n",
       "        .wandb-row {\n",
       "            display: flex;\n",
       "            flex-direction: row;\n",
       "            flex-wrap: wrap;\n",
       "            justify-content: flex-start;\n",
       "            width: 100%;\n",
       "        }\n",
       "        .wandb-col {\n",
       "            display: flex;\n",
       "            flex-direction: column;\n",
       "            flex-basis: 100%;\n",
       "            flex: 1;\n",
       "            padding: 10px;\n",
       "        }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>█▅▄▃▂▁▁▁▁▁▁▁▁▁▁▁▂▂▂▁▁▂▂▂▂▂▂▂▂▂▂▂▃▃▃▃▃▃▃▃</td></tr><tr><td>eval/runtime</td><td>▄▂▅▅▇▃▃▃▂▂▇▅▄▄▄▅▄▃▅▃▄█▃▅▂▃▆▃▅▅▅▇▄▄▂▃▃▁▅▁</td></tr><tr><td>eval/samples_per_second</td><td>▁▆█▅▆▇▇▆▇▇▇█▅▆▆▇▅▇▅▇▆▆▅█▇▆▆▆▇▆▇█▇█▇▇█▆▅▆</td></tr><tr><td>eval/steps_per_second</td><td>▁▇▆█▅▆▄▇▆▃▆▅▅█▆▇▂▅▆▆▇▆▆▅▄▆▆▆▅▃▅▆▆▄▅▅█▆▆█</td></tr><tr><td>train/epoch</td><td>▁▁▁▂▂▂▂▃▃▃▄▄▄▄▄▄▄▄▄▅▅▅▅▅▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇█</td></tr><tr><td>train/global_step</td><td>▁▁▁▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▅▅▅▅▅▅▅▆▆▇▇▇▇▇▇▇███</td></tr><tr><td>train/grad_norm</td><td>▂▂▂▂▁▃▂▃▃▃▃▅▂▃█▂▃▃▂▅▂▁▃▂▂▃▂▃▃▃▂▂▂▃▂▂▃▃▂▂</td></tr><tr><td>train/learning_rate</td><td>█████▇▇▇▇▇▇▆▆▆▅▅▅▅▅▅▄▄▄▄▄▃▃▃▃▃▂▂▂▂▂▂▂▁▁▁</td></tr><tr><td>train/loss</td><td>▇▆▅▅▅▂▂█▂▃▇▃▆▄▄▄▂▇▄▃▂▃▂▃▁▂▂▁▃▄▁▃▃▂▃▂▃▂▃▂</td></tr><tr><td>train/total_flos</td><td>▁</td></tr><tr><td>train/train_loss</td><td>▁</td></tr><tr><td>train/train_runtime</td><td>▁</td></tr><tr><td>train/train_samples_per_second</td><td>▁</td></tr><tr><td>train/train_steps_per_second</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>3.4773</td></tr><tr><td>eval/runtime</td><td>1.9504</td></tr><tr><td>eval/samples_per_second</td><td>15.382</td></tr><tr><td>eval/steps_per_second</td><td>15.382</td></tr><tr><td>train/epoch</td><td>120</td></tr><tr><td>train/global_step</td><td>14400</td></tr><tr><td>train/grad_norm</td><td>1.62296</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>3.1763</td></tr><tr><td>train/total_flos</td><td>329818521600000.0</td></tr><tr><td>train/train_loss</td><td>3.1351</td></tr><tr><td>train/train_runtime</td><td>3172.4386</td></tr><tr><td>train/train_samples_per_second</td><td>4.539</td></tr><tr><td>train/train_steps_per_second</td><td>4.539</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">bass_training_run_gpt2_cross_10secsdataset_final</strong> at: <a href='https://wandb.ai/uniqlabs/music_generation/runs/6wj98j1o' target=\"_blank\">https://wandb.ai/uniqlabs/music_generation/runs/6wj98j1o</a><br/> View project at: <a href='https://wandb.ai/uniqlabs/music_generation' target=\"_blank\">https://wandb.ai/uniqlabs/music_generation</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20241128_214720-6wj98j1o\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for drums...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>C:\\Users\\Ripple\\Desktop\\5th sem\\practical work\\code\\wandb\\run-20241128_224016-9ndj83h1</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/uniqlabs/music_generation/runs/9ndj83h1' target=\"_blank\">drums_training_run_gpt2_cross_10secsdataset_final</a></strong> to <a href='https://wandb.ai/uniqlabs/music_generation' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/uniqlabs/music_generation' target=\"_blank\">https://wandb.ai/uniqlabs/music_generation</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/uniqlabs/music_generation/runs/9ndj83h1' target=\"_blank\">https://wandb.ai/uniqlabs/music_generation/runs/9ndj83h1</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ripple\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\accelerate\\accelerator.py:436: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='16320' max='16320' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [16320/16320 59:55, Epoch 120/120]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>5.634700</td>\n",
       "      <td>4.957232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>5.371600</td>\n",
       "      <td>4.840569</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>5.310200</td>\n",
       "      <td>4.772684</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>5.393100</td>\n",
       "      <td>4.731454</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>5.017100</td>\n",
       "      <td>4.687002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>5.290000</td>\n",
       "      <td>4.654484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>5.148800</td>\n",
       "      <td>4.669750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>5.071800</td>\n",
       "      <td>4.609754</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>5.156600</td>\n",
       "      <td>4.611542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>5.389300</td>\n",
       "      <td>4.591094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>4.866000</td>\n",
       "      <td>4.586672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>4.590300</td>\n",
       "      <td>4.573672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>4.974600</td>\n",
       "      <td>4.549353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>4.504000</td>\n",
       "      <td>4.545960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>5.099800</td>\n",
       "      <td>4.531033</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>4.567100</td>\n",
       "      <td>4.535037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>5.019200</td>\n",
       "      <td>4.512216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>4.668200</td>\n",
       "      <td>4.518626</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>4.300800</td>\n",
       "      <td>4.524993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>4.248700</td>\n",
       "      <td>4.498063</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>4.910600</td>\n",
       "      <td>4.496309</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>4.489500</td>\n",
       "      <td>4.493145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>4.722500</td>\n",
       "      <td>4.483037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>5.046400</td>\n",
       "      <td>4.483225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>4.241900</td>\n",
       "      <td>4.490862</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>4.722200</td>\n",
       "      <td>4.469673</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>4.840300</td>\n",
       "      <td>4.470227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>4.660400</td>\n",
       "      <td>4.470942</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>4.655600</td>\n",
       "      <td>4.467505</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>4.866700</td>\n",
       "      <td>4.473438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31</td>\n",
       "      <td>4.387500</td>\n",
       "      <td>4.461758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>4.629100</td>\n",
       "      <td>4.466520</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33</td>\n",
       "      <td>4.204000</td>\n",
       "      <td>4.475597</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34</td>\n",
       "      <td>4.007500</td>\n",
       "      <td>4.471162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>4.640900</td>\n",
       "      <td>4.461619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36</td>\n",
       "      <td>4.630000</td>\n",
       "      <td>4.454770</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37</td>\n",
       "      <td>4.909300</td>\n",
       "      <td>4.457543</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38</td>\n",
       "      <td>4.595900</td>\n",
       "      <td>4.467862</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39</td>\n",
       "      <td>4.418800</td>\n",
       "      <td>4.457385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>4.685000</td>\n",
       "      <td>4.455530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41</td>\n",
       "      <td>3.813500</td>\n",
       "      <td>4.461441</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42</td>\n",
       "      <td>3.843300</td>\n",
       "      <td>4.465900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43</td>\n",
       "      <td>4.605500</td>\n",
       "      <td>4.455024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44</td>\n",
       "      <td>3.903700</td>\n",
       "      <td>4.455742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45</td>\n",
       "      <td>4.350000</td>\n",
       "      <td>4.457008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46</td>\n",
       "      <td>4.502100</td>\n",
       "      <td>4.461558</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47</td>\n",
       "      <td>4.532600</td>\n",
       "      <td>4.461071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48</td>\n",
       "      <td>4.472500</td>\n",
       "      <td>4.454962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49</td>\n",
       "      <td>4.476500</td>\n",
       "      <td>4.444068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>4.654500</td>\n",
       "      <td>4.455042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>51</td>\n",
       "      <td>4.704700</td>\n",
       "      <td>4.456514</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>52</td>\n",
       "      <td>4.402000</td>\n",
       "      <td>4.448630</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>53</td>\n",
       "      <td>3.386700</td>\n",
       "      <td>4.461221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>54</td>\n",
       "      <td>4.550100</td>\n",
       "      <td>4.457919</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>55</td>\n",
       "      <td>4.100700</td>\n",
       "      <td>4.453388</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>56</td>\n",
       "      <td>4.473000</td>\n",
       "      <td>4.448411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>57</td>\n",
       "      <td>3.747400</td>\n",
       "      <td>4.458097</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>58</td>\n",
       "      <td>4.367500</td>\n",
       "      <td>4.457064</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>59</td>\n",
       "      <td>4.089600</td>\n",
       "      <td>4.451606</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>3.954700</td>\n",
       "      <td>4.454968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>61</td>\n",
       "      <td>4.241600</td>\n",
       "      <td>4.453292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>62</td>\n",
       "      <td>4.101900</td>\n",
       "      <td>4.459708</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>63</td>\n",
       "      <td>4.102400</td>\n",
       "      <td>4.452662</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>64</td>\n",
       "      <td>4.002400</td>\n",
       "      <td>4.463188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>65</td>\n",
       "      <td>3.572600</td>\n",
       "      <td>4.462745</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>66</td>\n",
       "      <td>4.103600</td>\n",
       "      <td>4.477554</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>67</td>\n",
       "      <td>4.198000</td>\n",
       "      <td>4.460622</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>68</td>\n",
       "      <td>4.406400</td>\n",
       "      <td>4.473962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>69</td>\n",
       "      <td>4.178200</td>\n",
       "      <td>4.460557</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>4.298000</td>\n",
       "      <td>4.457212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>71</td>\n",
       "      <td>4.098400</td>\n",
       "      <td>4.464324</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>72</td>\n",
       "      <td>4.299900</td>\n",
       "      <td>4.457651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>73</td>\n",
       "      <td>4.263200</td>\n",
       "      <td>4.468151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>74</td>\n",
       "      <td>3.990300</td>\n",
       "      <td>4.472809</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75</td>\n",
       "      <td>4.001900</td>\n",
       "      <td>4.458958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>76</td>\n",
       "      <td>4.244300</td>\n",
       "      <td>4.463433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>77</td>\n",
       "      <td>3.762500</td>\n",
       "      <td>4.469455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>78</td>\n",
       "      <td>3.869000</td>\n",
       "      <td>4.467231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>79</td>\n",
       "      <td>4.296100</td>\n",
       "      <td>4.473464</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>4.172500</td>\n",
       "      <td>4.462577</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>81</td>\n",
       "      <td>4.115800</td>\n",
       "      <td>4.465915</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>82</td>\n",
       "      <td>4.248900</td>\n",
       "      <td>4.469476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>83</td>\n",
       "      <td>4.540200</td>\n",
       "      <td>4.466128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>84</td>\n",
       "      <td>4.599000</td>\n",
       "      <td>4.476860</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>85</td>\n",
       "      <td>4.086500</td>\n",
       "      <td>4.471095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>86</td>\n",
       "      <td>3.949700</td>\n",
       "      <td>4.478845</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>87</td>\n",
       "      <td>4.249500</td>\n",
       "      <td>4.480251</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>88</td>\n",
       "      <td>3.771300</td>\n",
       "      <td>4.472573</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>89</td>\n",
       "      <td>4.270200</td>\n",
       "      <td>4.475432</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>4.366100</td>\n",
       "      <td>4.482276</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>91</td>\n",
       "      <td>4.401000</td>\n",
       "      <td>4.474898</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>92</td>\n",
       "      <td>3.878600</td>\n",
       "      <td>4.477289</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>93</td>\n",
       "      <td>4.430200</td>\n",
       "      <td>4.471597</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>94</td>\n",
       "      <td>4.792800</td>\n",
       "      <td>4.474296</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>95</td>\n",
       "      <td>4.089000</td>\n",
       "      <td>4.483580</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>96</td>\n",
       "      <td>4.579000</td>\n",
       "      <td>4.488667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>97</td>\n",
       "      <td>4.012300</td>\n",
       "      <td>4.493606</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>98</td>\n",
       "      <td>3.889100</td>\n",
       "      <td>4.485127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>99</td>\n",
       "      <td>4.468400</td>\n",
       "      <td>4.487455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>4.435300</td>\n",
       "      <td>4.477312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>101</td>\n",
       "      <td>4.013300</td>\n",
       "      <td>4.484485</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>102</td>\n",
       "      <td>4.263600</td>\n",
       "      <td>4.487853</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>103</td>\n",
       "      <td>3.911900</td>\n",
       "      <td>4.487287</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>104</td>\n",
       "      <td>3.584500</td>\n",
       "      <td>4.483848</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>105</td>\n",
       "      <td>4.309900</td>\n",
       "      <td>4.485907</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>106</td>\n",
       "      <td>4.030700</td>\n",
       "      <td>4.490429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>107</td>\n",
       "      <td>3.770400</td>\n",
       "      <td>4.488180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>108</td>\n",
       "      <td>3.822000</td>\n",
       "      <td>4.489100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>109</td>\n",
       "      <td>4.130700</td>\n",
       "      <td>4.486158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>3.944600</td>\n",
       "      <td>4.487184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>111</td>\n",
       "      <td>4.116900</td>\n",
       "      <td>4.488138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>112</td>\n",
       "      <td>4.047800</td>\n",
       "      <td>4.487249</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>113</td>\n",
       "      <td>3.100300</td>\n",
       "      <td>4.491439</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>114</td>\n",
       "      <td>4.309300</td>\n",
       "      <td>4.491558</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>115</td>\n",
       "      <td>4.349700</td>\n",
       "      <td>4.490764</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>116</td>\n",
       "      <td>4.055800</td>\n",
       "      <td>4.488394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>117</td>\n",
       "      <td>4.363100</td>\n",
       "      <td>4.489264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>118</td>\n",
       "      <td>3.854200</td>\n",
       "      <td>4.489378</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>119</td>\n",
       "      <td>4.267500</td>\n",
       "      <td>4.489017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>4.327100</td>\n",
       "      <td>4.488671</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <style>\n",
       "        .wandb-row {\n",
       "            display: flex;\n",
       "            flex-direction: row;\n",
       "            flex-wrap: wrap;\n",
       "            justify-content: flex-start;\n",
       "            width: 100%;\n",
       "        }\n",
       "        .wandb-col {\n",
       "            display: flex;\n",
       "            flex-direction: column;\n",
       "            flex-basis: 100%;\n",
       "            flex: 1;\n",
       "            padding: 10px;\n",
       "        }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>█▅▅▄▃▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▂▁▁▁▁▂▁▂▂▁▂▂▂▂▂▂▂▂▂</td></tr><tr><td>eval/runtime</td><td>▁▄▃▄▂▂▄▃▂▃▄▄▃▄▃▄▆▄▄▄▇▆▆▅▆▅▃▆▄█▅▃▃▆▄▃█▆█▅</td></tr><tr><td>eval/samples_per_second</td><td>▄▂█▄▅▄▅▅▆▇▆█▅▅▆▅▆▅▃▆▄▃▃▄▅▅▆▁▂▆▆▅▆▅▄▃▅▃▇▄</td></tr><tr><td>eval/steps_per_second</td><td>██▇▁▅▅▄▇▇▇▆▅█▅█▇▂▅▅▄▇▇▅▂█▄▃▅▆▄▅▁▅▄▆▇▇▁▃▄</td></tr><tr><td>train/epoch</td><td>▁▁▁▁▁▂▂▂▂▃▃▄▄▄▄▄▄▅▅▅▅▅▅▅▆▆▆▆▇▇▇▇▇▇▇█████</td></tr><tr><td>train/global_step</td><td>▁▁▁▁▁▂▂▂▂▂▃▃▃▄▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▆▇▇▇▇████</td></tr><tr><td>train/grad_norm</td><td>▃▁▂▁▂▁▄▆▁▃▃▁▂▁▃▂▃▂▂▅█▁▂▃▂▄▃▂▃▃▂▂▂▂▃▂▂▃▄▂</td></tr><tr><td>train/learning_rate</td><td>███▇▇▇▇▇▇▇▆▆▆▆▅▅▅▅▅▅▄▄▄▄▄▄▄▄▃▃▃▃▃▂▂▂▂▂▁▁</td></tr><tr><td>train/loss</td><td>█▄▃▅▄▅▅▄▄▅▃▃▃▃▃▄▂▃▃▄▂▄▄▃▃▃▁▃▃▄▃▃▃▃▃▃▂▃▃▃</td></tr><tr><td>train/total_flos</td><td>▁</td></tr><tr><td>train/train_loss</td><td>▁</td></tr><tr><td>train/train_runtime</td><td>▁</td></tr><tr><td>train/train_samples_per_second</td><td>▁</td></tr><tr><td>train/train_steps_per_second</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>4.48867</td></tr><tr><td>eval/runtime</td><td>2.2132</td></tr><tr><td>eval/samples_per_second</td><td>15.362</td></tr><tr><td>eval/steps_per_second</td><td>15.362</td></tr><tr><td>train/epoch</td><td>120</td></tr><tr><td>train/global_step</td><td>16320</td></tr><tr><td>train/grad_norm</td><td>2.24763</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>4.3271</td></tr><tr><td>train/total_flos</td><td>373794324480000.0</td></tr><tr><td>train/train_loss</td><td>4.36324</td></tr><tr><td>train/train_runtime</td><td>3595.8163</td></tr><tr><td>train/train_samples_per_second</td><td>4.539</td></tr><tr><td>train/train_steps_per_second</td><td>4.539</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">drums_training_run_gpt2_cross_10secsdataset_final</strong> at: <a href='https://wandb.ai/uniqlabs/music_generation/runs/9ndj83h1' target=\"_blank\">https://wandb.ai/uniqlabs/music_generation/runs/9ndj83h1</a><br/> View project at: <a href='https://wandb.ai/uniqlabs/music_generation' target=\"_blank\">https://wandb.ai/uniqlabs/music_generation</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20241128_224016-9ndj83h1\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for keys...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>C:\\Users\\Ripple\\Desktop\\5th sem\\practical work\\code\\wandb\\run-20241128_234018-pjkwuoo0</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/uniqlabs/music_generation/runs/pjkwuoo0' target=\"_blank\">keys_training_run_gpt2_cross_10secsdataset_final</a></strong> to <a href='https://wandb.ai/uniqlabs/music_generation' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/uniqlabs/music_generation' target=\"_blank\">https://wandb.ai/uniqlabs/music_generation</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/uniqlabs/music_generation/runs/pjkwuoo0' target=\"_blank\">https://wandb.ai/uniqlabs/music_generation/runs/pjkwuoo0</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ripple\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\accelerate\\accelerator.py:436: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='16320' max='16320' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [16320/16320 59:43, Epoch 120/120]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>5.388700</td>\n",
       "      <td>5.061367</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>4.728700</td>\n",
       "      <td>4.840999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>4.458400</td>\n",
       "      <td>4.711109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>4.948700</td>\n",
       "      <td>4.630985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>4.586400</td>\n",
       "      <td>4.552410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>3.882600</td>\n",
       "      <td>4.531574</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>3.802200</td>\n",
       "      <td>4.496453</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>4.282100</td>\n",
       "      <td>4.460728</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>4.273100</td>\n",
       "      <td>4.433827</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>4.752300</td>\n",
       "      <td>4.401883</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>4.102900</td>\n",
       "      <td>4.383900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>4.033900</td>\n",
       "      <td>4.379894</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>4.069600</td>\n",
       "      <td>4.366794</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>3.814800</td>\n",
       "      <td>4.344045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>4.070400</td>\n",
       "      <td>4.326970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>3.942500</td>\n",
       "      <td>4.329793</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>4.220700</td>\n",
       "      <td>4.316901</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>4.019800</td>\n",
       "      <td>4.324110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>3.775900</td>\n",
       "      <td>4.304491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>3.854200</td>\n",
       "      <td>4.289832</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>3.887600</td>\n",
       "      <td>4.307933</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>3.764200</td>\n",
       "      <td>4.283702</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>4.084600</td>\n",
       "      <td>4.278380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>3.662500</td>\n",
       "      <td>4.279634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>4.124600</td>\n",
       "      <td>4.290072</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>4.473500</td>\n",
       "      <td>4.286249</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>3.923600</td>\n",
       "      <td>4.280193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>3.796300</td>\n",
       "      <td>4.279234</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>3.932200</td>\n",
       "      <td>4.279958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>3.685500</td>\n",
       "      <td>4.270518</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31</td>\n",
       "      <td>3.895300</td>\n",
       "      <td>4.279913</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>3.655900</td>\n",
       "      <td>4.280198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33</td>\n",
       "      <td>3.741500</td>\n",
       "      <td>4.266715</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34</td>\n",
       "      <td>3.614400</td>\n",
       "      <td>4.272862</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>4.267300</td>\n",
       "      <td>4.268412</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36</td>\n",
       "      <td>3.989900</td>\n",
       "      <td>4.273556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37</td>\n",
       "      <td>3.747500</td>\n",
       "      <td>4.276068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38</td>\n",
       "      <td>3.560700</td>\n",
       "      <td>4.263146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39</td>\n",
       "      <td>3.618300</td>\n",
       "      <td>4.279536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>3.155200</td>\n",
       "      <td>4.271697</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41</td>\n",
       "      <td>3.675400</td>\n",
       "      <td>4.278492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42</td>\n",
       "      <td>3.632600</td>\n",
       "      <td>4.265748</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43</td>\n",
       "      <td>3.067000</td>\n",
       "      <td>4.286089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44</td>\n",
       "      <td>3.214600</td>\n",
       "      <td>4.278380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45</td>\n",
       "      <td>3.781700</td>\n",
       "      <td>4.291869</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46</td>\n",
       "      <td>3.917800</td>\n",
       "      <td>4.273132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47</td>\n",
       "      <td>3.989800</td>\n",
       "      <td>4.280419</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48</td>\n",
       "      <td>3.654500</td>\n",
       "      <td>4.270549</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49</td>\n",
       "      <td>3.819000</td>\n",
       "      <td>4.291475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>4.366300</td>\n",
       "      <td>4.291304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>51</td>\n",
       "      <td>3.856900</td>\n",
       "      <td>4.285846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>52</td>\n",
       "      <td>3.375000</td>\n",
       "      <td>4.298820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>53</td>\n",
       "      <td>3.462400</td>\n",
       "      <td>4.286298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>54</td>\n",
       "      <td>2.975700</td>\n",
       "      <td>4.294849</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>55</td>\n",
       "      <td>3.436400</td>\n",
       "      <td>4.305198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>56</td>\n",
       "      <td>3.648600</td>\n",
       "      <td>4.298808</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>57</td>\n",
       "      <td>3.066900</td>\n",
       "      <td>4.301334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>58</td>\n",
       "      <td>4.015300</td>\n",
       "      <td>4.290974</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>59</td>\n",
       "      <td>3.764500</td>\n",
       "      <td>4.295857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>3.949800</td>\n",
       "      <td>4.298820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>61</td>\n",
       "      <td>3.609600</td>\n",
       "      <td>4.311196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>62</td>\n",
       "      <td>3.491600</td>\n",
       "      <td>4.308090</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>63</td>\n",
       "      <td>3.695300</td>\n",
       "      <td>4.305393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>64</td>\n",
       "      <td>3.935900</td>\n",
       "      <td>4.307873</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>65</td>\n",
       "      <td>3.913600</td>\n",
       "      <td>4.306723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>66</td>\n",
       "      <td>3.493300</td>\n",
       "      <td>4.313023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>67</td>\n",
       "      <td>3.726200</td>\n",
       "      <td>4.305022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>68</td>\n",
       "      <td>3.725800</td>\n",
       "      <td>4.320286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>69</td>\n",
       "      <td>3.577800</td>\n",
       "      <td>4.326390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>3.372500</td>\n",
       "      <td>4.321406</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>71</td>\n",
       "      <td>3.674600</td>\n",
       "      <td>4.315274</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>72</td>\n",
       "      <td>3.871000</td>\n",
       "      <td>4.320422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>73</td>\n",
       "      <td>3.109700</td>\n",
       "      <td>4.320841</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>74</td>\n",
       "      <td>3.160800</td>\n",
       "      <td>4.326583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75</td>\n",
       "      <td>3.369200</td>\n",
       "      <td>4.319073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>76</td>\n",
       "      <td>3.353400</td>\n",
       "      <td>4.330573</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>77</td>\n",
       "      <td>3.868800</td>\n",
       "      <td>4.341522</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>78</td>\n",
       "      <td>2.977400</td>\n",
       "      <td>4.321344</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>79</td>\n",
       "      <td>3.621900</td>\n",
       "      <td>4.335327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>3.329700</td>\n",
       "      <td>4.338709</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>81</td>\n",
       "      <td>3.745600</td>\n",
       "      <td>4.339032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>82</td>\n",
       "      <td>4.031100</td>\n",
       "      <td>4.351716</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>83</td>\n",
       "      <td>3.714800</td>\n",
       "      <td>4.329838</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>84</td>\n",
       "      <td>3.928600</td>\n",
       "      <td>4.347378</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>85</td>\n",
       "      <td>3.707700</td>\n",
       "      <td>4.340255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>86</td>\n",
       "      <td>3.683200</td>\n",
       "      <td>4.347232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>87</td>\n",
       "      <td>3.208500</td>\n",
       "      <td>4.347655</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>88</td>\n",
       "      <td>3.808600</td>\n",
       "      <td>4.351862</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>89</td>\n",
       "      <td>3.322000</td>\n",
       "      <td>4.352820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>2.816300</td>\n",
       "      <td>4.345688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>91</td>\n",
       "      <td>3.335600</td>\n",
       "      <td>4.344081</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>92</td>\n",
       "      <td>3.194900</td>\n",
       "      <td>4.354057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>93</td>\n",
       "      <td>3.768700</td>\n",
       "      <td>4.352908</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>94</td>\n",
       "      <td>3.764300</td>\n",
       "      <td>4.359227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>95</td>\n",
       "      <td>3.645600</td>\n",
       "      <td>4.350638</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>96</td>\n",
       "      <td>3.124200</td>\n",
       "      <td>4.359798</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>97</td>\n",
       "      <td>3.631300</td>\n",
       "      <td>4.361117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>98</td>\n",
       "      <td>3.242700</td>\n",
       "      <td>4.357153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>99</td>\n",
       "      <td>3.198000</td>\n",
       "      <td>4.354591</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>2.381400</td>\n",
       "      <td>4.359389</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>101</td>\n",
       "      <td>3.348400</td>\n",
       "      <td>4.360098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>102</td>\n",
       "      <td>3.826000</td>\n",
       "      <td>4.359967</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>103</td>\n",
       "      <td>3.556300</td>\n",
       "      <td>4.370903</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>104</td>\n",
       "      <td>3.392200</td>\n",
       "      <td>4.367692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>105</td>\n",
       "      <td>2.563200</td>\n",
       "      <td>4.371227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>106</td>\n",
       "      <td>3.566100</td>\n",
       "      <td>4.380568</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>107</td>\n",
       "      <td>3.542100</td>\n",
       "      <td>4.368793</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>108</td>\n",
       "      <td>3.471300</td>\n",
       "      <td>4.375941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>109</td>\n",
       "      <td>3.284900</td>\n",
       "      <td>4.374269</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>3.319600</td>\n",
       "      <td>4.375999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>111</td>\n",
       "      <td>2.547000</td>\n",
       "      <td>4.374613</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>112</td>\n",
       "      <td>3.462300</td>\n",
       "      <td>4.371259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>113</td>\n",
       "      <td>3.558100</td>\n",
       "      <td>4.373038</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>114</td>\n",
       "      <td>3.692500</td>\n",
       "      <td>4.377460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>115</td>\n",
       "      <td>3.733200</td>\n",
       "      <td>4.377873</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>116</td>\n",
       "      <td>3.346400</td>\n",
       "      <td>4.378436</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>117</td>\n",
       "      <td>3.309100</td>\n",
       "      <td>4.376793</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>118</td>\n",
       "      <td>3.581700</td>\n",
       "      <td>4.376495</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>119</td>\n",
       "      <td>3.074000</td>\n",
       "      <td>4.376774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>2.984600</td>\n",
       "      <td>4.377293</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <style>\n",
       "        .wandb-row {\n",
       "            display: flex;\n",
       "            flex-direction: row;\n",
       "            flex-wrap: wrap;\n",
       "            justify-content: flex-start;\n",
       "            width: 100%;\n",
       "        }\n",
       "        .wandb-col {\n",
       "            display: flex;\n",
       "            flex-direction: column;\n",
       "            flex-basis: 100%;\n",
       "            flex: 1;\n",
       "            padding: 10px;\n",
       "        }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>█▅▂▂▂▂▂▁▂▁▁▁▁▁▁▁▁▁▁▁▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂</td></tr><tr><td>eval/runtime</td><td>▃▂▁▁▁▁▁▁▁▃▂▂▂▃▃▄▄▃▃▄▄▄█▂▂▂▃▂▃▃▁▂▂▁▂▂▁▂▂▂</td></tr><tr><td>eval/samples_per_second</td><td>█▇▇▃█████▁██▇▇▇▆▆▆▆▆▆▆▆▆▅▇▇▇█▇▇▇█▇█▇▇██▇</td></tr><tr><td>eval/steps_per_second</td><td>▇▅▆▅▇▅█▇███▇█▇▆▆▅▅▅▄▃▃▃▃▁▁▄▂▆▆▅▆▆▆▇▇▇▇▆▇</td></tr><tr><td>train/epoch</td><td>▁▁▁▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▆▆▇▇▇▇▇██</td></tr><tr><td>train/global_step</td><td>▁▁▁▁▁▂▂▂▃▃▃▃▃▃▃▄▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇█████</td></tr><tr><td>train/grad_norm</td><td>▅▁▅▂▁▁▇▆▄▂▃▃▄▃▇▃▃▃▃▅▃▄▄▄▄▅▄█▃▄▃▄▆▃▆▄▅▅▂▃</td></tr><tr><td>train/learning_rate</td><td>█████▇▇▇▇▇▇▇▆▆▆▆▆▅▅▅▅▄▄▄▄▄▃▃▃▃▃▂▂▂▂▂▂▁▁▁</td></tr><tr><td>train/loss</td><td>█▆▅▅▄▃▄▃▃▄▃▃▄▄▃▃▁▃▂▄▂▃▂▂▃▃▂▁▂▂▂▃▂▂▂▁▂▁▂▃</td></tr><tr><td>train/total_flos</td><td>▁</td></tr><tr><td>train/train_loss</td><td>▁</td></tr><tr><td>train/train_runtime</td><td>▁</td></tr><tr><td>train/train_samples_per_second</td><td>▁</td></tr><tr><td>train/train_steps_per_second</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>4.37729</td></tr><tr><td>eval/runtime</td><td>2.2107</td></tr><tr><td>eval/samples_per_second</td><td>15.38</td></tr><tr><td>eval/steps_per_second</td><td>15.38</td></tr><tr><td>train/epoch</td><td>120</td></tr><tr><td>train/global_step</td><td>16320</td></tr><tr><td>train/grad_norm</td><td>2.97969</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>2.9846</td></tr><tr><td>train/total_flos</td><td>373794324480000.0</td></tr><tr><td>train/train_loss</td><td>3.68268</td></tr><tr><td>train/train_runtime</td><td>3584.525</td></tr><tr><td>train/train_samples_per_second</td><td>4.553</td></tr><tr><td>train/train_steps_per_second</td><td>4.553</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">keys_training_run_gpt2_cross_10secsdataset_final</strong> at: <a href='https://wandb.ai/uniqlabs/music_generation/runs/pjkwuoo0' target=\"_blank\">https://wandb.ai/uniqlabs/music_generation/runs/pjkwuoo0</a><br/> View project at: <a href='https://wandb.ai/uniqlabs/music_generation' target=\"_blank\">https://wandb.ai/uniqlabs/music_generation</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20241128_234018-pjkwuoo0\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for full_instrumental...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>C:\\Users\\Ripple\\Desktop\\5th sem\\practical work\\code\\wandb\\run-20241129_004006-h5ajyrjr</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/uniqlabs/music_generation/runs/h5ajyrjr' target=\"_blank\">full_instrumental_training_run_gpt2_cross_10secsdataset_final</a></strong> to <a href='https://wandb.ai/uniqlabs/music_generation' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/uniqlabs/music_generation' target=\"_blank\">https://wandb.ai/uniqlabs/music_generation</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/uniqlabs/music_generation/runs/h5ajyrjr' target=\"_blank\">https://wandb.ai/uniqlabs/music_generation/runs/h5ajyrjr</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ripple\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\accelerate\\accelerator.py:436: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='16320' max='16320' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [16320/16320 59:29, Epoch 120/120]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>6.148900</td>\n",
       "      <td>5.814071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>5.768200</td>\n",
       "      <td>5.651714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>5.564300</td>\n",
       "      <td>5.581490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>5.774700</td>\n",
       "      <td>5.539820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>5.563500</td>\n",
       "      <td>5.519042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>5.518800</td>\n",
       "      <td>5.506169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>5.446400</td>\n",
       "      <td>5.509966</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>5.467000</td>\n",
       "      <td>5.471783</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>5.531900</td>\n",
       "      <td>5.458737</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>5.785600</td>\n",
       "      <td>5.449085</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>5.379900</td>\n",
       "      <td>5.436927</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>4.959600</td>\n",
       "      <td>5.440443</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>5.384000</td>\n",
       "      <td>5.431747</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>4.917800</td>\n",
       "      <td>5.421088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>5.434000</td>\n",
       "      <td>5.418057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>5.023400</td>\n",
       "      <td>5.418813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>5.393300</td>\n",
       "      <td>5.411476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>5.169800</td>\n",
       "      <td>5.398131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>4.907100</td>\n",
       "      <td>5.397315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>4.872600</td>\n",
       "      <td>5.404374</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>5.307500</td>\n",
       "      <td>5.406814</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>5.027700</td>\n",
       "      <td>5.384699</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>5.174100</td>\n",
       "      <td>5.412378</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>5.288700</td>\n",
       "      <td>5.405702</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>5.211600</td>\n",
       "      <td>5.400624</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>5.225500</td>\n",
       "      <td>5.398648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>5.201700</td>\n",
       "      <td>5.400169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>5.041000</td>\n",
       "      <td>5.399811</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>5.164700</td>\n",
       "      <td>5.403050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>5.182600</td>\n",
       "      <td>5.404782</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31</td>\n",
       "      <td>4.957200</td>\n",
       "      <td>5.401176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>5.092000</td>\n",
       "      <td>5.384768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33</td>\n",
       "      <td>4.969600</td>\n",
       "      <td>5.399356</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34</td>\n",
       "      <td>4.819200</td>\n",
       "      <td>5.394269</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>5.148800</td>\n",
       "      <td>5.422573</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36</td>\n",
       "      <td>5.117100</td>\n",
       "      <td>5.395539</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37</td>\n",
       "      <td>5.205100</td>\n",
       "      <td>5.394615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38</td>\n",
       "      <td>4.993800</td>\n",
       "      <td>5.403602</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39</td>\n",
       "      <td>4.837200</td>\n",
       "      <td>5.409412</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>4.999500</td>\n",
       "      <td>5.396982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41</td>\n",
       "      <td>4.692700</td>\n",
       "      <td>5.424014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42</td>\n",
       "      <td>4.763300</td>\n",
       "      <td>5.418097</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43</td>\n",
       "      <td>4.947300</td>\n",
       "      <td>5.405341</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44</td>\n",
       "      <td>4.471300</td>\n",
       "      <td>5.402907</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45</td>\n",
       "      <td>4.765400</td>\n",
       "      <td>5.425292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46</td>\n",
       "      <td>4.979600</td>\n",
       "      <td>5.423495</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47</td>\n",
       "      <td>4.953200</td>\n",
       "      <td>5.420939</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48</td>\n",
       "      <td>4.748200</td>\n",
       "      <td>5.415564</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49</td>\n",
       "      <td>4.868300</td>\n",
       "      <td>5.405180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>5.102800</td>\n",
       "      <td>5.420580</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>51</td>\n",
       "      <td>4.952300</td>\n",
       "      <td>5.420184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>52</td>\n",
       "      <td>4.927600</td>\n",
       "      <td>5.424850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>53</td>\n",
       "      <td>4.448500</td>\n",
       "      <td>5.433595</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>54</td>\n",
       "      <td>4.801400</td>\n",
       "      <td>5.430172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>55</td>\n",
       "      <td>4.812100</td>\n",
       "      <td>5.439442</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>56</td>\n",
       "      <td>4.846900</td>\n",
       "      <td>5.421505</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>57</td>\n",
       "      <td>4.488000</td>\n",
       "      <td>5.424349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>58</td>\n",
       "      <td>4.833400</td>\n",
       "      <td>5.438362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>59</td>\n",
       "      <td>4.772000</td>\n",
       "      <td>5.445040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>4.741400</td>\n",
       "      <td>5.432783</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>61</td>\n",
       "      <td>4.837300</td>\n",
       "      <td>5.439233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>62</td>\n",
       "      <td>4.671200</td>\n",
       "      <td>5.450315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>63</td>\n",
       "      <td>4.578500</td>\n",
       "      <td>5.433370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>64</td>\n",
       "      <td>4.804400</td>\n",
       "      <td>5.453332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>65</td>\n",
       "      <td>4.592200</td>\n",
       "      <td>5.442899</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>66</td>\n",
       "      <td>4.813600</td>\n",
       "      <td>5.463628</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>67</td>\n",
       "      <td>4.851600</td>\n",
       "      <td>5.447744</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>68</td>\n",
       "      <td>4.889400</td>\n",
       "      <td>5.464723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>69</td>\n",
       "      <td>4.930900</td>\n",
       "      <td>5.450753</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>4.660600</td>\n",
       "      <td>5.462471</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>71</td>\n",
       "      <td>4.620600</td>\n",
       "      <td>5.454356</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>72</td>\n",
       "      <td>4.839800</td>\n",
       "      <td>5.459680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>73</td>\n",
       "      <td>4.633300</td>\n",
       "      <td>5.469768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>74</td>\n",
       "      <td>4.404000</td>\n",
       "      <td>5.465649</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75</td>\n",
       "      <td>4.662700</td>\n",
       "      <td>5.452326</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>76</td>\n",
       "      <td>4.808300</td>\n",
       "      <td>5.454078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>77</td>\n",
       "      <td>4.588400</td>\n",
       "      <td>5.462278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>78</td>\n",
       "      <td>4.410800</td>\n",
       "      <td>5.475766</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>79</td>\n",
       "      <td>4.862900</td>\n",
       "      <td>5.463058</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>4.502100</td>\n",
       "      <td>5.470795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>81</td>\n",
       "      <td>4.680800</td>\n",
       "      <td>5.473306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>82</td>\n",
       "      <td>4.839500</td>\n",
       "      <td>5.469765</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>83</td>\n",
       "      <td>4.940900</td>\n",
       "      <td>5.473052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>84</td>\n",
       "      <td>5.032400</td>\n",
       "      <td>5.472371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>85</td>\n",
       "      <td>4.667700</td>\n",
       "      <td>5.480755</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>86</td>\n",
       "      <td>4.663400</td>\n",
       "      <td>5.480610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>87</td>\n",
       "      <td>4.667700</td>\n",
       "      <td>5.486384</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>88</td>\n",
       "      <td>4.654800</td>\n",
       "      <td>5.489486</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>89</td>\n",
       "      <td>4.673600</td>\n",
       "      <td>5.486545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>4.738900</td>\n",
       "      <td>5.487696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>91</td>\n",
       "      <td>4.697300</td>\n",
       "      <td>5.483903</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>92</td>\n",
       "      <td>4.485500</td>\n",
       "      <td>5.495468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>93</td>\n",
       "      <td>4.788600</td>\n",
       "      <td>5.493735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>94</td>\n",
       "      <td>5.081500</td>\n",
       "      <td>5.485173</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>95</td>\n",
       "      <td>4.643000</td>\n",
       "      <td>5.493846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>96</td>\n",
       "      <td>4.896400</td>\n",
       "      <td>5.497425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>97</td>\n",
       "      <td>4.625700</td>\n",
       "      <td>5.496418</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>98</td>\n",
       "      <td>4.435500</td>\n",
       "      <td>5.494500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>99</td>\n",
       "      <td>4.733800</td>\n",
       "      <td>5.498459</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>4.675000</td>\n",
       "      <td>5.498753</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>101</td>\n",
       "      <td>4.381500</td>\n",
       "      <td>5.496493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>102</td>\n",
       "      <td>4.809300</td>\n",
       "      <td>5.497059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>103</td>\n",
       "      <td>4.604800</td>\n",
       "      <td>5.500155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>104</td>\n",
       "      <td>4.227800</td>\n",
       "      <td>5.496416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>105</td>\n",
       "      <td>4.596100</td>\n",
       "      <td>5.505097</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>106</td>\n",
       "      <td>4.573900</td>\n",
       "      <td>5.507053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>107</td>\n",
       "      <td>4.453000</td>\n",
       "      <td>5.499843</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>108</td>\n",
       "      <td>4.430500</td>\n",
       "      <td>5.501550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>109</td>\n",
       "      <td>4.659300</td>\n",
       "      <td>5.504323</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>4.416500</td>\n",
       "      <td>5.501720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>111</td>\n",
       "      <td>4.406300</td>\n",
       "      <td>5.505222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>112</td>\n",
       "      <td>4.490500</td>\n",
       "      <td>5.502540</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>113</td>\n",
       "      <td>4.169800</td>\n",
       "      <td>5.503827</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>114</td>\n",
       "      <td>4.832100</td>\n",
       "      <td>5.504635</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>115</td>\n",
       "      <td>4.753400</td>\n",
       "      <td>5.508057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>116</td>\n",
       "      <td>4.528200</td>\n",
       "      <td>5.507278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>117</td>\n",
       "      <td>4.729200</td>\n",
       "      <td>5.506270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>118</td>\n",
       "      <td>4.392200</td>\n",
       "      <td>5.507415</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>119</td>\n",
       "      <td>4.610900</td>\n",
       "      <td>5.505223</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>4.705400</td>\n",
       "      <td>5.505191</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <style>\n",
       "        .wandb-row {\n",
       "            display: flex;\n",
       "            flex-direction: row;\n",
       "            flex-wrap: wrap;\n",
       "            justify-content: flex-start;\n",
       "            width: 100%;\n",
       "        }\n",
       "        .wandb-col {\n",
       "            display: flex;\n",
       "            flex-direction: column;\n",
       "            flex-basis: 100%;\n",
       "            flex: 1;\n",
       "            padding: 10px;\n",
       "        }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>█▄▄▃▃▂▂▂▁▁▁▁▁▁▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▃▃▃▃▃▃▃▃▃▃</td></tr><tr><td>eval/runtime</td><td>▁▂▁▂▅▅▄▄▄▆▅▅▆▅▅▅▃▇▅▇▃█▅▆▆▅▇▅▆▅▄▆▇█▄▇▆▅▆▄</td></tr><tr><td>eval/samples_per_second</td><td>█▃▇▄▆▅▁▇▄▃▇▄▅▅▆▅▄▂▃▆▄▃▄▅▃▁▂▆▄▃▄▂▃▃▄▃▅▇▃▅</td></tr><tr><td>eval/steps_per_second</td><td>▇▄█▇▅▅▅▄▅▄▅▄▅▃▅▄▅▆▄▅▅▃▅▅▃▅▄▃▂▃▅▄▅▄▄▅▁▄▃▄</td></tr><tr><td>train/epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▇▇▇▇████</td></tr><tr><td>train/global_step</td><td>▁▂▂▂▂▂▂▂▂▂▃▃▃▃▄▄▄▄▄▄▅▅▅▅▆▆▆▆▆▆▆▇▇▇▇█████</td></tr><tr><td>train/grad_norm</td><td>▂█▁▁▂▁▄▁▁▁▁▁▁▄▂▃▃▂▁▂▂▂▂▂▂▃▂▂▂▃▂▂▃▂▃▂▂▂▂▂</td></tr><tr><td>train/learning_rate</td><td>████▇▆▆▆▆▆▆▆▅▅▅▅▅▅▅▄▄▄▄▄▄▄▃▃▃▃▃▂▂▂▂▁▁▁▁▁</td></tr><tr><td>train/loss</td><td>█▆▇▇▇▆▄▅▇▆▅▄▅▆▄▆▄▄▅▃▄▄▃▄▄▄▃▃▃▃▃▁▄▃▄▄▃▂▅▃</td></tr><tr><td>train/total_flos</td><td>▁</td></tr><tr><td>train/train_loss</td><td>▁</td></tr><tr><td>train/train_runtime</td><td>▁</td></tr><tr><td>train/train_samples_per_second</td><td>▁</td></tr><tr><td>train/train_steps_per_second</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>5.50519</td></tr><tr><td>eval/runtime</td><td>2.2084</td></tr><tr><td>eval/samples_per_second</td><td>15.396</td></tr><tr><td>eval/steps_per_second</td><td>15.396</td></tr><tr><td>train/epoch</td><td>120</td></tr><tr><td>train/global_step</td><td>16320</td></tr><tr><td>train/grad_norm</td><td>2.6315</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>4.7054</td></tr><tr><td>train/total_flos</td><td>373794324480000.0</td></tr><tr><td>train/train_loss</td><td>4.87225</td></tr><tr><td>train/train_runtime</td><td>3570.0138</td></tr><tr><td>train/train_samples_per_second</td><td>4.571</td></tr><tr><td>train/train_steps_per_second</td><td>4.571</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">full_instrumental_training_run_gpt2_cross_10secsdataset_final</strong> at: <a href='https://wandb.ai/uniqlabs/music_generation/runs/h5ajyrjr' target=\"_blank\">https://wandb.ai/uniqlabs/music_generation/runs/h5ajyrjr</a><br/> View project at: <a href='https://wandb.ai/uniqlabs/music_generation' target=\"_blank\">https://wandb.ai/uniqlabs/music_generation</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20241129_004006-h5ajyrjr\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training completed for all tracks.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset\n",
    "from transformers import (\n",
    "    GPT2LMHeadModel,\n",
    "    GPT2Config,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    ")\n",
    "import torch.nn as nn\n",
    "from sklearn.model_selection import train_test_split\n",
    "import wandb\n",
    "\n",
    "# Set the device to CUDA if available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Load your saved .npy file\n",
    "data = np.load(\n",
    "    'fulldataset_10sec_positional_embs.npy',\n",
    "    allow_pickle=True\n",
    ").item()\n",
    "\n",
    "VOCAB_SIZE = 1024\n",
    "MAX_LENGTH = 3000\n",
    "track_classes = ['hi_hat', 'kick', 'snare', 'clap', 'bass', 'drums', 'keys', 'full_instrumental']\n",
    "\n",
    "class MusicDataset(Dataset):\n",
    "    def __init__(self, data, track_class):\n",
    "        self.track_class = track_class\n",
    "        self.data = {\n",
    "            k: v for k, v in data.items()\n",
    "            if self.track_class in v['generation_data']\n",
    "        }\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample_id = list(self.data.keys())[idx]\n",
    "        sample = self.data[sample_id]\n",
    "\n",
    "        vocal_audio_codes = sample['generation_data'].get(\n",
    "            'vocal', np.zeros((4, 750))\n",
    "        )\n",
    "        track_data = sample['generation_data'].get(\n",
    "            self.track_class, np.zeros((4, 750))\n",
    "        )\n",
    "        positional_embedding = sample.get(\n",
    "            'positional_embedding', np.zeros((4, 750))\n",
    "        )\n",
    "\n",
    "        # Clip values to valid range\n",
    "        vocal_audio_codes = np.clip(vocal_audio_codes, 0, VOCAB_SIZE - 1)\n",
    "        track_data = np.clip(track_data, 0, VOCAB_SIZE - 1)\n",
    "\n",
    "        # Pad and truncate sequences to MAX_LENGTH\n",
    "        vocal_audio_codes = np.pad(\n",
    "            vocal_audio_codes.flatten(),\n",
    "            (0, MAX_LENGTH - len(vocal_audio_codes.flatten())),\n",
    "            'constant',\n",
    "            constant_values=(0, 0)\n",
    "        )[:MAX_LENGTH]\n",
    "        track_data = np.pad(\n",
    "            track_data.flatten(),\n",
    "            (0, MAX_LENGTH - len(track_data.flatten())),\n",
    "            'constant',\n",
    "            constant_values=(0, 0)\n",
    "        )[:MAX_LENGTH]\n",
    "\n",
    "        attention_mask = (vocal_audio_codes != 0).astype(int)\n",
    "\n",
    "        # Flatten and pad positional embeddings\n",
    "        pos_emb_flat = positional_embedding.flatten()\n",
    "        pos_emb_flat = np.pad(\n",
    "            pos_emb_flat,\n",
    "            (0, MAX_LENGTH * positional_embedding.shape[1] - len(pos_emb_flat)),\n",
    "            'constant',\n",
    "            constant_values=(0, 0)\n",
    "        )[:MAX_LENGTH * positional_embedding.shape[1]]\n",
    "\n",
    "        positional_embedding = pos_emb_flat.reshape(MAX_LENGTH, positional_embedding.shape[1])\n",
    "\n",
    "        return {\n",
    "            'input_ids': torch.tensor(vocal_audio_codes, dtype=torch.long),\n",
    "            'attention_mask': torch.tensor(attention_mask, dtype=torch.long),\n",
    "            'labels': torch.tensor(track_data, dtype=torch.long),\n",
    "            'positional_embeddings': torch.tensor(positional_embedding, dtype=torch.float),\n",
    "            'conditioning_inputs': torch.tensor(track_data, dtype=torch.long),  # Track data as conditioner\n",
    "            'sample_id': sample_id\n",
    "        }\n",
    "\n",
    "class CustomGPT2WithCrossAttention(GPT2LMHeadModel):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.cross_attention_layer = nn.MultiheadAttention(\n",
    "            embed_dim=config.n_embd,\n",
    "            num_heads=config.n_head,\n",
    "            dropout=config.attn_pdrop\n",
    "        )\n",
    "        self.fc_layer = nn.Linear(config.n_embd, config.n_embd)\n",
    "\n",
    "    def forward(self, input_ids=None, attention_mask=None, labels=None,\n",
    "                positional_embeddings=None, conditioning_inputs=None, **kwargs):\n",
    "        # Get input embeddings\n",
    "        input_embeds = self.transformer.wte(input_ids)\n",
    "\n",
    "        # Combine positional embeddings with input embeddings\n",
    "        embs_dim = positional_embeddings.shape[2]\n",
    "        input_embeds = torch.cat((input_embeds[:, :, :-embs_dim], input_embeds[:, :, -embs_dim:] + positional_embeddings), dim=-1)\n",
    "\n",
    "\n",
    "        # Apply self-attention within the model\n",
    "        transformer_outputs = self.transformer(\n",
    "            inputs_embeds=input_embeds,\n",
    "            attention_mask=attention_mask,\n",
    "            output_attentions=False,\n",
    "            output_hidden_states=True,\n",
    "        )\n",
    "\n",
    "        hidden_states = transformer_outputs.last_hidden_state\n",
    "\n",
    "        # Apply cross-attention with conditioning inputs (track data)\n",
    "        if conditioning_inputs is not None:\n",
    "            conditioning_mask = (conditioning_inputs != 0).float()  # Create attention mask\n",
    "            conditioning_inputs_embeds = self.transformer.wte(conditioning_inputs)\n",
    "\n",
    "            # Cross-attention mechanism\n",
    "            cross_attention_output, _ = self.cross_attention_layer(\n",
    "                query=hidden_states.permute(1, 0, 2),  # (seq_len, batch_size, embed_dim)\n",
    "                key=conditioning_inputs_embeds.permute(1, 0, 2),  # (seq_len, batch_size, embed_dim)\n",
    "                value=conditioning_inputs_embeds.permute(1, 0, 2),  # (seq_len, batch_size, embed_dim)\n",
    "                key_padding_mask=~conditioning_mask.bool()  # Use `~` to invert the boolean mask\n",
    "            )\n",
    "            cross_attention_output = cross_attention_output.permute(1, 0, 2)  # Back to (batch_size, seq_len, embed_dim)\n",
    "\n",
    "            # Add cross-attention output to hidden states\n",
    "            hidden_states = hidden_states + self.fc_layer(cross_attention_output)\n",
    "\n",
    "        # Pass through the language model head for logits\n",
    "        logits = self.lm_head(hidden_states)\n",
    "\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            shift_logits = logits[..., :-1, :].contiguous()\n",
    "            shift_labels = labels[..., 1:].contiguous()\n",
    "            loss_fct = nn.CrossEntropyLoss()\n",
    "            loss = loss_fct(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))\n",
    "\n",
    "        return {\"loss\": loss, \"logits\": logits}\n",
    "\n",
    "class DataCollatorWithPositionalEmbeddings:\n",
    "    def __call__(self, batch):\n",
    "        input_ids = torch.stack([item['input_ids'] for item in batch])\n",
    "        attention_mask = torch.stack([item['attention_mask'] for item in batch])\n",
    "        labels = torch.stack([item['labels'] for item in batch])\n",
    "        positional_embeddings = torch.stack([item['positional_embeddings'] for item in batch])\n",
    "        conditioning_inputs = torch.stack([item['conditioning_inputs'] for item in batch])\n",
    "\n",
    "        return {\n",
    "            'input_ids': input_ids,\n",
    "            'attention_mask': attention_mask,\n",
    "            'labels': labels,\n",
    "            'positional_embeddings': positional_embeddings,\n",
    "            'conditioning_inputs': conditioning_inputs\n",
    "        }\n",
    "\n",
    "config = GPT2Config(\n",
    "    vocab_size=VOCAB_SIZE,\n",
    "    n_positions=MAX_LENGTH,\n",
    "    n_ctx=MAX_LENGTH,\n",
    "    n_embd=128,\n",
    "    n_layer=6,\n",
    "    n_head=8,\n",
    "    activation_function='gelu',\n",
    "    resid_pdrop=0.1,\n",
    "    embd_pdrop=0.1,\n",
    "    attn_pdrop=0.1,\n",
    ")\n",
    "\n",
    "model = CustomGPT2WithCrossAttention(config=config).to(device)\n",
    "data_collator = DataCollatorWithPositionalEmbeddings()\n",
    "\n",
    "for track_idx, track in enumerate(track_classes):\n",
    "    print(f\"Training for {track}...\")\n",
    "\n",
    "    dataset = MusicDataset(data, track)\n",
    "    train_indices, val_indices = train_test_split(\n",
    "        range(len(dataset)), test_size=0.2, random_state=42\n",
    "    )\n",
    "    train_dataset = torch.utils.data.Subset(dataset, train_indices)\n",
    "    val_dataset = torch.utils.data.Subset(dataset, val_indices)\n",
    "\n",
    "    track_output_dir = f'./independent_track_generation_gpt2_checkpointing_{track}_model_cross_attention_10secsdataset_final'\n",
    "\n",
    "    wandb.init(project=\"music_generation\", name=f'{track}_training_run_gpt2_cross_10secsdataset_final')\n",
    "\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=track_output_dir,\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        learning_rate=1e-4,\n",
    "        per_device_train_batch_size=1,\n",
    "        per_device_eval_batch_size=1,\n",
    "        num_train_epochs=120,\n",
    "        weight_decay=0.01,\n",
    "        save_total_limit=3,\n",
    "        logging_dir=f'./logs_{track}',\n",
    "        fp16=True,\n",
    "        report_to=['wandb'],\n",
    "        dataloader_pin_memory=False,\n",
    "        logging_steps=10,\n",
    "    )\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        data_collator=data_collator,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=val_dataset\n",
    "    )\n",
    "\n",
    "    trainer.train()\n",
    "    trainer.save_model(track_output_dir)\n",
    "    wandb.finish()\n",
    "\n",
    "print(\"Training completed for all tracks.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c1c5eb0",
   "metadata": {},
   "source": [
    "# Inference\n",
    "This script performs inference for music track generation using pre-trained GPT-2 models that were originally trained with cross-attention mechanisms and conditioning inputs for specific stems. However, during inference, the conditioning inputs are excluded due to the availability of only vocal encodings and reference beat and rhythm features. The model definition has been adjusted to omit the cross-attention layer, relying solely on encoded 10-second audio segments and positional embeddings derived from beat and downbeat information detected via Madmom.\n",
    "\n",
    "Despite the lack of conditioning inputs during inference, the model demonstrates its ability to generate musical patterns for various track classes, maintaining coherence and some structure. The positional embeddings are integrated into the input embeddings during the forward pass, enabling temporal consistency. The generated sequences are reshaped, clamped to valid ranges, and decoded back into audio using Facebook's EnCodec, producing musically plausible outputs for each track class. This highlights the model's robustness and adaptability in generating music even under altered inference conditions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "60df1f7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ripple\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\huggingface_hub\\file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration. Please open a PR/issue to update `preprocessor_config.json` to use `image_processor_type` instead of `feature_extractor_type`. This warning will be removed in v4.40.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing inference.wav -> hi_hat in folder 1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ripple\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\madmom\\io\\audio.py:493: WavFileWarning: Chunk (non-data) not understood, skipping it.\n",
      "  file_sample_rate, signal = wavfile.read(filename, mmap=True)\n",
      "C:\\Users\\Ripple\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\madmom\\features\\downbeats.py:287: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  best = np.argmax(np.asarray(results)[:, 1])\n",
      "C:\\Users\\Ripple\\AppData\\Local\\Temp\\ipykernel_14872\\66958963.py:193: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  generated_sequence = generate_track(model, torch.tensor(audio_codes), positional_embeddings, attention_mask)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reshaped Output: torch.Size([1, 1, 4, 750]), Device: cpu\n",
      "Max value in reshaped_output: 1017\n",
      "Min value in reshaped_output: 25\n",
      "Contains NaN: False\n",
      "Contains Inf: False\n",
      "Saved: ././independent_track_generation_gpt2_checkpointing_hi_hat_model_cross_attention_10secsdataset_final/1/hi_hat_generated.wav\n",
      "Processing inference.wav -> kick in folder 1...\n",
      "Reshaped Output: torch.Size([1, 1, 4, 750]), Device: cpu\n",
      "Max value in reshaped_output: 1022\n",
      "Min value in reshaped_output: 5\n",
      "Contains NaN: False\n",
      "Contains Inf: False\n",
      "Saved: ././independent_track_generation_gpt2_checkpointing_kick_model_cross_attention_10secsdataset_final/1/kick_generated.wav\n",
      "Processing inference.wav -> snare in folder 1...\n",
      "Reshaped Output: torch.Size([1, 1, 4, 750]), Device: cpu\n",
      "Max value in reshaped_output: 1023\n",
      "Min value in reshaped_output: 23\n",
      "Contains NaN: False\n",
      "Contains Inf: False\n",
      "Saved: ././independent_track_generation_gpt2_checkpointing_snare_model_cross_attention_10secsdataset_final/1/snare_generated.wav\n",
      "Processing inference.wav -> clap in folder 1...\n",
      "Reshaped Output: torch.Size([1, 1, 4, 750]), Device: cpu\n",
      "Max value in reshaped_output: 1023\n",
      "Min value in reshaped_output: 16\n",
      "Contains NaN: False\n",
      "Contains Inf: False\n",
      "Saved: ././independent_track_generation_gpt2_checkpointing_clap_model_cross_attention_10secsdataset_final/1/clap_generated.wav\n",
      "Processing inference.wav -> bass in folder 1...\n",
      "Reshaped Output: torch.Size([1, 1, 4, 750]), Device: cpu\n",
      "Max value in reshaped_output: 1022\n",
      "Min value in reshaped_output: 5\n",
      "Contains NaN: False\n",
      "Contains Inf: False\n",
      "Saved: ././independent_track_generation_gpt2_checkpointing_bass_model_cross_attention_10secsdataset_final/1/bass_generated.wav\n",
      "Processing inference.wav -> drums in folder 1...\n",
      "Reshaped Output: torch.Size([1, 1, 4, 750]), Device: cpu\n",
      "Max value in reshaped_output: 1023\n",
      "Min value in reshaped_output: 7\n",
      "Contains NaN: False\n",
      "Contains Inf: False\n",
      "Saved: ././independent_track_generation_gpt2_checkpointing_drums_model_cross_attention_10secsdataset_final/1/drums_generated.wav\n",
      "Processing inference.wav -> keys in folder 1...\n",
      "Reshaped Output: torch.Size([1, 1, 4, 750]), Device: cpu\n",
      "Max value in reshaped_output: 1022\n",
      "Min value in reshaped_output: 6\n",
      "Contains NaN: False\n",
      "Contains Inf: False\n",
      "Saved: ././independent_track_generation_gpt2_checkpointing_keys_model_cross_attention_10secsdataset_final/1/keys_generated.wav\n",
      "Processing inference.wav -> full_instrumental in folder 1...\n",
      "Reshaped Output: torch.Size([1, 1, 4, 750]), Device: cpu\n",
      "Max value in reshaped_output: 1020\n",
      "Min value in reshaped_output: 1\n",
      "Contains NaN: False\n",
      "Contains Inf: False\n",
      "Saved: ././independent_track_generation_gpt2_checkpointing_full_instrumental_model_cross_attention_10secsdataset_final/1/full_instrumental_generated.wav\n",
      "Processing inference2.wav -> hi_hat in folder 2...\n",
      "Reshaped Output: torch.Size([1, 1, 4, 750]), Device: cpu\n",
      "Max value in reshaped_output: 1017\n",
      "Min value in reshaped_output: 36\n",
      "Contains NaN: False\n",
      "Contains Inf: False\n",
      "Saved: ././independent_track_generation_gpt2_checkpointing_hi_hat_model_cross_attention_10secsdataset_final/2/hi_hat_generated.wav\n",
      "Processing inference2.wav -> kick in folder 2...\n",
      "Reshaped Output: torch.Size([1, 1, 4, 750]), Device: cpu\n",
      "Max value in reshaped_output: 1022\n",
      "Min value in reshaped_output: 5\n",
      "Contains NaN: False\n",
      "Contains Inf: False\n",
      "Saved: ././independent_track_generation_gpt2_checkpointing_kick_model_cross_attention_10secsdataset_final/2/kick_generated.wav\n",
      "Processing inference2.wav -> snare in folder 2...\n",
      "Reshaped Output: torch.Size([1, 1, 4, 750]), Device: cpu\n",
      "Max value in reshaped_output: 1019\n",
      "Min value in reshaped_output: 25\n",
      "Contains NaN: False\n",
      "Contains Inf: False\n",
      "Saved: ././independent_track_generation_gpt2_checkpointing_snare_model_cross_attention_10secsdataset_final/2/snare_generated.wav\n",
      "Processing inference2.wav -> clap in folder 2...\n",
      "Reshaped Output: torch.Size([1, 1, 4, 750]), Device: cpu\n",
      "Max value in reshaped_output: 1022\n",
      "Min value in reshaped_output: 16\n",
      "Contains NaN: False\n",
      "Contains Inf: False\n",
      "Saved: ././independent_track_generation_gpt2_checkpointing_clap_model_cross_attention_10secsdataset_final/2/clap_generated.wav\n",
      "Processing inference2.wav -> bass in folder 2...\n",
      "Reshaped Output: torch.Size([1, 1, 4, 750]), Device: cpu\n",
      "Max value in reshaped_output: 1022\n",
      "Min value in reshaped_output: 5\n",
      "Contains NaN: False\n",
      "Contains Inf: False\n",
      "Saved: ././independent_track_generation_gpt2_checkpointing_bass_model_cross_attention_10secsdataset_final/2/bass_generated.wav\n",
      "Processing inference2.wav -> drums in folder 2...\n",
      "Reshaped Output: torch.Size([1, 1, 4, 750]), Device: cpu\n",
      "Max value in reshaped_output: 1023\n",
      "Min value in reshaped_output: 1\n",
      "Contains NaN: False\n",
      "Contains Inf: False\n",
      "Saved: ././independent_track_generation_gpt2_checkpointing_drums_model_cross_attention_10secsdataset_final/2/drums_generated.wav\n",
      "Processing inference2.wav -> keys in folder 2...\n",
      "Reshaped Output: torch.Size([1, 1, 4, 750]), Device: cpu\n",
      "Max value in reshaped_output: 1022\n",
      "Min value in reshaped_output: 6\n",
      "Contains NaN: False\n",
      "Contains Inf: False\n",
      "Saved: ././independent_track_generation_gpt2_checkpointing_keys_model_cross_attention_10secsdataset_final/2/keys_generated.wav\n",
      "Processing inference2.wav -> full_instrumental in folder 2...\n",
      "Reshaped Output: torch.Size([1, 1, 4, 750]), Device: cpu\n",
      "Max value in reshaped_output: 1023\n",
      "Min value in reshaped_output: 10\n",
      "Contains NaN: False\n",
      "Contains Inf: False\n",
      "Saved: ././independent_track_generation_gpt2_checkpointing_full_instrumental_model_cross_attention_10secsdataset_final/2/full_instrumental_generated.wav\n",
      "Processing inference3.wav -> hi_hat in folder 3...\n",
      "Reshaped Output: torch.Size([1, 1, 4, 750]), Device: cpu\n",
      "Max value in reshaped_output: 1017\n",
      "Min value in reshaped_output: 36\n",
      "Contains NaN: False\n",
      "Contains Inf: False\n",
      "Saved: ././independent_track_generation_gpt2_checkpointing_hi_hat_model_cross_attention_10secsdataset_final/3/hi_hat_generated.wav\n",
      "Processing inference3.wav -> kick in folder 3...\n",
      "Reshaped Output: torch.Size([1, 1, 4, 750]), Device: cpu\n",
      "Max value in reshaped_output: 1022\n",
      "Min value in reshaped_output: 5\n",
      "Contains NaN: False\n",
      "Contains Inf: False\n",
      "Saved: ././independent_track_generation_gpt2_checkpointing_kick_model_cross_attention_10secsdataset_final/3/kick_generated.wav\n",
      "Processing inference3.wav -> snare in folder 3...\n",
      "Reshaped Output: torch.Size([1, 1, 4, 750]), Device: cpu\n",
      "Max value in reshaped_output: 1022\n",
      "Min value in reshaped_output: 25\n",
      "Contains NaN: False\n",
      "Contains Inf: False\n",
      "Saved: ././independent_track_generation_gpt2_checkpointing_snare_model_cross_attention_10secsdataset_final/3/snare_generated.wav\n",
      "Processing inference3.wav -> clap in folder 3...\n",
      "Reshaped Output: torch.Size([1, 1, 4, 750]), Device: cpu\n",
      "Max value in reshaped_output: 1023\n",
      "Min value in reshaped_output: 11\n",
      "Contains NaN: False\n",
      "Contains Inf: False\n",
      "Saved: ././independent_track_generation_gpt2_checkpointing_clap_model_cross_attention_10secsdataset_final/3/clap_generated.wav\n",
      "Processing inference3.wav -> bass in folder 3...\n",
      "Reshaped Output: torch.Size([1, 1, 4, 750]), Device: cpu\n",
      "Max value in reshaped_output: 1022\n",
      "Min value in reshaped_output: 11\n",
      "Contains NaN: False\n",
      "Contains Inf: False\n",
      "Saved: ././independent_track_generation_gpt2_checkpointing_bass_model_cross_attention_10secsdataset_final/3/bass_generated.wav\n",
      "Processing inference3.wav -> drums in folder 3...\n",
      "Reshaped Output: torch.Size([1, 1, 4, 750]), Device: cpu\n",
      "Max value in reshaped_output: 1022\n",
      "Min value in reshaped_output: 1\n",
      "Contains NaN: False\n",
      "Contains Inf: False\n",
      "Saved: ././independent_track_generation_gpt2_checkpointing_drums_model_cross_attention_10secsdataset_final/3/drums_generated.wav\n",
      "Processing inference3.wav -> keys in folder 3...\n",
      "Reshaped Output: torch.Size([1, 1, 4, 750]), Device: cpu\n",
      "Max value in reshaped_output: 1017\n",
      "Min value in reshaped_output: 11\n",
      "Contains NaN: False\n",
      "Contains Inf: False\n",
      "Saved: ././independent_track_generation_gpt2_checkpointing_keys_model_cross_attention_10secsdataset_final/3/keys_generated.wav\n",
      "Processing inference3.wav -> full_instrumental in folder 3...\n",
      "Reshaped Output: torch.Size([1, 1, 4, 750]), Device: cpu\n",
      "Max value in reshaped_output: 1021\n",
      "Min value in reshaped_output: 1\n",
      "Contains NaN: False\n",
      "Contains Inf: False\n",
      "Saved: ././independent_track_generation_gpt2_checkpointing_full_instrumental_model_cross_attention_10secsdataset_final/3/full_instrumental_generated.wav\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torchaudio\n",
    "import numpy as np\n",
    "from transformers import AutoProcessor, EncodecModel, GPT2LMHeadModel\n",
    "import madmom\n",
    "import torch.nn as nn\n",
    "import tempfile\n",
    "\n",
    "# Constants\n",
    "VOCAB_SIZE = 1024\n",
    "MAX_LENGTH = 3000\n",
    "device = torch.device(\"cpu\")  # Use \"cuda\" if GPU is available\n",
    "\n",
    "# Track classes\n",
    "track_classes = ['hi_hat', 'kick', 'snare', 'clap', 'bass', 'drums', 'keys', 'full_instrumental']\n",
    "\n",
    "# Initialize Encodec Model and Processor\n",
    "processor = AutoProcessor.from_pretrained(\"facebook/encodec_24khz\")\n",
    "model_encodec = EncodecModel.from_pretrained(\"facebook/encodec_24khz\").to(device)\n",
    "\n",
    "# Function Definitions (unchanged except added safety checks)\n",
    "def encode_audio(audio_path):\n",
    "    audio, rate = torchaudio.load(audio_path)\n",
    "    max_length_in_samples = int(rate * 10)\n",
    "\n",
    "    if audio.shape[1] > max_length_in_samples:\n",
    "        audio = audio[:, :max_length_in_samples]\n",
    "    else:\n",
    "        pad_length = max_length_in_samples - audio.shape[1]\n",
    "        audio = torch.nn.functional.pad(audio, (0, pad_length))\n",
    "\n",
    "    if audio.shape[0] > 1:\n",
    "        audio = audio.mean(dim=0)\n",
    "    else:\n",
    "        audio = audio.squeeze(0)\n",
    "\n",
    "    inputs = processor(audio.numpy(), sampling_rate=rate, return_tensors=\"pt\")\n",
    "    inputs = {key: val.to(device) for key, val in inputs.items()}\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model_encodec.encode(inputs[\"input_values\"], inputs[\"padding_mask\"], 3)\n",
    "    \n",
    "    duration = audio.shape[0] / rate\n",
    "    return outputs.audio_codes.squeeze(), min(duration, 10.0)\n",
    "\n",
    "def extract_beats_and_downbeats(audio_path, fps=100, duration=10):\n",
    "    audio, rate = torchaudio.load(audio_path)\n",
    "    audio = audio[:, :int(duration * rate)]\n",
    "\n",
    "    with tempfile.NamedTemporaryFile(delete=False, suffix='.wav') as temp_audio_file:\n",
    "        temp_audio_path = temp_audio_file.name\n",
    "        torchaudio.save(temp_audio_path, audio, rate)\n",
    "\n",
    "    proc_downbeats = madmom.features.downbeats.DBNDownBeatTrackingProcessor(beats_per_bar=[4], fps=fps)\n",
    "    act_downbeats = madmom.features.downbeats.RNNDownBeatProcessor(fps=fps)(temp_audio_path)\n",
    "    downbeats = proc_downbeats(act_downbeats)\n",
    "\n",
    "    proc_beats = madmom.features.beats.BeatDetectionProcessor(fps=fps)\n",
    "    act_beats = madmom.features.beats.RNNBeatProcessor(fps=fps)(temp_audio_path)\n",
    "    beats = proc_beats(act_beats)\n",
    "\n",
    "    os.remove(temp_audio_path)\n",
    "\n",
    "    if len(beats) == 0 or len(downbeats) == 0:\n",
    "        raise ValueError(f\"No beats or downbeats detected in {audio_path}\")\n",
    "\n",
    "    return beats, downbeats[downbeats[:, 1] == 1, 0]\n",
    "\n",
    "def create_positional_embeddings(beat_times, downbeat_times, audio_duration, fps=75, K=32):\n",
    "    total_frames = int(np.ceil(audio_duration * fps))\n",
    "\n",
    "    def ramps(positions, size):\n",
    "        result = np.zeros(size)\n",
    "        for a, b in zip(positions[:-1], positions[1:]):\n",
    "            result[a:b] = np.linspace(0, 1, b - a, endpoint=False)\n",
    "        missing = positions[0]\n",
    "        if missing:\n",
    "            piece = result[positions[0]:positions[1]]\n",
    "            pieces = np.tile(piece, missing // len(piece) + 1)\n",
    "            result[:missing] = pieces[-missing:]\n",
    "        missing = size - positions[-1]\n",
    "        if missing:\n",
    "            piece = result[positions[-2]:positions[-1]]\n",
    "            pieces = np.tile(piece, missing // len(piece) + 1)\n",
    "            result[-missing:] = pieces[:missing]\n",
    "        return result\n",
    "\n",
    "    vector_downbeat = ramps((downbeat_times * fps).astype(int), total_frames)\n",
    "    vector_beat = ramps((beat_times * fps).astype(int), total_frames)\n",
    "\n",
    "    frequencies = np.arange(1, K + 1)\n",
    "    embeddings_downbeat = []\n",
    "    embeddings_beat = []\n",
    "\n",
    "    for k in frequencies:\n",
    "        embeddings_downbeat.append(np.sin(2 * np.pi * vector_downbeat * k))\n",
    "        embeddings_downbeat.append(np.cos(2 * np.pi * vector_downbeat * k))\n",
    "        embeddings_beat.append(np.sin(2 * np.pi * vector_beat * k))\n",
    "        embeddings_beat.append(np.cos(2 * np.pi * vector_beat * k))\n",
    "\n",
    "    embeddings_downbeat = np.stack(embeddings_downbeat, axis=1)\n",
    "    embeddings_beat = np.stack(embeddings_beat, axis=1)\n",
    "    embeddings = np.hstack((embeddings_downbeat, embeddings_beat))\n",
    "\n",
    "    return torch.from_numpy(embeddings).float()\n",
    "\n",
    "class CustomGPT2ForConditionalGeneration(GPT2LMHeadModel):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        # No need to define projection layer if not used\n",
    "\n",
    "    def forward(self, input_ids=None, attention_mask=None, labels=None,\n",
    "                positional_embeddings=None, **kwargs):\n",
    "        # Get input embeddings\n",
    "        input_embeds = self.transformer.wte(input_ids)\n",
    "        # Combine positional embeddings with input embeddings\n",
    "        input_embeds = input_embeds + positional_embeddings\n",
    "\n",
    "        # Proceed with the standard GPT-2 forward pass\n",
    "        return super().forward(\n",
    "            inputs_embeds=input_embeds,\n",
    "            attention_mask=attention_mask,\n",
    "            labels=labels,\n",
    "            **kwargs\n",
    "        )\n",
    "\n",
    "def find_highest_checkpoint(folder):\n",
    "    checkpoints = [d for d in os.listdir(folder) if d.startswith(\"checkpoint-\")]\n",
    "    if not checkpoints:\n",
    "        raise ValueError(f\"No checkpoints found in {folder}\")\n",
    "    checkpoints = sorted(checkpoints, key=lambda x: int(x.split(\"-\")[-1]), reverse=True)\n",
    "    return os.path.join(folder, checkpoints[0])\n",
    "\n",
    "def generate_track(model, audio_codes, positional_embeddings, attention_mask):\n",
    "    # Flatten and pad audio_codes to MAX_LENGTH\n",
    "    audio_codes = audio_codes.flatten().to(device)\n",
    "    audio_codes = torch.nn.functional.pad(\n",
    "        audio_codes,\n",
    "        (0, MAX_LENGTH - audio_codes.shape[0]),\n",
    "        value=0\n",
    "    )[:MAX_LENGTH]\n",
    "\n",
    "    # Generate attention_mask from padded audio_codes\n",
    "    attention_mask = (audio_codes != 0).long().to(device)\n",
    "\n",
    "    # Ensure positional_embeddings are padded to MAX_LENGTH\n",
    "    if positional_embeddings.shape[0] < MAX_LENGTH:\n",
    "        padding_length = MAX_LENGTH - positional_embeddings.shape[0]\n",
    "        positional_embeddings = torch.nn.functional.pad(\n",
    "            positional_embeddings,\n",
    "            (0, 0, 0, padding_length),\n",
    "            mode=\"constant\",\n",
    "            value=0\n",
    "        )\n",
    "\n",
    "    positional_embeddings = positional_embeddings[:MAX_LENGTH].to(device)\n",
    "\n",
    "\n",
    "    # Pass inputs to the model\n",
    "    with torch.no_grad():\n",
    "        outputs = model(\n",
    "            input_ids=audio_codes.unsqueeze(0),  # [1, MAX_LENGTH]\n",
    "            attention_mask=attention_mask.unsqueeze(0),  # [1, MAX_LENGTH]\n",
    "            positional_embeddings=positional_embeddings.unsqueeze(0)  # [1, MAX_LENGTH, embedding_dim]\n",
    "        )\n",
    "        return outputs.logits.argmax(dim=-1).squeeze().detach().cpu()\n",
    "\n",
    "\n",
    "\n",
    "# Inference\n",
    "inference_files = [\n",
    "    (\"inference.wav\", \"inference_posemb.wav\", \"1\"),\n",
    "    (\"inference2.wav\", \"inference_posemb2.wav\", \"2\"),\n",
    "    (\"inference3.wav\", \"inference_posemb3.wav\", \"3\"),\n",
    "]\n",
    "\n",
    "for audio_path, posemb_path, folder in inference_files:\n",
    "    for track_class in track_classes:\n",
    "        print(f\"Processing {audio_path} -> {track_class} in folder {folder}...\")\n",
    "        model_folder = f'./independent_track_generation_gpt2_checkpointing_{track_class}_model_cross_attention_10secsdataset_final'\n",
    "        model = CustomGPT2ForConditionalGeneration.from_pretrained(model_folder).to(device)\n",
    "        model.eval()\n",
    "\n",
    "        audio_codes, audio_length = encode_audio(audio_path)\n",
    "        beats, downbeats = extract_beats_and_downbeats(posemb_path, duration=audio_length)\n",
    "        positional_embeddings = create_positional_embeddings(beats, downbeats, audio_length)\n",
    "\n",
    "        padding_length = MAX_LENGTH - positional_embeddings.shape[0]\n",
    "        positional_embeddings = torch.nn.functional.pad(positional_embeddings, (0, 0, 0, padding_length))\n",
    "\n",
    "        attention_mask = (audio_codes != 0).long()\n",
    "        generated_sequence = generate_track(model, torch.tensor(audio_codes), positional_embeddings, attention_mask)\n",
    "\n",
    "        reshaped_output = generated_sequence.view(4, 750).unsqueeze(0).unsqueeze(0)\n",
    "        reshaped_output = torch.clamp(reshaped_output, min=0, max=1023)\n",
    "        print(f\"Reshaped Output: {reshaped_output.shape}, Device: {reshaped_output.device}\")\n",
    "        print(f\"Max value in reshaped_output: {reshaped_output.max()}\")\n",
    "        print(f\"Min value in reshaped_output: {reshaped_output.min()}\")\n",
    "        print(f\"Contains NaN: {torch.isnan(reshaped_output).any()}\")\n",
    "        print(f\"Contains Inf: {torch.isinf(reshaped_output).any()}\")\n",
    "        decoded_audio = model_encodec.decode(reshaped_output, [None])[0]\n",
    "        decoded_audio = decoded_audio.detach()\n",
    "        decoded_audio = decoded_audio.squeeze(0).squeeze(0)  # Shape: [samples]\n",
    "        decoded_audio = decoded_audio.unsqueeze(0)\n",
    "        output_audio_path = f\"./{model_folder}/{folder}/{track_class}_generated.wav\"\n",
    "        os.makedirs(os.path.dirname(output_audio_path), exist_ok=True)\n",
    "        torchaudio.save(output_audio_path, decoded_audio.cpu(), processor.sampling_rate)\n",
    "        print(f\"Saved: {output_audio_path}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
